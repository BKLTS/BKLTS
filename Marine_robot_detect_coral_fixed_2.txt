&&&&&&&1.Real-time Detecting Method of Marine Small Object
with Underwater Robot Vision
Fengqiang Xu, Xueyan Ding, Jinjia Peng, Guoliang Yuan, Yafei Wang, Jun Zhang, Xianping Fu
Information Science and Technology College
Dalian Maritime University
Dalian 116026, China
{xfq, dingxueyan meow, pengjinjia, yuan, wangyafei, william, fxp}@dlmu.edu.cn
Abstract—Detection and counting small objects using underwater
robot draw an appealing attention because of its urgent
demands in marine aquaculture. Because this challenge problem
must be solved before the underwater robot can be used to catch
seafood in practice instead of diver. This paper proposed a novel
method using Faster R-CNN and kernelized correlation filter
(KCF) tracking algorithm to detect seafood objects, such as sea
cucumber, sea urchin, and scallop and so on in real time. Firstly,
we trained an accurate and stable Faster R-CNN detector with
VGG model using underwater image database, which is built by
ourselves. Next, we recognized and tracked the seafood objects in
order to fetch them using underwater robot vision in naturalistic
ocean environment. The experimental results show the proposed
method can recognized and catch seafood in real time using our
integrated underwater robot.
Keywords—Marine small target; Object detection; Object tracking;
Convolutional neural network; Underwater robot
I. INTRODUCTION
Recently, using underwater robot to detect and to catch
marine small object is a hot research topic,because it is
an efficient solution to replace the diver to capture seafood
in marine aquaculture[1]. To reach this goal, there are two
problems need to be solved: (1) detection of marine small
object and (2) development of underwater robot.
Poor visibility of underwater image captured by cameras
results in a difficult task for perception of ocean environments
and recognizing underwater objects[2]. Due to this reason,
traditional detection methods based on image feature can‘t
perform well. Motivated by the fact that Convolutional Neural
Networks (CNNs) have produced impressive performance improvements
in several areas, such as face recognition, pedestrian
detection, autonomous driving and so on. This paper adopt
Convolutional Neural Network to detect marine small object.
In the past several years, many advanced object detection
method based on CNNs have been proposed, including RCNN[
3], Fast R-CNN[4], Faster R-CNN[5], etc. The Faster RCNN
is state-of-the-art in object detection. And several method
have been proposed in the foundation of it. In this paper, we
adopt Faster R-CNN as the detector to detect seafood.
This work was supported in part by the National Natural Science Foundation
of China Grant 61370142 and Grant 61272368, by the Fundamental Research
Funds for the Central Universities Grant 3132016352, by the Fundamental
Research of Ministry of Transport of P. R. China Grant 2015329225300.
Different underwater robots have been created for ocean‘s
exploration and exploitation. However, almost of them are used
to observe the marine object or capture images and videos. It is
hard to find the application of automatic detection in underwater
robot. Thus, using underwater robot to automatically catch
marine small object still face a huge challenge. To conquer this
problem, it is supposed to develop a underwater robot which
have ability to detect marine small object.
To deal with above problems, in this paper, we proposed
a novel method using Faster R-CNN and KCF tracking algorithm
to detect marine objects. Firstly, we train a detector
on our underwater dataset to produce the bounding boxes and
categories of the object. Then we train a tracker based on
the KCF to track and count the object. Finally, this method is
adopted to detect marine objects using underwater robot vision
in real time.
II. RELATED WORK
A. Object detection
According to whether need region proposal, State-of-theart
object detection methods based on CNN can be divided
into two groups: (1) region proposal based methods and (2)
proposal-free methods[6]. Proposal based methods, including
R-CNN, Fast R-CNN, Faster R-CNN, achieve excellent object
detection accuracy. R-CNN uses selective search to first generate
potential object regions in an image and then perform
classification on the proposed region. However, R-CNN requires
high computational costs since each region is processed
by the CNN network separately. Fast R-CNN builds deep
convolutional networks to efficiently classify object proposals.
Faster R-CNN introduces a Region Proposal Network(RPN)[5]
that shares full-image convolutional features with the object
bounds and scores at each position.
Proposal-free methods, such as YOLO and SSD, have
recently been proposed for real-time detection. These methods
frame object detection as a regression problem which using
a single neural network to detect object and category from
full images in one evaluation. So, it can be optimized endto-
end directly on detection performance. YOLO computes a
global feature map and uses a fully-connected layer to predict
objects classes and locations in a fixed set of regions[7]. SSD
improves YOLO by adding layers of feature maps for each
scale and using a convolutional filter for prediction[8].
978-1-5386-1654-3/18/$31.00 ©2018 IEEE
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 11:34:13 UTC from IEEE Xplore. Restrictions apply.
1.The control mainboard
2. The buoyancy module
3. The underwater propeller
4. The LED lights
5. The shell
6. The buoyancy cables
7. The underwater camera
8. The manipulator
8
7
3
4
1
2
6
5
2
2
(c)
(a) (b)
(d) (e)
Fig. 1. The structure and view of our underwater robot.(a) is the top view of the robot; (d) is the structure of the underwater robot; (b) is the explanation of
(d); (c) and (e) are captured under the water.
B. Faster R-CNN
In object detection, region-based CNN detection methods
are now the main paradigm. Compared to traditional
methodology, deep ConvNets have significantly improved the
performance on image classification and object detection[3].
The Faster R-CNN is state-of-the-art object detection network
depends on region proposal algorithms to hypothesize object
location. It creatively introduces a Region Proposal Network
that shares full-image convolutional features with the detection
network to reduce computational burden of proposal generation.
The structure of The Faster R-CNN is consisted of two
modules: (1)the Region Proposal Network and (2)the Fast RCNN
object detection network. the RPN took an image(of
any size) as input and outputted a set of rectangular object
proposals, each with an objectness score[5]. And the Fast
R-CNN object detection network[1] was adopt to refine the
proposals.
III. THE PROPOSED APPROACH
A. The structure of underwater robot
Recently, the research and development of remotely operated
vehicle (ROV)[9] draws an attractive attention in marine
aquaculture. It has urgent demands to adopt the underwater
robot to detect and grab the marine organism. However, the
manufacturing of the underwater robot is confronted with huge
obstacle, because it needs to address additional issues from the
ocean, such as undercurrent, variation of buoyancy and so on.
To solve these problems, our underwater robot is constitutive
of seven essential parts contained in the shell,which is
showed in Fig.1. The control mainboard is the hub of signal
communication which responsibilities are gathering signal
information from the operating handle to command the device
to work and responding to requirement, such as diving, turning
direction, capturing video and so on. The underwater propellers
are responsible for improving thrust to the robot, and the
buoyancy module could conquer the gravity to keep balance.
To capture high-quality video, the underwater camera is
demanded to support 4K video recording. Both sides of the
fuselage are equipped with LED lights to satisfy the shooting
of underwater dark and turbid environment[9]. The buoyancy
cables are ad hoc bridge for robot to connect with power supply
and displayer. Once done with detection, we can operate the
manipulator to accurately grab the object.
B. Marine object detection based on Faster R-CNN
Because of feature attenuation in underwater image, it is
hard to detect marine objects. For detection algorithm based on
region proposal, Faster R-CNN is classical and fundamental,
which could extract abundant features from image and train as
a powerful detector. Thus, we take advantage of Faster R-CNN
as base framework to train our detector.
In this paper, we adopt ImageNet dataset to pre-train the
network and produce the parameters which are adopted to
initialize our model. After then we make use of our own dataset
collected by ourselves with underwater robot for training and
testing. We generate the proposal regions and scores by RPN.
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 11:34:13 UTC from IEEE Xplore. Restrictions apply.
(d) (e) (f)
(a) (b) (c)
sea cucumber:5; sea urchin:17; scallop:12 sea cucumber:4; sea urchin:8; scallop:11 sea cucumber:4; sea urchin:18; scallop:16
sea cucumber:5; sea urchin:17; scallop:15 sea cucumber:5; sea urchin:8; scallop:12 sea cucumber:4; sea urchin:19; scallop:17
Fig. 2. The experimental results of tracking and counting marine objects. The top images (a),(b) and (c) are the begin frames, and the bottom images (d),(e)
and (f) are the next frames respectively relative to the above images. The boxes of sea cucumber, sea urchin and scallop are separately labeled with blue, red
and yellow color. From (a) to (d), the total of ( sea cucumber, sea urchin, scallop) are varied from (5,17,12) to (5,17,15); From (b) to (e), the total of ( sea
cucumber, sea urchin, scallop) are varied from (4,8,11) to (5,8,12); From (c) to (f), the total of ( sea cucumber, sea urchin, scallop) are varied from (4,18,16)
to (4,19,17).
In the RPN, the last shared convolutional layers of a pretrained
network are followed by a 3 × 3 convolutional layer.
Two sibling 1 × 1 convolutional layers are then added for
classification and regression. After training our detector, we
can generate and select the bounding boxes of the object target
accurately for each frame.
C. Marine object tracking and counting based on KCF
Tracking is an important assignment for statistic of seafood
target[10].And the kernelized correlation filters tracker achives
very impressive results on Visual Tracker Benchmark[11].In
this paper, we employ the KCF tracking algorithm to track
marine objects. Firstly, we initialize KCF tracker with the
bounding boxes of object getting by Faster R-CNN detection.
Then we track the new object of the next frames by aforementioned
tracker. Finally, the total of the seafood object should
be produced.It is significant to figure out the density of the
seafood in marine aquaculture.
By calculating the distance d between the center location
of the bounding box of the seafood objects from detection and
tracking, we further calculate the result r (r = d/w, where w
is the width of the input image). If r < λ (λ = 0.05, where λ
is a threshold), they are regarded as the same one, otherwise
the latter should be regarded as a new object.
However, it is unreasonable to count directly, for instance
the object in one frame may be detected miss or false, but
in other frames is correct[12]. To address the miss or false
detection[13], only when the new object continuously appears
in the next 5 frames should it be counted.
IV. EXPERIMENTAL RESULTS
A. The datasets of underwater image
For task of object detection based on CNNs, datasets are
very important and necessary. There are lots of famous datasets,
such as ImageNet[14], COCO[15], PASCAL VOC[16] and
so on. These datasets usually include the category of common
objects, for instance, dog, cat, vehicle and people. However, the
seafood objects are seldom covered in it. It is disadvantageous
to detection of marine objects. Thus, It is of great importance
to build a dataset of underwater image. So we build a dataset
of underwater images by ourselves.
Nowadays, our underwater image dataset is mainly including
28,000 pictures and three categories: sea cucumber, sea
urchin, and scallop. The images are captured by our integrated
underwater robot in naturalistic ocean environment. In order
to improving the multiplicity of the dataset, we augment
the underwater by doing mirror transformation and image
enhancement for some pictures.
B. Training and testing our model
In order to adapt marine environment of ocean, we train
and test our model on our underwater image dataset, which
are captured by our integrated underwater robot. We compiled
the results in Table 1 using a Nvidia GeForce GTX 1080 Ti
GPU and cuDNN v5.1 and an Intel Core i7-6700K@4.00GHz.
Whats more, We have separately trained two network models
Faster R-CNN ZF and VGG-16. In fact, VGG-16 model is
more complicated than ZF. From the results of experiment,
the mAP of VGG-16 model is higher than ZF.
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 11:34:13 UTC from IEEE Xplore. Restrictions apply.
TABLE I. DETECTION RESULTS ON OUR DATASET
Network mAP
Faster R-CNN ZF 71.3
Faster R-CNN VGG-16 79.6
C. Tracking and counting the object
After training our detector, it is supposed to initialize the
KCF tracker by parameters produced from detection stage.
Then we track the object from frames. With the foundation of
aforementioned work, we can distinguish whether the detected
object and the tracked object are the same one or not.In the
following work, it is possible to count the seafood object. And
the examples of the experimental results are displayed in Fig.2.
As is shown in Fig.2, image (b) is one of 5 frames adjacent to
image (a). From (a) to (d), the number of scallop is increasing
up to fifteen. In (d), all of the seafood except the three new
scallops are same as (a). Only the number of scallop is varied
from 12 to 15, and the number of sea cucumber and sea urchin
remain unchanged.
The experimental results show that our method has a good
performance on detection and counting of marine small target.
It is of great significance to figure out the density of seafood.
D. Applying the method to the underwater robot
After experiment, we have applied the method proposed
in this paper to our underwater robot. As is shown in Fig.3,
the underwater robot is responsible for capturing video from
ocean. Meanwhile, the video is transferred from the robot to
computer by video capture card. Then our algorithm detect
frames in video to produce categories and locations of the
object in real time.
V. CONCLUSION
This paper proposed a novel method using Faster R-CNN
and KCF tracking algorithm to detect marine objects. Firstly,
we train a detector on our underwater dataset to produce the
bounding boxes and the categories of the object. Then we train
a tracker based on the KCF to track and count the object.
Finally, the whole method is adopted to detect marine objects
using underwater robot vision in real time.
In the future work, we plan to test other popular object
detection model, for instance YOLO and SSD. In addition, we
will continue to expand our underwater dataset.



&&&&&&&2.Detection, Localization and Classification of Fish
and Fish Species in Poor Conditions using
Convolutional Neural Networks
Jesper Haahr Christensen
DTU Electrical Engineering
Technical University of Denmark
AUV Competence Centre
ATLAS MARIDAN ApS
2960 Rungsted Kyst, Denmark
jhc@atlasmaridan.com
Lars Valdemar Mogensen
AUV Competence Centre
ATLAS MARIDAN ApS
2960 Rungsted Kyst, Denmark
lvm@atlasmaridan.com
Roberto Galeazzi, Jens Christian Andersen
DTU Electrical Engineering
Technical University of Denmark
2800 Kgs. Lyngby, Denmark
rg@elektro.dtu.dk
jca@elektro.dtu.dk
Abstract—In this work the initial steps towards a system
capable of parametrising fish schools in underwater images are
presented. For this purpose a deep convolutional neural network
called Optical Fish Detection Network (OFDNet) is introduced.
This is based on state-of-the-art deep learning object detection
architectures and carries out the task of fish detection, localization
and species classification using visual data obtained by
underwater cameras. This work is focused towards applications
in the poorly conditioned North and Baltic Sea and is initially
developed for the purpose of recognizing herring and mackerel.
Based on experiments on a dataset obtained at sea, OFDNet is
shown to successfully detect 66.7% of the fish included and
furthermore classify 89.7% of these correctly.
Index Terms—artificial intelligence, deep learning, convolutional
neural networks, object detection, fish detection
I. INTRODUCTION
In certain marine applications such as fish monitoring or
commercial fisheries it would be useful to have a system
capable of automatically detect, localize and classify fish and
fish species in images taken underwater. In monitoring tasks
such a system could be applied for counting populations,
distinguish species present in the area or survey the migration
of fish. This could be in dams, streams, lakes, fish farms or at
sea. For fishing operations, the application could be applied to
parameterize a fish school to effectively enable an estimation
of species distribution or numbers. From this, expected bycatch
and discard rates may be calculated and thus present an
option for commercial fisheries to inspect a detected school of
fish before officially spending any quotas fishing it.
Initially the algorithm is designed to distinguish only among
three classes: herring, mackerel and a “catch-all” class. This
approach is chosen to overcome issues connected with the
availability of sufficient image material needed to train the
algorithm. However this does not hinder future possibilities of
extending the method to include additional species or classes
and integrating the system into real-time environments.
A novel data-driven approach based on current state-ofthe-
art deep learning object detectors utilizing convolutional
neural networks is at the core of the proposed algorithm. This
seeks to overcome challenges in generalization of different
environments and to become a more robust option for such
problems rather than utilizing traditional image processing.
A. Literature Survey
In more recent publications [3], [4], the approach to classify
fish into species is based on deep neural networks. In
[4] a multi-layer neural network is utilized as classifier for
processing already extracted features from a fish image. These
features are based on known parameters such as size and shape
of the fish. In [3] current state-of-the-art convolutional neural
networks (CNNs) are utilized as both feature extractor of an
input image as well as classifier for these extracted features.
In [5]–[7] methods are proposed for the entire process of
detecting and localizing fish in images as well as classifying
detected fishes into their respective species by utilizing stateof-
the-art object detectors based on deep regional convolutional
neural networks (R-CNNs) for both region proposals
and classifications.
The work in [5] and [7] is based on a well-known, but large,
architecture called Faster R-CNN [8]. This is currently among
the highest achieving models for precision in object detection.
Most referenced work for underwater fish detection and
classification are developed for non-intrusive environments,
i.e. the camera system is fixed to a given position. Most of
the data presented in these publications are generally of good
quality w.r.t. visibility, contrast and underwater conditions. The
main targets for species classification is based and proved
upon already available large-scale fish dataset, e.g. [9], which
mainly contains good quality footage of tropical fish species
and does not target specifically chosen species outside of this
scope.
Limitations of such automated systems for fish detection
and classification are commonly scalability and reliability.
Traditional image processing often fails to generalize outside
the scope of which it has been developed and is therefore
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:21:03 UTC from IEEE Xplore. Restrictions apply.
challenged when introduced to new environments, noise and
more poor conditions. Large-scale data-driven models seeks
to eliminate the effects of such variations in data but in turn
usually requires very large annotated datasets and puts a much
larger demand on processing power.
B. Novelty and Contributions
This paper contributes with the design, development and
testing of a novel system to automatically carry out fish
detection and localization as well as fish species classification
on images taken underwater.
In an attempt to overcome limitations present in current
methods, as presented in the literature review, the main technological
contributions are the utilization of a unified neural
network that simultaneously performs bounding box and class
predictions coupled together with the application area within
poorly conditioned environments supported by the dataset
collected for this specific purpose.
Beyond the intended application areas and supported conditions
of image quality, OFDNet further presents a novelty
by being designed at a scale where real-time implementation
on embedded platforms for autonomous underwater vehicles
(AUVs), remotely operated vehicles (ROVs) or simply as part
of the camera system is feasible.
II. METHODOLOGY
This section presents the deep learning based object detection
meta-architecture at the core of OFDNet and the adopted
method to establish a relevant dataset of fish images to train
the neural network.
A. OFDNet Architecture
The work in [10] presents and investigates speed/accuracy
trade-offs of current state-of-the-art object detectors. From
here a desired state-of-the-art object detection architecture has
been identified with requirements based on low latency and
good precision for later to be integrated on small embedded
processors in a real-time environment. This architecture is
known as Single Shot Multibox Detector (SSD) [2] and uses
the small, yet efficient, feature extractor known as MobileNet
[11]. Both the object detection architecture and the feature
extractor are based on deep convolutional neural networks.
The SSD architecture, as shown in Fig. 1, consists of a
single unified network that simultaneously predicts bounding
boxes and box labels with confidence values. This is
inspired by MultiBox [12] which is a technique used for fast
class-agnostic bounding box predictions. This eliminates the
computational overhead from commonly used region proposal
networks or other region search methods; therefore it is much
faster proven with a small decrease in accuracy. The original
work of SSD utilizes VGG-16 [1] as feature extractor, however
in this work MobileNet is utilized instead.
For identifying regions of interest (ROIs) in the image, a
set of predefined boxes are used to evaluate the feature map.
In addition to this, the network extends the MobileNet feature
extractor with additional layers from which predictions are
combined. This introduces a natural handling of objects in
various sizes that complements the use of the predefined boxes
of different sizes and aspect ratios. Unlike methods presented
in the literature review, SSD does not use a set of fully
connected layers to predict class labels or box regressions.
Instead, it employs a set of 3×3 convolutional filters at each
output feature map. This produces the softmax output for class
categories and the regression targets for box offsets.
B. Data Collection
For training the model presented above, a very large dataset
containing annotated images with the objects of interest is
required. As such a dataset for the purpose of fish detection
in the North and Baltic sea is not available, an underwater
data acquisition system has been developed. This enabled
the possibility of acquiring visual data of herring, mackerel
and “other species” fish at Danish aquariums. Fig. 2 shows
the developed system recording data of herring at Fiskeri- &
Søfartsmuseet in Esbjerg, Denmark.
Through three data acquisition campaigns a total of 13 124
images have been acquired and annotated. This dataset in total
contains 4593 fish distributed across herring, mackerel and
“other species” on multiple locations.
Fig. 1. Architecture of the single shot multibox detector. This produces bounding box predictions and class labels with a single feed forward network. Note:
The original paper presents the architecture with a VGG-16 [1] feature extractor. This could be any feature extractor and in the case of this work MobileNet
is used. Source: [2].
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:21:03 UTC from IEEE Xplore. Restrictions apply.
Fig. 2. The developed underwater data acquisition platform recording data of
a school of herring circulating an aquarium at Fiskeri- & Søfartsmuseet in
Esbjerg, Denmark.
Each image was annotated by the means of a bounding box
encapsulating each fish along with a label. This process was
partly done manually using the LabelImg tool [13] and partly
automatically using a sparsely trained neural network.
III. TRAINING
OFDNet produces output in six different layers using the
3 × 3 convolution filters. The classification output in all six
layers are modified such that the network complies with the
3 + 1 classes used in this work, where the additional class is
the background. The predefined boxes are scale-wise applied
in each associated output feature map. This efficiently handles
different object scales in the input image and seeks to mimic
the effect from handling the input image at different scales.
The scale sk of the default boxes for each considered feature
map is computed as
sk = smin +
smax − smin
m − 1
(k − 1), k∈ {1, 2, . . . , m} (1)
where smin = 0.2, smax = 0.95 and m = 6 is the number of
different scales. This generates six equally spaced boxes such
that the lowest feature layer has a scale of 0.2 and the highest
layer has a scale of 0.95. The aspect ratios of the default boxes
are ar ∈ {1, 2, 3, 1
2 , 1
3}. In the convolutional layers Conv4_3,
Conv10_2 and Conv11_2 (see. Fig. 1) are aspect ratios of
1
3 and 3 omitted. The default boxes can be visually interpreted
as shown in Fig. 3.
During training time, images are augmented to increase the
samples in the dataset by random horizontal flips and random
patch cropping. Patch cropping samples a patch of size [0.1, 1]
of the original image size with an aspect between 1
2 and 2. This
drastically improve the detection of smaller objects, as shown
Fig. 3. Visualization of the default boxes chosen for OFDNet utilizing the
SSD architecture.
in [2]. The remaining hyper-parameters used for training are
given in Table I.
The overall training objective is to minimize a combined
loss index accounting for classification confidence and bounding
box regression. This multi-objective loss index L is defined
as
L(p, u, tu, v) = Lcls(p, u) + [u ≥ 1]Lloc(tu, v) (2)
where p is the softmax probability of the evaluated ROI
belonging to true class u. [u ≥ 1] evaluates to 1 when u ≥ 1
such that the catch-all background class, u = 0, is not
considered.
For classification loss Lcls a natural logarithmic loss is used
as Lcls(p, u) = −log(pu) where the softmax probability pu is
defined as
σ(z)j =
ezj
ny
i=1 ezi
, ∀j ∈ {0, 1, . . . , ny}. (3)
for the output layer z and j output classes from 0 (background)
to ny.
For regression (localization) loss Lloc the smoothL1 loss
function [14] is used
smoothL1 (x) =

0.5x2 if |x| < 1
|x| − 0.5 otherwise
(4)
over a tuple of regression targets for the class u, v =
(vx, vy, vw, vh), and a predicted tuple tu = (tux
, tuy
, tu
w, tu
h)
where (x, y) is the centre position, and w and h is the width
and height respectively. Both the center point (x, y) and the
TABLE I
HYPER-PARAMETERS FOR TRAINING THE SSD ARCHITECTURE OF OFDNET.
Architecture
Feature
extractor
Input size Batch size as
1
ar
2 Optimizer LR3
SSD MobileNet
300
×
300
24
{0.2, 0.35,
0.5, 0.65,
0.8, 0.95}
{1:1, 2:1,
3:1, 1:2,
1:3}
Momentum
γ = 0.9 0.0004
1Anchor scales. 2Anchor aspect ratios. 3Learning rate.
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:21:03 UTC from IEEE Xplore. Restrictions apply.
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85
0
0.5
1
1.5
2
2.5
3
3.5
4
4.5
5
5.5
Loss
a
0 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Precision
b
Fig. 4. Training of OFDNet: (a) loss during training, (b) precision.
width w and height h are represented as offsets to the default
bounding box size.
To further increase the accuracy of OFDNet and overcome
commonly known challenges from utilizing a sparse dataset,
the strategy of transfer learning has been adopted. Here the
feature extractor is first trained on the ImageNet database [15]
containing more than 14 million images in 1000 different class
categories. Next the full object detection model is trained on
the Microsoft Common Objects In Content (MS COCO) [16]
database. From this, very good and general filters are obtained
in which the weights are transferred to OFDNet and then finetuned
to the purpose of herring, mackerel and “other species”
detection, localization and classification.
The fully trained OFDNet is evaluated using the metrics of
mean average precision (mAP) which is the commonly used
evaluation metrics for object detectors defined in [17]. OFDNet
achieves a mAP of 0.55 distributed as average precisions
(AP) on the three class categories as: herring AP = 0.47,
mackerel AP = 0.41 and “other species” AP = 0.77.
The training process is shown in Fig. 4. The precision
initially converges at 10 000 steps with a high loss. To guard
against local minima during gradient descent, the learning
rate is altered for a few thousand steps and then set back
to its initial value (cf. Table I). At 75 000 steps, the precision
converges again and the overall loss is now much lower.
a
b
c
Fig. 5. Three samples evaluated by OFDNet. Detected fish are shown with
coloured bounding boxes along with a confidence and class label. Blue is
mackerel, green is herring and cyan is “other species”. The white boxes shows
the annotated ground truth bounding box.
IV. VALIDATION
To inspect the validity of OFDNet some samples from the
evaluation dataset are evaluated as shown in Fig. 5. Most of
the fish initially annotated, with a few exceptions, are detected
by the network and correctly classified.
Other methods present ways to peer into the trained network
to diagnose attentions and assess whether the network is
over/underfitting the dataset. First, some learned filters within
the network can be visualized to reveal the validity of the
type of input patterns or features detected. Three examples
of this are shown in Fig. 6a–6c, which illustrates how the
filters evolves towards more complex states and the patterns
resembles eyes, heads or contours of fishes. Fig. 6d shows an
image generated by investigating which input maximizes the
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:21:03 UTC from IEEE Xplore. Restrictions apply.
a b c d
Fig. 6. Visualized filters from the trained neural network. (a)–(c) shows how the filters evolves from the early layers to the deeper layers in the network. (d)
shows the activation maximization of one of the output classes of the network.
Fig. 7
Fig. 8. Example of an activation map calculated for regions in the input image
where OFDNet detected fish.
output of all filters towards one of the output classes in the
network. Here it can be clearly seen that this resembles parts
of fish in many different orientations.
Activation maps as proposed in [18], also known as attention
maps or heatmaps, uses the final feature map instead of the
final output of the network. This retrieves spatial information
that gets lost in the dense layers. Activation maps enable
visualizing and exploring exactly which regions of an input
image supports the prediction with respect to the output. A
sample of this is shown in Fig. 7, where the network focus on
different regions of the fish such as fins, head, top and bottom.
If the network had overfitted to environmental features or by
other means failed to learn relevant features, the attention maps
would show highest score for non-fish-related regions in the
input image.
V. EXPERIMENTS
For experimental purposes in unexplored real-world conditions
at sea, a dataset consisting of images of fish in the
Weser river in Bremen, Germany, and in the Baltic Sea off
the coast of R¨ugen, Germany, has been collected. The dataset
includes a total of 97 fishes distributed among herring and
“other species” of which the overall quality of the images
is drastically decreased compared to the initial dataset used
for training OFDNet. The visibility in the water is at best 30-
40 cm and the low-light conditions introduce extensive motion
blur.
Given that OFDNet has been trained on good condition
aquarium data that do not include such harsh real-world
conditions, it obtains a 0.21 mAP and successfully detects
44.8% of the fish included in the dataset. Furthermore, within
these detected fishes it classifies 23.1% as belonging to the
correct class category, i.e. either a herring, mackerel or “other
species”.
To increase the detection and classification performance in
sea conditions, and to investigate the capability of OFDNet
to learn and generalize in such environment, 17 data samples
obtained at sea are introduced into the initial training dataset.
This generates a sparse representation of the conditions and
data distribution of real-world data in which OFDNet will seek
to extract features and learn from. The 17 samples are not
included in the evaluation set and have been manually chosen
to have minimum correlation with the evaluation set.
By retraining OFDNet on the extended training dataset, the
performance on the real-world dataset is drastically improved.
OFDNet obtains a 0.56 mAP and successfully detects 66.7%
of the fishes included in the dataset and furthermore classifies
89.7% of these correctly. This shows that OFDNet is capable
of learning from new and previously unexplored environments,
which are defined in a much more difficult state than the initial
dataset.
Fig. 9 shows three images from the real-world dataset
evaluated by OFDNet. Here the low visibility and motion blur
are clearly identified, however OFDNet remains capable to
corretly detect and classify fish for these samples.
VI. CONCLUSION
The paper presented the design of an algorithm for detection,
localization and classification of fishes and fish species
in murky water conditions typical of the Baltic and North
Sea. The algorithm is composed by a neural network called
OFDNet based on the architecture of a low-latency state-ofthe-
art deep learning object detector.
The collected dataset is successfully utilized for training
OFDNet with the purpose of performing fish detection,
localization and fish species classification. During training,
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:21:03 UTC from IEEE Xplore. Restrictions apply.
a
b
c
Fig. 9. Experimental results from evaluating OFDNet with herring data
obtained in the Baltic Sea off the coast of R¨ugen in Germany (a) and (b), and
data obtained of “other species” in the Weser in Bremen, Germany (c).
OFDNet is shown to deliver good results both in terms of
localization and classification of the three principal classes:
herring, mackerel and “other species”. OFDNet is further
validated on data obtained during sea-trials whose image
quality is affected by low illumination and murky conditions.
On this dataset OFDNet successfully detects 44% of the fish
present in the samples, and it classifies 23.1% of the detected
fish into the correct class category.
To prove the major advantage of using data-driven models
over traditional image processing algorithms, the training
dataset is extended with few samples from the real-world
dataset generating a very sparse representation of the previously
unknown real-world environment. After retraining OFDNet,
the detection and classification performance is drastically
improved with 66.7% of detection and 89.7% of correct
classification.
ACKNOWLEDGMENT
This work was supported by Patrizio Mariani from the
National Institute of Aquatic Resources of DTU, ATLAS
MARIDAN ApS and by Max Abildgaard from ATLAS ELEKTRONIK
GmbH in Bremen, Germany. Thanks to colleagues
who have contributed to the work, in particular Marco Jacobi,
Morten S. Nielsen, Martin C. Rotne and Claus Eriksen.


&&&&&&&3.Fast Classification and Detection of Marine Targets
in Complex Scenes with YOLOv3
1st Tingchao Shi
School of Marine Engineering
Northwestern Polytechnical University
Xian, China, 710072
shi tingchao@mail.nwpu.edu.cn
2nd Mingyong Liu
School of Marine Engineering
Northwestern Polytechnical University
Xian, China, 710072
liumingyong@nwpu.edu.cn
3rd Yang Yang
School of Marine Engineering
Northwestern Polytechnical University
Xian, China, 710072
y yang@126.com
4th Sainan Li
School of Marine Engineering
Northwestern Polytechnical University
Xian, China, 710072
18829043370@163.com
5th Peixin Wang
School of Marine Engineering
Northwestern Polytechnical University
Xian, China, 710072
447006353@qq.com
6th Yuxuan Huang
School of Marine Engineering
Northwestern Polytechnical University
Xian, China, 710072
767805632@qq.com
Abstract—In order to meet the needs of fast detection and classification
of different marine targets during intelligent unmanned
surface vehicle (USV) operations, In this paper, I introduce a
convolutional neural network based on one of the most effective
object detection algorithms, named YOLOv3, to classify and
detect images of different marine targets. Firstly, I showed
the network structure of the algorithm in this paper. Then, I
explained how I got the optimal anchor box parameter of the
algorithm. Finally, I improved the activation function to make
the algorithm more robust to noise. The final results show that
the MAP of the detector in this paper is 91:83%,and we reach a
detection rate of 58.3 fps by improving the YOLOV3 algorithm.
Keywords—YOLOv3, Classification, Detection, Marine Targets
I. INTRODUCTION
Nearly 70% of the Earth’s surface area is covered by the
ocean, and 90% of the world economy is done by ocean
transportation. Today, with the deepening of globalization, the
ocean is an important link for economic and cultural exchanges
among countries all over the world. Marine target identification
is playing an important part of modern maritime intelligent
monitoring system.It is of great significance to identify all
kinds of ship targets in monitoring maritime traffic, safeguarding
maritime rights and interests, and improving early warning
capability of coastal defense quickly and accurately.
In recent years, the target detection method based on deep
learning has made great breakthroughs, and the main methods
can be divided into two categories. One is based on the
regional recommendation target detection algorithm. Firstly,
the regional target is used to generate the candidate target, and
then the convolutional neural network is used for processing.
The accuracy is high but it cannot meet the requirements
of real-time application. The representative algorithms are
RCNN[1], Fast RCNN[2] , Faster RCNN[3], Mask RCNN[4],
etc. The other type is the target detection algorithm based
on the regression method. The detection problem is treated
as a regression problem, and the target position and category
are directly predicted. This type of method is fast, but relatively
low precision. Representative algorithms are YOLO[5],
YOLOv2[6], YOLOv3[7], etc. Xiu Li, et al. classified and
detected twelve species of fish using Faster R-CNN[8]. Zuoying
Cui,et al. proposed a method on account of the 32-layer
residual networks to classify different categories of plankton
images[9]. Minsung Sung, et al. applied the YOLO neural
network to detect and classify fish[10].Medhdi et al. proposed
an underwater cable recognition method based on texture
features[11].
However, there are many difficulties in image classification
and object detection due to the complex and harsh sea conditions.
In this paper, we use a convolutional neural network
based YOLOv3 algorithm to classify and detect images of
different marine targets.
II. MODEL DETECTION PROCESS
YOLOv3 is a regression-based target detection algorithm.
Firstly, the input images are divided into S*S grids. Each
grid cell predicts N bounding boxes while training.The information
contained in each bounding box can be represented as
tx; ty; tw; th C , where tx; ty; tw; th represent predicted coordinate
information and C represent predicted confidence information.
8>><
>>:
bx = s(tx) + cx
by = s(ty) + cy
bw = pwetw
bh = pheth
(1)
978-1-7281-1450-7/19/$31.00 ©2019 IEEE
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:22:06 UTC from IEEE Xplore. Restrictions apply.

&&&&&&&4.Development of a Perception System for an Autonomous Surface Vehicle using
Monocular Camera, LIDAR, and Marine RADAR
Thomas Clunie, Michael DeFilippoy, Michael Sacarnyy and Paul Robinette
University of Massachusetts at Lowell, Lowell, MA, USA
yAutonomous Underwater Vehicles Laboratory, MIT Sea Grant, Cambridge, MA, USA
Abstract—This paper describes a set of software modules and
algorithms for maritime object detection and tracking. The approach
described here is designed to work in conjunction with various sensors
from a maritime surface vessel (e.g. marine RADAR, LIDAR, camera).
The described system identifies obstacles from the input sensors, estimates
their state, and fuses the obstacle data into a consolidated report.
The system is verified using experiments conducted on a live system and
successfully demonstrates the ability to detect and track obstacles up to
450m away while operating at 7 fps. The software is open source and
available at https://github.com/uml-marine-robotics/asv_perception.
Index Terms—Autonomous systems, marine robotics, intelligent
robots, mobile agents unmanned autonomous vehicles, autonomous
surface vehicles, marine RADAR, LIDAR, segmentation, classification,
object detection, calibration, sensor fusion.
I. INTRODUCTION
Despite the significant research in the autonomous ground vehicle
domain, relatively little effort has been expended to research the
solutions to the unique problems in the autonomous surface vehicle
(ASV) domain. These unique challenges include wind, waves,
current, glint, sea fog, etc. Even further, there has been relatively
little research in software solutions which combine data from
multiple heterogeneous sensors to form a comprehensive view of
the surrounding environment while addressing these challenges.
The worldwide ASV market is projected to reach $1.2B by
2027 [1], with the bulk of that funding coming from the defense
segment. The US Navy alone has requested $579.9M in FY2021
for the research and development of large unmanned vehicles and
their enabling technologies [32], with various levels of human
involvement, ranging from human operators in the loop to fully
autonomous systems.
An awareness of the environment; specifically, the perception of
both static and dynamic obstacles on the water surface along with
their classification, estimated speed and heading, is a prerequisite for
effective autonomous and semi-autonomous operations, including
International Regulations for Preventing Collisions at Sea (COLREGs)
[33] compliance, navigation, and other specialized tasks.
This perception task is most critical in dense environments such
as inland navigation, where the water surface may be populated
by non-cooperative agents such as human swimmers, boats, other
ASVs, etc. as well as fixed structures such as docks, moorings, etc.
Additionally, the detection of these obstacles must be completed
early enough to allow adequate time for the ASV to perform
avoidance maneuvers, especially at near-field ranges (<275 meters
[25]).
Furthermore, the capabilities and cost of computer hardware,
software, and sensors continues to improve, offering new solutions
to problems that were previously difficult to solve. In the ASV
domain, the integration of LIDAR as well as advancements in deep
learning make reliable obstacle detection a more tractable problem.
In this paper, we describe an efficient obstacle detection, localization,
and tracking system for ASVs. We start with the sensor
payload recommended by Robinette et al. [40] and DeFilippo et al.
[8] (marine RADAR, LIDAR, visible-light camera), and integrate
state of the art research in object detection [6] and on-water image
segmentation [7]. This open source, modular system integrates a
suite of heterogeneous sensors and runs on the Robot Operating
System (ROS) platform at 7 fps. Aside from the system itself,
unique contributions include a method of qualitative monocular
camera and point cloud calibration, along with a fast and modular
3D object tracking and fusion algorithm.
II. RELATED WORK
A. Maritime Sensor Evaluation
Prasad et al. [37] includes a survey of different sensor types
for maritime situational awareness, including RADAR, visible light
camera, infrared, and sonar. Robinette et al. [40] analyzes the utility
and practical limitations of RADAR, LIDAR, stereo camera, and
visible light monocular cameras for inland maritime navigation.
Practically speaking, marine RADAR is currently the only sensor
which can currently provide 360-degree situational awareness at
ranges needed for effective navigation in all weather conditions.
However, it has a slow refresh rate, lacks contact height information,
lacks sufficient resolution at close range, and does not provide
sufficient obstacle identification characteristics.
LIDAR has shown [3] [28] [40] [8] to be useful for short-range
obstacle detection in a marine environment, overcoming the shortrange
limitations of RADAR but fails to detect objects at medium to
long range (typically limited to <100m). Lastly, obstacle detection
using visible light cameras in a marine environment has been shown
to be effective, and can be used to provide supplementary obstacle
information for effective COLREGs compliance, despite the limited
range and field of view.
Utilization of thermal imagery is promising, as it is more resistant
than visible light cameras to solar glare and lens flare, and operates
during the day or night. However, it still may suffer from fog
[40]. Despite these benefits, there has been limited research into
the utilization of this sensor in the marine environment. Recently,
Helgesen et al. [16] found that combining infrared with RADAR
and LIDAR improved small obstacle tracking over RADAR and
LIDAR alone in a maritime environment, in both daytime and
nighttime conditions. Schöller et al. [43] demonstrated some success
detecting ships using Convolutional Neural Networks and data from
a Long Wavelength Infrared sensor, with the detection range varying
from 65m for small vessels to 850m for large vessels. Additional
research is needed on the ability to extract and fuse useful obstacle
information from thermal data while the sensor is mounted on an
ASV.
B. Multi-Sensor Systems
In 2017, Schiaretti et al. [42] provided a survey of existing
ASV prototypes. Of the 60 systems listed, 11 of the systems list
obstacle avoidance capabilities. Only 3 of those systems combined
978-1-7281-9077-8/21/$31.00 ©2021 IEEE
2021 IEEE International Conference on Robotics and Automation (ICRA 2021)
May 31 - June 4, 2021, Xi'an, China
14112
2021 IEEE International Conference on Robotics and Automation (ICRA) | 978-1-7281-9077-8/21/$31.00 ©2021 IEEE | DOI: 10.1109/ICRA48506.2021.9561275
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:22:45 UTC from IEEE Xplore. Restrictions apply.
multiple perception sensor modalities into a cohesive system and
have literature available for review. These 3 systems are discussed
below.
Larson et al. [25], [26] used a two-tier obstacle avoidance system,
with the near-field/reactive avoidance system using RADAR,
stereo vision, nautical charts, monocular vision, and millimeterwave
radar. The far-field system utilized AIS, nautical charts, and
ARPA contacts. For depth estimation in a monocular camera image,
the horizon is estimated using a Hough line transform and then the
distance to obstacle was calculated via trigonometry. However, the
researchers faced obstacle detection and horizon estimation challenges
due to the dynamic water surface and other environmental
factors. In calm waters, RADAR obstacle tracking was successful
up to 200m.
Gray et al. [13] combined camera and LIDAR data in ROS-based
PropaGator I, while Frank et al. [11] added infrared, and passive
SONAR in PropaGator2. LIDAR data was projected onto the 2D
camera image and OpenCV image algorithms were used for obstacle
identification.
Since Schiaretti’s work in 2017, we found 3 more multi-sensor
perception systems available for review. Norbye [31] combined
LIDAR and camera data. The ROS-based system used a customtrained
YOLOv3 [38] object detector and successfully tracked
obstacles up to 60m away in real time. Sorbara et al. [44] utilized a
camera, infrared, and a single-beam laser range finder on a rotating
platform to achieve a 180 degree field of view. Sorial et al. [45]
combined a camera with LIDAR and used YOLOv3 [38] for boat
detection, and tracked the boats in the camera and LIDAR frames.
Each of the aforementioned systems fail to provide adequate
perception data for an ASV. Larson’s system was simply ahead
of its time; performant deep learning frameworks for on-water
detection were not yet conceived, and commercial LIDAR sensors
were generally not available. The remaining systems have been
shown to be effective at short range detection. We aim to build on
these concepts to extend obstacle detection and tracking to distances
beyond 100m, exploiting the long range capabilities of both the
camera and marine RADAR, as well as potentially improving shortrange
obstacle detection during adverse environmental conditions.
C. Single-Sensor Systems
Recently, there have been several other systems which utilize a
single perception sensor and/or include sensors not considered in
this work. However, the authors’ approaches towards solving their
respective problems may be utilized to solve subproblems of the
current task at hand.
Kufoalor et al. [23] utilized Automatic Identification System
(AIS) for obstacle tracking. Benjamin et al. [3] utilized LIDAR
to generate pointclouds, which were then clustered to produce
obstacles. Muhoviˇc et al. [29] used LIDAR to generate obstacles
from a pointcloud, with depth fingerprint for tracking, and later
[28] employed a stereo camera to generate a point cloud, filtered out
the water surface, and then used a histogram-like depth appearance
model for obstacle identification and tracking. Zhuang et al. [52]
used radar-generated images and image processing algorithms to
detect and track obstacles. Kuang et al. [22] applied clustering
methods to generate obstacles using UHF radar. Paccaud et al. [34]
used a low cost, consumer-grade camera for obstacle detection on a
lake. Manderson et al. [27] used deep neural networks for collision
avoidance and object detection on an underwater vehicle. Fiorini
[10] et al. developed an efficient machine learning approach towards
person and vessel detection with a UAV-based moving and zooming
camera. Ueland et al. [47] used LIDAR for SLAM on a surface
vessel for autonomous marine exploration. Heidarsson et al. [15]
used a profiling SONAR for obstacle detection and avoidance.
Many of these systems are designed for smaller, specialized, low
power vessels thus do not completely solve the current problem.
However, these researchers employed a variety of methods in
processing sensor data and provided useful insights into current
methods of obstacle detection.
D. Maritime Object & Horizon Detection, Segmentation
There have been many recent developments in the area of onwater
object detection, segmentation and horizon detection from
visible light imagery. Prasad et al. [37] presents a survey of maritime
object detection and tracking approaches from video, and Muhoviˇc
et al. [28] provides a thorough review of obstacle detection on
the water surface. Additionally, there has been recent success by
Bovcon and Kristan [7] and Steccanella et al. [46] in the use of
neural networks for segmentation of on-water images to distinguish
between water, sky, and obstacle pixel data. Petkovi´c [36] et al.
provides an overview of horizon detection methods in maritime
video surveillance.
Although these approaches do not directly address the current
problem in its totality, they are useful when considering approaches
to processing the data from visible light cameras and can be
utilized within the context of a larger perception system. The
ability to differentiate between obstacle and non-obstacle, even if
the type of obstacle is unknown, is an important capability of a
perception system. Additionally, the ability to identify the horizon
helps decrease the size of the search space while reducing the
number of irrelevant detections.
III. SYSTEM ARCHITECTURE
The primary goal of the perception system is to identify both
static and dynamic obstacles (of known and unknown types) and
estimate their position, size, and heading. Building on the work
of previous researchers, we start by selecting the sensor platform
which provides the necessary raw data which meet the obstacle
detection needs of an ASV, and then select the software approaches
which process the sensor data to generate candidate obstacles for
consolidation into a cohesive view. We then report this information
to an external navigation/obstacle avoidance system (such as [3])
and/or human operator.
From a sensor perspective, we utilize the recommendations of
Robinette et al. [40] and DeFilippo et al. [8], choosing a marine
RADAR, LIDAR, and 3 forward-facing monocular visible light
cameras for our experiments. See Section IV-A for details on the
selected sensor configuration utilized in this work.
From a software perspective, we utilize the ideas employed by
Norbye [31] and Sorial et al. [45] to integrate state-of-the-art object
detection algorithms along with LIDAR data to detect, identify,
and localize obstacles using the visible light camera and LIDAR.
Detection of unidentified obstacle types is provided by Bovcon and
Kristan’s WaSR segmentation network [7] . Lastly, the integration of
RADAR data is inspired by the Larson et al. model [25] to achieve
obstacle detection at the desired range. We then fuse the obstacle
information together using a simple fusion system, as described
below. Fig 1 provides an overview of the software architecture.
Conceptually, we have N heterogeneous sensors, each associated
with a sensor-specific module. This module is responsible for creating
localized obstacle detection data for each obstacle it encounters
14113
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:22:45 UTC from IEEE Xplore. Restrictions apply.
Fig. 1. Perception System Architecture
in every frame. These detections are then processed with a Kalman
filter [21] to reduce noise, and are then fused with detections from
the other N-1 modules to create a cohesive report of all obstacles.
Additionally, the system is designed to be flexible in the quantity
and type of sensor inputs, easily accommodating additional sensor
inputs (e.g. extra cameras, radar, etc) and fusing their outputs into a
singular, cohesive view. Below are the details of the sensor-specific
modules which generate obstacle detection data.
A. RADAR and LIDAR Obstacle Generation
Both of the RADAR and LIDAR sensors utilized on our test
platform output their data as point clouds. Therefore, the problem
can be reframed as one of identification of obstacles from point
cloud data, while allowing some flexibility in the identification
process to account for differences in sensor data. For example, a
marine RADAR limited to 100m may provide a denser point cloud
for the same obstacle as generated by the same marine RADAR
when operating in excess of 1km. Additionally, marine RADAR
only returns data in two dimensions as compared to LIDAR which
provides data in all three dimensions.
For the efficient manipulation of point clouds, we utilize the Point
Cloud Library (PCL) [41] to filter out land masses and generate
obstacles from point clusters. Land mass filtering is accomplished
by removing points which can be clustered into two-dimensional
areas greater than some two-dimensional area A, and then candidate
obstacles are generated by clustering points based on some Euclidean
distance D. In our tests, the optimal values of A and D varied
by sensor, as described above, and were discovered empirically.
Moreover, the land mass filtering process was not completely
successful in removing all land-based obstacles, resulting in many
false positives when operating near shore. Ideally, our land mass
filtering would utilize known map data for the removal of land mass
obstacles to significantly reduce the number of false positives.
After the candidate obstacles are generated from the point clouds,
their data is sent to the sensor fusion module (described below) for
further processing.
B. Monocular Camera Obstacle Generation
To generate candidate obstacles from a camera image, we must
have a way to 1- identify both known and unknown obstacles in an
image and 2- understand where those obstacles are located relative
to our vessel. Furthermore, we wish to preserve unique information
that is acquired from the camera image, such as object classification
(e.g. boat, person) and color information (e.g. colored navigational
buoy). This information can be used to supplement detection data
Fig. 2. Refined visualization of a sailboat when combining the classification
and segmentation images
from other sensors, as well as identify unique obstacles which
cannot be identified by other sensor types alone.
In order to accomplish this, we combine an object detector using
YOLOv4 [6] with the semantic segmentation network developed
by Bovcon and Kristan [7]. After performing object detection and
image segmentation independently, the two images are fused (Figure
2) to refine the predicted object detection bounding box, create
candidate obstacle data, and then project the remaining unclassified
obstacle pixels to a pseudo-LIDAR pointcloud for further processing.
1) RADAR-camera calibration: For the accurate estimation of
3D position of an obstacle identified in a monocular camera image,
we require a way to convert 2D image pixel location (u, v) to 3D
vessel-relative position (x, y, z). An accurate transformation to the
vessel’s frame is critical for the perception task, where the difference
of a single vertical image pixel can represent over a dozen meters
in world position as an obstacle approaches the horizon.
Achieving proper calibration is especially challenging in this
context due to 1- the continual shift of camera pitch and roll
due to waves and boat propulsion and 2- the coarse returns and
a slow refresh rate (1 Hz) of our marine RADAR relative to the
visible light camera. It is also possible that camera positions change
slightly during operation due to adverse environmental conditions
(e.g. choppy water). Maintaining calibration over time also presents
a challenge, as operators may remove the cameras from the boat
between missions, resulting in the need to recalibrate between
the two sensors before each mission in order to achieve proper
alignment.
Existing literature [2], [49], [48] on the topic of RADAR-camera
calibration generally involve the transformation using corresponding
points between the camera and RADAR planes, and then using an
ordinary least squares or RANSAC methods to find the projection
matrix P which defines a transformation between the two planes.
Alternative approaches using horizon detection [35] have limited
utility in inland waterways (such as our targeted operating environment),
while other georeferencing approaches (e.g. [17]) assume
known/fixed camera extrinsics. However, in contrast to the RADAR
type and calibration environment used in existing literature, our
situation required an alternative approach to achieve some success.
14114
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:22:45 UTC from IEEE Xplore. Restrictions apply.
To address the aforementioned challenges, we utilize a simple,
yet effective method for RADAR-camera calibration using a
visually intuitive, qualitative approach. This approach combines
a 2D top-down RADAR point cloud with a monocular camera
image, allowing the operator to visualize and qualitatively calibrate
the camera and RADAR using multiple parameters to achieve
adequate precision. More generically, this problem can be thought
of as the minimization of error in a projection between two
nearly perpendicular planes. This method can be employed in a
live, on-water environment with unknown camera intrinsics and
extrinsics, only requiring the existence of some arbitrary obstacles
(e.g. land, ship, buoy) which are visible in both the camera image
and RADAR returns to be used for alignment visualization and
parameter tuning. An additional benefit of the user interface tool
is that the calibration becomes resilient to hardware configuration
changes; if the mounting of the camera is adjusted or the camera is
replaced entirely, the user has the ability to adjust the calibration to
the new environment. Moreover, pitch and roll measurements from
an Inertial Measurement Unit are integrated into the calculations in
order to reduce the calibration error that arises from continual pitch
and roll changes.
In order to to project camera image point (ui; vi) to RADAR
image point (ur; vr), we seek the 3x3 perspective transformation
matrix Pi
r such that
0
@
ur
vr
1
1
A = Pi
r
0
@
ui
vi
1
1
A
We first define the parameters of the perspective transformation
which we seek to tune. Let f be the field of view, 
; ; 
represent the yaw, pitch and roll, respectively, along with tx, ty,
tz for a translation in the x, y, and z dimensions. The pitch 
is computed by the multiplying the pitch imu and pitch velocity
􀀀im_ u acquired from the IMU with coefficients  and  such that
 = imu   + im_ u  

. Roll 
 is calculated using the same
method.
Similar to the work in [20], we seek the matrix F from its
constituent components. Let R
RR
 be the yaw, pitch and roll
rotation matrices, T is the translation matrix, and P is the projection
matrix, defined as follows:
P =
0
BB@
1= tan (f=2) 0 0 0
0 1= tan (f=2) 0 0
0 0 1 0
0 0 􀀀1 1
1
CCA
(1)
T =
0
BB@
1 0 0 tx
0 1 0 ty
0 0 1 tz
0 0 0 1
1
CCA
(2)
R
 =
0
BB@
cos 
 􀀀sin 
 0 0
sin 
 cos 
 0 0
0 0 1 0
0 0 0 1
1
CCA
(3)
R =
0
BB@
1 0 0 0
0 sin  cos  0
0 cos  􀀀sin  0
0 0 0 1
1
CCA
(4)
R
 =
0
BB@
cos 
 0 sin 
 0
0 1 0 0
􀀀sin 
 0 cos 
 0
0 0 0 1
1
CCA
(5)
F = PTR
RR
 (6)
Fig. 3. Homography visualization. Sailboat (top left) and associated RADAR
obstacle (green). Left: correct homography. Right: homography error during
rapid pitch change. Also note the bow eye position change (bottom center),
indicative of a change in camera pitch relative to our platform.
The projective transformation Pi
r is then computed by using 4
static points from the the RADAR image dimensions L and matrix
F, followed by solving a system of linear equations for these points,
as described by Hartley and Zisserman [14].
Next, we define a conversion between radar image pixels and
world coordinates. The RADAR image is generated by flattening
the points in the RADAR pointcloud and placing them in a square
image with an arbitrary side length of L=1024. Let Prw
be the
projection from radar image pixels to world coordinates, then the
transformation to a right-handed coordinate system is defined as
Prw
=
0
@
0 􀀀1 L=2
􀀀1 0 L=2
0 0 L=2R
1
A (7)
where R is the real-world range of the RADAR. Lastly, transforming
camera image coordinates (u,v) to world coordinates (x,y)
is simply
0
@
x
y
s
1
A = Prw
Pi
r (8)
followed by a normalization of the resulting vector so that s = 1.
Additionally, due to the isomorphic nature of the transformation, this
process can be reversed in order to find image coordinates (u,v) from
world coordinates (x,y). See Figure 3 for an example of a visualized
homography.
The resulting matrix can also be used to find an estimate of the
horizon location in the camera image, which is constantly changing
due to boat movement. Because we are primarily concerned with
obstacles which are located on the surface of the water, we can use
the location of the horizon to limit our search space, simultaneously
decreasing processing time while filtering out some irrelevant
detections.
Given an image pixel (u,v) and homography matrix M, where
M = Prw
Pi
r (see equation (8)), we seek the vertical component v
which represents the horizon at horizontal component u. Mathematically,
this may be expressed as setting the denominator of the v component
(where v = (uM10+vM11+M12)=(uM20+vM21+M22)
14115
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:22:45 UTC from IEEE Xplore. Restrictions apply.
Fig. 4. Tracking and Fusion Framework
to zero and solving for v, while treating u as the independent
variable for a given perspective transformation matrix M:
horizon(u;M) =
uM20 + vM21 +M22 = 0
v = 􀀀(uM20 +M22)=M21
(9)
Future work in this area would focus on employing more sophisticated
methods for heterogeneous sensor calibration, e.g. [53], [19],
[9].
C. Obstacle Tracking and Sensor Fusion
In order to operate autonomously in a maritime environment, it is
important to understand information about nearby obstacles. Salient
attributes include location, dimensions, classification, convex hull,
and trajectory.
In our system, we have a collection of n  N heterogeneous
sensors, each with different noise characteristics and update rates.
Each sensor n generates d  Dn detections (containing incomplete
and/or inaccurate information) at time t. Our goal is to create a
comprehensive list of all obstacles K by combining the data from
all sensors over the set of time points T, where t  T.
To accomplish this, we employ a simple, high performance, twophase
online method as pictured in Figure 4. At a high level,
we manage the detections and obstacle state estimation at the
sensor level using a single-frame global nearest neighbors approach,
and then fuse the outputs of the sensor-level obstacle state into a
comprehensive view for delivery to an external system and/or human
operator. Here, we define obstacle state, or obstacle, as a collection
of obstacle-related attributes to include position, heading, velocity,
etc. and is defined in Table I.
1) Phase One: In the first phase, detections d  Dn from sensor
n are associated with sensor-level obstacles s; s  Sn via a single
frame global-nearest neighbors approach. Each detection d contains
a subset of the obstacle state attributes, but requires at least a
vessel-relative position. We then calculate a cost matrix between
detections d and sensor-level obstacles Sn using user-configurable
choice of distance metrics, e.g. Hellinger [18], Bhattacharyya [5],
or Euclidean. Empirically, the Hellinger distance (which considers
position covariance) was more effective than Euclidean distance
for noisy detections such as those generated by our cameras, but
was less effective than simple Euclidean distance for more accurate
sensors such as LIDAR or RADAR.
After the cost matrix is calculated, we then attempt to match all
detections in Dn with the set of existing obstacles Sn, which were
defined at time t 􀀀 1. Here we use the Hungarian algorithm [24]
to solve the bipartite graph matching problem with minimum cost
in polynomial time. Recent work [50] [4] has shown this to be an
effective combination for multi-object tracking in the autonomous
car domain, and our research indicates this also applies to the
autonomous surface vehicle domain as well.
Detections which do not match within a user-configurable maximum
distance result in the birth of a new obstacle s, while
existing obstacles within Sn that are not matched with the current
set of detections are removed after i consecutive frames of failed
matching.
Lastly, for all obstacles in Sn, the obstacle’s position, velocity,
and 3D rectangular dimensions are updated using a Kalman filter
[21] with a constant velocity model. When the next set of detections
Dn arrives at time t+1, we execute the predict step of the Kalman
filter for each obstacle Sn, and this updated information is used in
the next cycle for detection association as described above.
2) Phase Two: In phase two (fusion), we employ a modified
version of the phase one algorithm. In the fusion module, we define
a fused obstacle k  K where k = fs  S0; :::; s  Sng;
i.e. each fused obstacle contains up to 1 obstacle s from each of
the sensors Sn. Distance cost, matching, and fused obstacle birth
proceed as described above. However, rather than using a Kalman
filter as in phase one, we combine the obstacle state from each
obstacle s to provide a fused state estimate. Fused obstacle position
and covariance is computed using a fixed interval smoother [12],
while size estimates are obtained from the largest estimate of the
associated sensor obstacles s  k.
Finally, after the fusion step has completed, we output the list of
obstacles over IP using the NMEA and JSON formats. This output
can be used by external obstacle avoidance/navigation systems or
human operators, for example. This information contains a rich set
of features describing each fused obstacle, as listed in Table I.
TABLE I
GENERATED FEATURES FOR EACH OBSTACLE
Obstacle ID Timestamp Area
Classification
Label
Classification
Label Probability
3D Position
3D Position
Covariance
Orientation 3D Dimensions
3D Linear
Velocity
3D Linear
Velocity
Covariance
2D Convex Hull
IV. RESULTS
A. Test Platform & Data
Experiments for this system were conducted on a live system
in conjunction with the AUV Lab at MIT Sea Grant College. 60
minutes of data was collected along the Charles River from the
MIT sailing pavilion towards the the Boston Harbor and back
in October 2020 during fair-weather, daylight conditions. A total
of 67 obstacles were observed, consisting of a variety of smallto
medium-sized boats and static, free-standing obstacles such as
pillars. The sensor platform was a Boston Whaler R/V Philos, with
a sensor payload consisting of a Simrad Broadband 4G Radar, three
forward-facing visible light cameras (FLIR Blackfly, BFLY-PGE-
13E4C-CS, 1280x1024, 12fps) covering 140 degree horizontal FoV,
a Velodyne VLP-16 LIDAR, and a SGB Systems Ellipse-D Dual
Antenna RTK INS for GPS/IMU. The perception code was executed
14116
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:22:45 UTC from IEEE Xplore. Restrictions apply.
Fig. 5. Navigating a mooring field. Left: classified bounding boxes of boats.
Right: 3D RViz view showing RADAR returns (green) and tracking of both
classified (purple) and unclassified (gray) obstacles.
on a PC with an Intel i7-8700 3.2 GHz processor, 32GB of RAM,
and a Nvidia GTX 1060 video card with 6GB VRAM, running
Ubuntu 16.04. The machine learning model (trained on MS-COCO)
for YOLOv4 [6] was downloaded from the author’s website. WaSR
[7] was not utilized due to insufficient hardware resources, but was
shown to be effective in segmenting obstacle pixels in the camera
image when executed offline against logged data.
B. Evaluation
We are not aware of any annotated datasets which combine
marine RADAR, LIDAR, and monocular camera to use as a baseline
for this system. Therefore, to establish a ground truth, we use
manual verification of the obstacles in the visible light cameras and
compare that to the outputs of the sensors, both individually and as
a whole. At the obstacle level, we define success as the system’s
ability to identify and localize free-standing obstacles which can
be verified within a camera’s field of view, along with the ability
of the tracking and fusion system to correctly associate detections
between sensors and track them over time.
System evaluation metrics include the total number of true
positives (TP), false positives (FP), and false negatives (FN), along
with the corresponding precision and recall scores. A definition of
these metrics is available in [51]. Land-based obstacles are omitted.
TABLE II
SENSOR TRACKING EVALUATION
Sensor Range TP FP FN Precision Recall
RADAR 250m 59 7 7 0.89 0.89
Camera 450m 11 30+ 1 <0.27 0.92
LIDAR 50m 15 1 3 0.94 0.83
Totals 85 38+ 11 <0.69 0.89
V. DISCUSSION
At the sensor level, detection and tracking generally worked well
on the open water and in moderately congested areas, as shown
in Figure 5. The marine RADAR tracker successfully identified
and tracked obstacles up to 250m, and after some parameter
adjustments, later demonstrated the ability to identify and track
obstacles up to 500m (the configured limit of the RADAR) on
logged data. The LIDAR tracker identified and tracked nearby boats
in most cases. One of the false positives was generated from a
wake, and false negatives can be attributed to obstacle generation
parameters. Meanwhile, the camera tracker had little difficulty in
identifying obstacles of the class ’boat’, but had great difficulty in
consistently localizing obstacles across frames, resulting in a large
number of false positives. These difficulties can be attributed, in
part, to a suboptimal homography and camera tracking parameter
configuration. However, the primary source of the failures is the
difficulty in maintaining a proper homography between the camera
and the RADAR while the boat’s pitch is changing rapidly (see
figure 3). Additionally, not included in the metrics is a significant
number of false positive camera detections which where generated
from a single miscalibrated camera. After adjusting the calibration
and embarking on a subsequent mission, these false positives were
significantly reduced.
In small, confined areas (e.g. locks, narrow channels, under
bridges), sensor-level detection and tracking did not perform well.
The large, amorphous blobs produced by marine RADAR were
generally filtered out as land masses, and were the source of the
RADAR-level false negatives. Similar to the RADAR, the LIDAR
data was often filtered out as land mass, leading to missed detections
of two boats when in close proximity. In both instances, however,
the boats were correctly detected by the cameras, though in one of
those instances, the detection did not occur until 20m away due to
lens flare.
The sensor fusion system performed adequately at combining the
sensor-level obstacle data that was correctly localized, but failed
at times to correctly localize the obstacle relative to the perceived
ground truth, and failed in a few instances where false positive
sensor obstacles were not consolidated into a single instance. With
some additional work in combining sensor obstacle data more
intelligently, the fusion system’s accuracy and reliability would
increase.
VI. CONCLUSION
Overall, the system performed adequately on a live vessel, but
there are many ways to improve this system in the future. Due
to its use of visible light cameras, object detection capabilities
are reduced during adverse environmental conditions (night, fog,
etc). Research into object detection and segmentation of infrared
imagery could improve detection capability in these scenarios. The
RADAR-camera calibration remains an error-prone process with
little margin of error, and an improved system would address
these shortfalls to improve localization. With the integration of
nautical charts, known land masses could be filtered and reduce
the number of false positive detections, but charts alone do not
provide sufficient obstacle information (especially while traversing
confined areas), necessitating complementary sensor information. A
probabilistic occupancy map could be generated from the various
sensor inputs, complimenting the obstacle detection algorithms and
improve autonomous decision-making in confined areas. AIS data
could be integrated for improved obstacle type identification, rather
than solely relying on visible light classification. A custom image
classification model could be built to better identify more types
of maritime obstacles in order to leverage that information for
COLREGs compliance. More robust sensor fusion algorithms (e.g.
GLMB [39], JIPDA [30]) may improve the output of the sensor
fusion system. Subsampling the visible light image (e.g. across the
horizon) may improve object detection range.
VII. ACKNOWLEDGEMENTS
Partial support for this project was provided by Lockheed Martin.
Partial support for this project was provided by the Brunswick
Corporation. The views, opinions and/or findings expressed are
those of the authors and should not be interpreted as representing
the official views or policies of our sponsors. The authors thank
Shailesh Nirgudkar for feedback on a draft of this paper.
14117
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:22:45 UTC from IEEE Xplore. Restrictions apply.


&&&&&&&5.Multi-Target Tracking Considering the Uncertainty of Deep
Learning-based Object Detection of Marine Radar Images
Eunghyun Kim1, Jonghwi Kim2, and Jinwhan Kim2
Abstract—In this paper, a multi-target tracking approach
that integrates the extended Kalman filter and deep learningbased
object detection in marine radar images is presented.
The Gaussian YOLOv3 method is utilized for object detection,
providing both position measurements and their uncertainties.
The extended Kalman filter is employed to estimate the position,
heading, and speed of each detected target considering the
uncertainty values obtained from the object-detection process.
The global nearest neighbor-based data association and a dual
filter structure composed of a confirmed track and a reserved
track are applied to enhance the robustness of the tracking
process. The feasibility of the proposed algorithm is validated
through a real-world marine radar dataset collected in a coastal
environment.
I. INTRODUCTION
In recent years, autonomous ships in the field of marine
robotics have seen remarkable growth in demand and
interest. This is due to their potential to perform various
tasks, such as coastal monitoring and accident response, in
a more cost-effective and efficient manner than traditional
manned ships. To guarantee the safety and efficiency of these
autonomous ships, it is crucial to have robust methods for
perceiving obstacles and motion estimation of target ships.
Among the various sensors used in maritime environments,
marine radar is a primary sensor in view of its capability
to detect distant obstacles under diverse weather conditions
and lighting scenarios, while maintaining a high degree of
reliability. However, to guarantee the accuracy of the radar
data, it is critical to differentiate targets from the noise
generated by marine clutter.
Recently, deep learning has been spotlighted to be an
effective approach to solving complex problems in computer
vision. The use of deep learning algorithms for object detection
in marine radar images is a natural choice due to their
ability to learn complex patterns for differentiating between
targets from the noise. Several studies have explored the
utilization of deep learning-based object detection of marine
radar[1], [2].
Furthermore, in addition to object detection, motion analysis
of the targets is also important. An extended Kalman
filter (EKF) using marine radars is commonly used for multitarget
tracking in marine environments, as it can effectively
handle nonlinear dynamics and incorporate measurements
from radar sensors. The EKF provides optimal state estimates
of multiple targets by using a statistical model of the
1Robotics Program, KAIST, Daejeon, 34141, Korea
olohyun@kaist.ac.kr
2The Department of Mechanical Engineering, KAIST, Daejeon, 34141,
Korea (stkimjh, jinwhan)@kaist.ac.kr
Fig. 1: Network architecture of Gaussian YOLOv3
uncertainty in the measurements and model.
In the measurement update stage of the EKF, it is important
to implement a method for determining uncertainty
values. There are several methods available, including experimental
evaluation or tuning of existing deep learning
algorithms, or obtaining the uncertainty value together with
the result value. Incorporating the uncertainty obtained from
deep learning into the EKF can improve the precision and
stability of multi-target tracking in marine environments. In
multi-target tracking scenarios, deep learning enables the
update of different uncertainties for each target, as opposed
to other methods that apply the same uncertainty value to all
targets.
There are several pieces of research dealing with uncertainty
in deep learning in computer vision. Gal et al. [3]
provides a Bayesian approximation of model uncertainty
using dropout, a widely used regularization technique in
deep learning. Kendall et al. [4] modeled both aleatoric
and epistemic uncertainties and showed that this leads to
improved performance on several computer vision tasks.
However, previous studies have mainly focused on predicting
the level of uncertainty while not utilizing the uncertainty of
the measured values. To address this issue, Kim et al. [5]
incorporate particle filter and Gaussian YOLOv3[6] which
models the localization uncertainty for tracking ships from
the camera sensor. Inspired by this paper, we integrate
adapting Gaussian YOLOv3 to marine radar with the EKF
framework. Gaussian YOLOv3 predicted localization uncertainty
by Gaussian modeling and reconstruction loss function
of YOLOv3[7]. Compared to previous studies, this study
estimated each actual measurement uncertainty for detected
objects in one scene, rather than using predefined static
2023 20th International Conference on Ubiquitous Robots (UR)
June 25-28, 2023. Hawaii Convention Center, Honolulu, Hawaii
979-8-3503-3517-0/23/$31.00 ©2023 IEEE 191
2023 20th International Conference on Ubiquitous Robots (UR) | 979-8-3503-3517-0/23/$31.00 ©2023 IEEE | DOI: 10.1109/UR57808.2023.10202163
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:29:28 UTC from IEEE Xplore. Restrictions apply.
uncertainty. Additionally, to distinguish whether measurements
are the existing track or the new track, a dual-track
filter structure and the global nearest neighbor (GNN)-based
data association[8], [9] were employed. The validation of
the proposed method was conducted on a real-world marine
radar dataset obtained in coastal environments[10].
II. PROPOSED METHOD
A. Object Detection with Uncertainty using Gaussian
YOLOv3
Among the commonly used detection algorithms based
on YOLO, we adopted Gaussian YOLOv3 because of its
ability to estimate uncertainty. The Gaussian YOLOv3[6]
models the position and size of the object detection result as
Gaussian distributions. The network architecture of Gaussian
YOLOv3 is shown in Fig. 1. The result of detection is
shown in Fig. 2. The training set and test set consist of 509
and 80 images, respectively, and all images were manually
annotated. To augment the dataset, the images transformed
horizontal, vertical, and 90◦ rotations. In Fig. 2-(a), marine
radar images are displayed as inputs. The result of the
input after inference through the detection layer is shown in
Fig. 2-(b). The outputs illustrated the mean (μ) by a center
point within the bounding box and variance (Σ) depicted by
crossed lines inside the boxes for each coordinate (x, y). Additionally,
the size of the bounding box (w, h) is represented
by solid lines and predicted uncertainties of box sizes are
indicated by dashed lines around the boxes.
This result was obtained by adjusting the loss function of
YOLOv3[7] to incorporate mean and variance parameters as
follows:
Lx = −ΣW i=1ΣHj
=1ΣKk
=1γijk log(N(xG
ijk|μtx (xijk),Σtx (xijk) + ϵ)),
(1)
where Lx is the negative log likelihood loss function, W, H
are the number of grids of each width and height, K is the
number of anchors, γijk is the parameter from ground truth
bounding box size, N means normal distribution, μtx (xijk)
is the output of the detection layer at the k-th anchor in
(a) Input (b) Output
Fig. 2: Example of object detection: It represented position
by a center point and size by bounding box. Predicted
uncertainties of box center coordinates are visualized by
crossed lines inside the boxes and uncertainties of box sizes
are represented by dashed lines around the boxes.
(a) Clear edge image object
has low uncertainty of position
(b) Blurry edge image object
has high uncertainty of position
Fig. 3: Comparison of detection results for images with the
same object(ship) but different blurring in other scenes
the (i,j) grid, Σtx (xijk) is the output of detection layer,
indicating uncertainty of coordinate, xG
ijk is the ground truth
of coordinate, and the ϵ is 10−9 that assigned value for
numerical stability of the logarithmic function. The others
(i.e., Ly, Lw, and Lh) are the same as Lx except for each
parameter. The high noise in the learning data results in an
increase in the loss, and the model adjusts by increasing the
variance(Σ) to decrease the loss.
This approach explains effectively marine radar image
detection scenarios, where the same object in different scenes
can appear distinct in some images and blurred in others.
As a result, this approach captures some scenes where the
objects with a more blurry edge elevate uncertainty in their
position. Fig. 3 (a) and (b) reflect this tendency, showing
that more clear object image leads to the low uncertainty of
the object’s position increasing accordingly. In Fig. 3 (b), A
blurry edge shape object image gets a high uncertainty of
position results which is illustrated with long crossed lines
inside the boxes.
B. Dual Filter Structure and Data Association Method
After the targets are detected from the radar image, they
are used as measurements of the tracking filter. The dual
filter structure and GNN-based data association make use
of the distinctive strengths of marine radar, such as its
resilience in tough weather conditions and its high-resolution
range, to achieve precise target tracking in complex and
cluttered environments. Additionally, it exhibits robustness
to challenging marine tracking scenarios, including false
detections, target merging, and splitting.
The tracking algorithm is structured with a dual track
system consisting of reserved and confirmed tracks [9]. The
phase conversion of a reserved track to a confirmed track
is contingent upon the fulfillment of the traditional M-of-
N rule, which requires the detection of the target in N
or more out of the total M observation scenes. For those
tracks that have been converted to confirmed tracks using
the M-of-N rule, the motion estimation is started using
the EKF-based tracking filter. When the uncertainty level
in the position exceeds a predefined value, the tracking is
terminated and deleted. This approach alleviates false detections
and missed targets, and enhances the performance of
192
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:29:28 UTC from IEEE Xplore. Restrictions apply.
Fig. 4: The multiple target tracking process using dual filter
structure flowchart
multi-target tracking systems through efficient management
of track initialization and deletion. The overall scheme is
shown in Fig. 4.
The newly detected target measurements are matched with
the existing targets in the filter through the utilization of
the GNN-based data association [8]. They are associated
when the Mahalanobis distance between them is lower than
a predefined ellipsoidal gate as follows:
d =
p
˜z′S−1˜z ≤ G, (2)
where the Mahalanobis distance d is the normalized statistical
distance, S is the measurement innovation covariance
matrix, G is a predefined gate, and ˜z is the measurement
innovation vector which is defined by the difference between
the expected measurement estimated by the filter and measurement
results. Because multiple measurements may lie
within the gates of multiple tracks, the track with the smallest
Mahalanobis distance to the measurement is selected as the
best match.
C. Target Tracking using EKF
The EKF is a widely used algorithm for estimating the
state of a dynamic system in real-time. One of the primary
advantages of the EKF is its robustness in measurement noise
and non-linearities in the system model. The EKF iteratively
updates its estimate of the system state by predicting the
state based on the previous estimate and then correcting the
prediction based on the measurement. Thus, by correcting
the prediction by detection results with uncertainty in deep
learning results as described in Section II-A
The motion of targets is predicted through the application
of an EKF by a constant velocity model. The filter structure
utilizes a three-degrees-of-freedom (3DOF) kinematic model
to describe the movements of the detected targets.
In order to estimate multi targets, the state vector of the
tracking filter structure is augmented by cascading the target
state vector as follows:
xT = [x⊤T1 x⊤T2 . . . ]⊤, (3)
where the ith registered state vector is xTi .
xTi is represented as follows:
xTi = [xTi yTi ψTi VTi ]⊤, (4)
where xTi and yTi are the positions in the global frame, ψTi
is the heading angle, and VTi is the speed of target. The
equations of the motion for the filter’s kinematics can be
described as follows:
˙xTi = [VTi cos ψTi VTi sin ψTi 0 0]⊤ + w, (5)
where w is the zero-mean Gaussian process noise which
reflects the uncertainty of the motion model.
The measurement model for updating the filter is expressed
as follows:
zj = [zxj zyj ]⊤ + v, (6)
where zxj and zyj represent the jthx, y position of the
bounding box, v = [vxj vyj ]⊤ denotes the positional uncertainty
of the bounding box obtained from the results in
Section II-A. These measurements are updated based on
the results of the data association procedure outlined in
Section II-B, which determines whether they correspond to
an existing track or a new reserved track.
III. RESULT
The feasibility of the proposed algorithm was verified
through its application to experimental data obtained near
Suyeong Bay in Busan [10]. The ship was equipped with
GPS, compass, and radar, and the sensor measurements
were synchronized at a rate of 0.5 Hz. Two representative
scenes, depicted in Fig. 5 and Fig. 6, were selected to
demonstrate the feasibility of the algorithm. The red dashed
lines represent estimated target trajectories, black circles
show the AIS data of each target ship. The blue dashed line
represents own ship’s trajectory which was obtained by GPS,
while the measurements from marine radar images obtained
using Gaussian YOLOv3 were represented by magenta dots.
The first scene as shown in Fig. 5 depicts a situation in
which a target ship crosses in front of the own ship. In this
scene, it was possible to observe that the tracking algorithm
was accurate and stable. The second scene as shown in Fig. 6
shows ships moving in a straight line being captured by the
radar while the own ship was rotating near a floating object
without AIS data. The results show that the target ships were
tracked accurately despite the dynamic movements of the
own ship.
A constant offset of approximately 30 m can be observed
when comparing the estimated target trajectories to the AIS
data, which may have been caused by the incorrect radar
installation location or the large size of the target ships.
However, the offset remained consistent, indicating that the
targets were being tracked accurately.
The results demonstrate the efficiency of combining measurements
with uncertainty from deep learning and EKF for
multi-target tracking in marine environments. The figures
confirm that all targets follow the AIS data and their states
were accurately estimated.
193
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:29:28 UTC from IEEE Xplore. Restrictions apply.
Fig. 5: Result of tracking target ship with crossing situation
in global coordinates
IV. CONCLUSIONS
In this research, we demonstrate the efficiency of integrating
deep learning-based object detection with the EKF for
multi-target tracking in marine environments. The Gaussian
YOLOv3 algorithm was employed to detect target positions
and uncertainties for the purpose of tracking. The proposed
method employs a GNN-based data association and a dualtrack
structure to achieve robustness and efficiency. The
performance of the proposed approach was evaluated on
a real-world marine radar dataset. The results showed that
the proposed approach achieved stable and constant tracking
performance, demonstrating the potential of this integration
for marine autonomous systems.
V. ACKNOWLEDGEMENT
This is supported by the ’Autonomous Ship Technology
Development Program(20011722, Development of a Situational
Awareness System for Preventing Collisions and
Accidents of Autonomous Ships)’ funded by the Ministry
of Trade, Industry & Energy(MOTIE, Korea).


&&&&&&&7.Coral Identification and Counting with an Autonomous Underwater
Vehicle*
Md Modasshir1, Sharmin Rahman1, Oscar Youngquist2, and Ioannis Rekleitis1
Abstract—Monitoring coral reef populations as part of
environmental assessment is essential. Recently, many marine
science researchers are employing low-cost and power efficient
Autonomous Underwater Vehicles (AUV) to survey coral reefs.
While the counting problem, in general, has rich literature, little
work has focused on estimating the density of coral population
using AUVs. This paper proposes a novel approach to identify,
count, and estimate coral populations. A Convolutional Neural
Network (CNN) is utilized to detect and identify the different
corals, and a tracking mechanism provides a total count for
each coral species per transect. Experimental results from
an Aqua2 underwater robot and a stereo hand-held camera
validated the proposed approach for different image qualities.
I. INTRODUCTION
Coral reefs are an essential part of the marine ecosystem
and support a rich diversity of life [1], [2] with significant
economic value and social amenity. Projected increases in
global temperatures of 2 − 4.5◦C by 2100 [3] indicate that
mass coral bleaching events are likely to become an annual
phenomenon by 2050 [4], [5]. The widespread mortality of
corals following mass bleaching events reduces the structural
complexity of reefs, thus eliminating the 3-D habitat. This
habitat loss affects diversity and population of coral reef
fish and invertebrates communities adversely. Therefore, the
monitoring of health of coral ecosystems by scuba divers and
new technologies, such as underwater robots [6], has become
increasingly significant. As a result, millions of images are
being collected. While image acquisition has evolved, reef
monitoring still requires the identification and counting of
different coral species, a task primarily performed by human
experts.
Underwater and surface autonomous vehicles have been
used for a variety of monitoring tasks, mainly focusing
coral reef inspections, [7]–[9] even in deep waters [10].
Furthermore, floating cameras have also been employed [11],
[12] to collect visual data with reduced cost. In this paper
we have utilized an Aqua2 [13] Autonomous Underwater
Vehicle (AUV) [14] and a hand-held stereo GoPro camera1.
Object counting is an active research field, and in particular,
the coral counting problem is challenging for many
*This work was supported by NSF award 1513203.
1M. Modasshir, S. Rahman, and I. Rekleitis are with Computer
Science and Engineering Department, University of South
Carolina, USA. [modssshm,srahman]@email.sc.edu,
yiannisr@cse.sc.edu
2 O. Youngquist is with the Department of Computer Science
and Software Engineering, Rose-Hulman Institute of Technology, USA.
youngqom@rose-hulman.edu
1http:\\www.gopro.com
Fig. 1. Aqua2 AUV collecting visual and acoustic data over a coral reef,
Barbados.
reasons. Firstly, visibility, color suppression and hazing underwater
make detection extremely difficult. Objects within
the field of view are often so obscure that both deep models
and humans cannot identify. Moreover, coral counting
encompasses both spatial and temporal domains. Once an
object is detected, it is imperative to prevent recount after
detection in the subsequent image frames. Finally, performing
detection and counting on a low-powered AUV poses
significant constraints on the choice of the detection model
and the frequency of the detection process.
To analyze coral reef visual data, marine biologists cover
certain transects, such as straight lines or rectangles, over
the coral reef. Afterwards, domain experts analyze the video
to count or annotate coral species to estimate population
density. This paper proposes a novel technique to automate
this process. In this work we have slightly modified a recent
deep learning network, RetinaNet [15], to account for the
much-reduced number of training examples, in the presence
of high-class imbalance among coral samples in the dataset.
The modified network identifies and localizes different coral
species in an image. The complete system has a constant
stream of images as input, either from an AUV or a handheld
camera; images are fed into the Convolutional Neural
Network (CNN), and different coral species are identified
and localized in the image by a bounding box. Then each
bounding box is tracked in successive images using the KCF
tracker [16] from OpenCV2. If a new detection occurs in
the vicinity of a tracked bounding box, then the bounding
box coordinates are updated, but the coral count does not
increase.
It is worth noting that the proposed methodology for coral
identification and counting, even when performed off-line,
2http://www.opencv.org
978-1-7281-0377-8/18/$31.00 © 2018 IEEE 524
Proceedings of the 2018 IEEE
International Conference on Robotics and Biomimetics
December 12-15, 2018, Kuala Lumpur, Malaysia
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:31:13 UTC from IEEE Xplore. Restrictions apply.
automates the process of identifying the biodiversity for a
fixed trajectory using data collected by a robot. Different
hardware architectures have been evaluated [17] to port the
detection and tracking system on-line, a task that is beyond
the scope of this paper.
Experimental verification over two datasets collected at the
coral reefs of Barbados has demonstrated the accuracy of the
proposed system. To our knowledge, there has not been any
other work on automated coral counting.
The next section provides an overview of related work.
Section III describe the proposed methodology for autonomous
coral identification and counting. Experimental
results from an AUV and a hand-held stereo camera are
discussed in Section IV. We conclude this paper with a
discussion of experimental results and directions for future
research.
II. RELATED WORK
A large number of algorithms has been proposed to tackle
the counting problem in the visual domain. These algorithms
mostly fall into either regression-based methods or detection
based methods. Regression-based methods [18]–[23] use
CNN based models to predict the number of objects in
the image without explicitly classifying and localizing the
objects. Recent works on regression based methods generate
density maps for image patches and later integrate over the
map to produce the object count. However, most of these
regression based methods aimed at solving counting the
problem in a single image, thus have no mechanism for tracking
over successive images. On the other hand, in detection
based methods [24]–[26], a detector (usually CNN based) is
utilized to localize the objects in the image with bounding
boxes. These bounding boxes are counted to estimate the
number of objects present in the image. In CNN based
detectors, there exist two distinct approaches: two-stage
detectors (region proposal based) and one-stage detectors.
In the two-stage approach, the first stage generates a sparse
set of object proposals and the second stage classifies the
proposals into foreground classes and background. R-CNN
[27] significantly improved two-stage detectors using a CNN
to generate proposals. The Faster-RCNN [24] integrated the
region proposal generation and the proposal classification
into a single CNN, achieving higher speed and accuracy
gains in object detection. On the other hand, one stage
detectors uses Feature Pyramid Networks [28] to enable
object detection at multiple scale. In one-stage detectors,
OverFeat [29], SSD [30], [31] and YOLO [25], [32] showed
tremendous speed improvements but with accuracy tradeoff.
Even with large computational resource available, the
single stage detectors trail in accuracy behind the two stage
detectors.
Recent work by Lin et al. [15] significantly advanced the
one-stage detection network matching state-of-the-art results
of the two-stage detectors. Lin et al.indicated that the class
imbalance during training of one-stage detectors is the main
reason for low accuracy performance. This class imbalance
is usually handled in two-stage detectors by using different
sampling heuristics such as a fixed foreground-to-background
ratio or On-Line Hard Example Mining (OHEM). To better
train detector models in the presence of class imbalance, Lin
et al. [15] proposed a new loss function that is a dynamically
scaled cross entropy loss. Our work is inspired by the model
proposed by Lin et al. [15] as it closely matches our problem
domain.
Visual tracking is an active research area in computer
vision. Numerous works utilize correlation filter (CF) for
robust visual tracking. The popularity of CF based trackers
are due to their rapid speed and efficient learning techniques.
With low computational load, CF can learn a large number
of samples. The early CF-based trackers used a single
channel feature as input with tremendous tracking speed.
The MOSSE [33] tracker exploited adaptive correlation filter.
Henriques et al. [34] introduced kernel trick in the correlation
filter formula. Later, Henriques et al. [16] further improved
CF by integrating multi-channel input and introduced KCF.
Inspired by the improvement on multi-channel correlation
filters, many CF based trackers employing deep learning features
[35]–[38] achieved state-of-the-art performance. However,
even with GPU acceleration, most of these CNN based
trackers cannot track in real-time (15-30 fps) which is a
requirement for tracking and counting coral objects in AUVs.
III. METHODOLOGY
Automated collection of visual data has an advantage
that images are acquired in a sequence, so the location of
the camera together with the structure of the scene can be
recovered. Therefore, a coral detected in an image can be
effectively tracked over successive images, while in the field
of view, and not over-counted. Furthermore, tracking the
location of the different corals reduces the rate of detection,
resulting in a more efficient algorithm.
There are two steps to automate the coral population
estimation process: detection and tracking. Detection of
coral species allows the localization of coral objects in the
image. Later, tracking is essential to prevent the recount in
subsequent detection. Our system integrates both of these
steps as shown in Fig. 2. When processing a stream of
images from either Aqua2 or hand-held camera, the first
frame, f0 is fed to the detection model which produces
labels and bounding boxes for coral objects in the image.
These bounding boxes are used as the Region Of Interest
(ROI) to initialize a tracker that keeps updating the location
of those detected coral species in frames f1 to fn−1. The
system keeps count of the ROIs with labels. Then the coral
detector model runs again on frame fn and produces another
set of bounding boxes (ROIs) with labels. At this point,
the ROIs are compared with the earlier tracked ROIs and
only the new ROIs with overlap (intersection over union)
less than a threshold are counted as new coral objects and
tracked in the subsequent frames along with earlier ROIs.
This step ensures that no coral is counted more than once.
For the coral detector model, we utilize a deep convolutional
neural network inspired by the RetinaNet model [15], and as
525
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:31:13 UTC from IEEE Xplore. Restrictions apply.
Frame f0
Frame f1-fn
Frame f0-fn
Coral
Location
Coral Detector
KCF Tracker & Counter
KCF Tracker Initializer
Class + box
subnets
Class x4
subnet
box
subnet
(c) class subnet (top) (d) box subnet (bottom)
Class + box
subnets
Class + box
subnets
(a) ResNet
(b) Feature pyramid net
Fig. 2. Proposed system with detection and tracking. For n frames, detection is performed on f0. Then jointly f0 and detected object bounding boxes
are used to initialize the tracker. From Frames f1 to fn−1 are sent only to the tracker to update object locations.
a tracker, we use the KCF [16]. Next we will explain the
coral detector model and the KCF tracker.
A. RetinaNet
RetinaNet [15] is a one-stage detector comprising a backbone
network and two subnetworks. The two subnetworks
are used for classification and bounding box regression.
Using the output of the backbone network the first subnet
computes class confidence and the second subnet regresses
bounding box coordinates. We choose the Feature Pyramid
Network (FPN) [28] as the backbone network. The FPN
creates multi-scale features from a single resolution image
by augmenting a standard CNN with top-down pathway
and lateral connections as shown in Fig. 2. This multi-scale
feature pyramid allows any level of features to be used to
detect objects at a different scale. We choose ResNet50 with
50 layers, a variant of deep Residual Networks [39] as the
base network for the FPN. We redesigned the final layers for
RetinaNet to reflect the eight classes of interested.
Focal Loss: We choose focal loss [15] to optimize classification
subnet. Focal loss is designed to facilitate training
under high dataset imbalance. For estimated probability pt ∈
[0, 1] for a target class t, the focal loss is defined as
loss(pt) = −α (1 − pt)γ log(pt)
where γ ≥ 0 is a tunable focusing parameter and α ∈ [0, 1]
is a weighting factor. The focal loss assigns a lower loss to
easily classified examples and focuses more on the misclassified
data. Therefore, the network is better tuned to recognize
difficult samples. Moreover, γ parameter controls the downweighting
of easy examples smoothly. The weighting factor
α is chosen as the inverse of samples for classes in our
dataset.
Smooth L1 Loss: For bounding box regression, smooth
L1 loss is used. For bounding box prediction t and v, the
loss is defined as
Lossbbox =

i∈x,y,w,h smoothL1 (ti − vi)
where
smoothL1 (x) =

0.5 x2, if |x| < 1.
|x| − 0.5, otherwise.
(1)
Training: We initialize the ResNet50 [39] network with
the weights trained on imagenet [40], and all other layers are
randomly initialized to zero-mean Gaussian distribution with
a standard deviation of 0.01. Stochastic gradient descent was
used to optimize the parameters. The network is trained for
150 iterations on our dataset with an initial learning rate of
0.001 and a decay of 1e−5.
B. KCF Tracker:
The KCF [16] tracker improves generic correlation filter
by using multiple channel data of color images. Let y =
[y1, y2, ..., yk]T ∈ R be the Gaussian shaped response and
let xd ∈ Rk×1 be the input vector. The correlation filter
learns filter weights w by optimizing:
w˙ = arg min
w
K
k=1
(yk −
D
d=1
XT
k,dWd)22
+ λ W2
2 (2)
where Xk,d is the k-step circular shift of the input vector Xd,
yk is the k-th element of y, W = [WT
1 ,WT
2 , ...,WTD
]T where
Wd ∈ RK∗1 refers to the filter of the d-th channel [41].
IV. EXPERIMENTAL RESULTS AND DISCUSSION
For this paper, we have selected different approximately
straight line trajectories over open areas with sparse live
coral populations. We have trained our network to detect
seven different kinds of corals (Brain, Maze, Mustard, Finger,
Fire, Star, and Starlet) and sponges. The training samples are
annotated from several other underwater videos. Using the
system described above each sequence of the images is fed
526
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:31:13 UTC from IEEE Xplore. Restrictions apply.
(a) (b) (c)
(d) (e) (f)
(g) (h) (i)
(j) (k) (l)
Fig. 3. Examples of different coral detections. First two rows ((a)-(f)) are collected with an Aqua2 AUV; second two rows ((g)-(l)) using a stereo GoPro
camera. Each two rows are separated by a couple of seconds: (a),(d) Single brain coral detection. (b),(e) Multiple corals (brains, and star) and a sponge
detected. (c),(f) Multiple brain corals detected in a single image. (g),(j) Single brain coral detection. (h),(k) Multiple corals (brain, mustard, and finger)
detected. (i),(l) Multiple mustard corals detected.
Brain Mustard Finger Star Starlet Maze Fire Sponge
T1:GoPro 173/197 125/147 15/9 1/3 11/18 15/20 3/3 2/3
T1:Aqua2 57/66 5/9 0/0 0/0 2/2 0/0 0/0 12/9
TABLE I. CORAL IDENTIFICATION AND COUNTING FOR DIFFERENT TRAJECTORIES. CNN-PREDICTION/HUMAN-ANNOTATED
into the tracking system, and the total count for each class
is reported at the end.
Fig. 3 present representative images with different corals
tracked. The first two rows are images collected by the Aqua2
AUV in two different time instances. As can be seen in
Fig. 3(a) a single Brain coral is detected at the top of the
image; on the second row, Fig. 3(d), the same coral is tracked
when the AUV approached closer. The first column presents
a single coral tracked over several successive frames (only
two displayed). In the second column, several corals from
different classes are tracked, while the last column displays
tracking of several different corals belonging to the same
class. More specifically, Fig. 3(b) and Fig. 3(e) present the
detection and tracking of Brain and Star corals and a large
sponge (three classes); and Fig. 3(c) and Fig. 3(f) display
the tracking of multiple Brain corals. The images collected
from the AUV have lower resolution and display a certain
amount of blurriness. However, identification and tracking of
different coral species were feasible.
The last two rows of Fig. 3 presents underwater images
from a GoPro camera over a coral reef. The image resolutions
are different, and the image quality is relatively higher. The
three columns contain results from single coral, multiple
corals from different classes, and multiple corals from the
same class, respectively. A single brain coral is detected in
Fig. 3(g) and then tracked across the image in Fig. 3(j).
Multiple corals belonging to the Brain, Mustard, and Finger
coral classes are tracked in Fig. 3(h) and Fig. 3(k). Finally,
527
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:31:13 UTC from IEEE Xplore. Restrictions apply.
multiple corals all belonging to the Mustard coral class are
tracked in Fig. 3(i) and Fig. 3(l).
While most corals were localized, corals belonging to
the Finger coral presented a challenge as they are usually
distributed over a much larger area. If there are patches of
dead coral inside a large patch of Finger corals, then the
patch is counted as two different occurrences. Future work
will specifically target finger corals and how to identify all
the corals that belong to the same patch.
Quantitative results are presented in Table I for two sample
trajectories. For each of the seven coral species (Brain,
Maze, Mustard, Finger, Fire, Star, and Starlet), the number
of corals detected by our system against the number of corals
annotated by a human is presented, and the same numbers
are presented for the sponge class. The estimated number
of individual coral population is reasonably close to the
human annotated numbers for corals and sponge for handheld
camera. For Aqua2 transact, there is close matches with
ground truth. We did not have any finger coral in the Aqua2
transact. However, for finger coral, there are more predictions
than actual instances. On the speed performance, the system
can run at 15 fps which will allow online detection and
tracking in Aqua2.
In our empirical analysis, there were fewer mispredictions
and the difference in number of coral species between
predicted and ground truth are due to detector’s incapability
to detect some coral species. Furthermore, although KCF
performed reasonably well, there were some instances of
track losses, which resulted in over-counting of several coral
objects. The detector model could be further be improved
by utilizing more data in the training process. Using a better
tracker will certainly improve the performance of the system.
We observed that for such system of counting, the ability to
correctly identify at close range is more important than the
ability to detect all coral objects within field of view for a
detector model. This is because once detected, a coral object
is being tracked and counted, therefore, earlier or subsequent
detection failure does not hamper the counting process. The
number of frames between subsequent detection plays crucial
role here. For hand-held camera which has more clarity
and higher resolution, 10 frames between detection provided
good result as tracking worked reasonably well. However,
for Aqua2 which has lower resolution, less illumination, and
less clear images, 5 frames between detection was chosen
and more than 5 frames reduced the counting performance
because of frequent tracking failure.
One future direction of this work is utilize vision-based
state estimation information to prevent recount. State estimation
is still a very challenging problem [42] especially
underwater. Fig. 4 sketches the main idea of utilizing state
estimation in counting corals, in which the AUV moves
over the coral reef; inertial, visual, depth, and acoustic data
are collected. Depending on the choice of software, all or
a subset of the data are used to estimate the pose of the
cameras and over time. From the pose of the camera for each
image, a simple projection can identify the 3D position of
each detected coral. Therefore, detected corals from different
Fig. 4. Sonar, Visual, Inertial Estimation underwater.
images are distinguished if they are projected in 3D locations
that are sufficiently apart.
V. CONCLUSIONS
In this paper, we presented an automated system for
identifying and counting the different corals encountered by
an AUV. A modification of a popular CNN architecture [15]
ensures improved performance with a limited dataset of
eight classes. The current performance of the proposed
system ensures near real-time performance on state of the
art GPU machines, without any optimization. Ongoing work
on porting the algorithm on a portable system such as the
NVidia TX2 or the Intel Neural Compute Stick will enable
the on-line deployment. The system as it is can achieve 12
fps on NVidia TX2.
Tighter integration of the camera position into the tracking
algorithm will enable the full 3D localization of the different
corals and prevent double counting when a coral exits
and then re-enters the camera’s field of view. Furthermore,
integrating a dense reconstruction of the observed corals
will enable the study of the structural complexity of reefs.
Modeling the 3D habitat, which is critical to maintaining
diversity and population of coral reef fish communities, will
allow for better health assessment of the reef ecosystem.


&&&&&&&6.Vision Based Underwater Environment Analysis: A Novel
Approach to Estimate Size of Coral Reefs
A.Maurya1, R.Govekar1, Pramod Maurya2and Anirban Chaterjee2
1National Institute of Technology, GOA, India
2National Institute of Oceanography, GOA, India
abhishek11.maurya1997@gmail.com, rajatgovekar@gmail.com, maurya@nio.org,
snanirban@nitgoa.ac.in
Abstract—The purpose of our work is to detect and estimate the size of underwater
coral reef from image frames using vision based technique. Deep learning
architecture is proposed to detect the coral reefs. To extract the frames and build
data set, real time underwater video is used. The size of reef is estimated using
distance-based algorithm with detected coral information. A nonlinear function is
formulated and optimized to get accurate reef size as pixels taken by it in image
changes with distance. This algorithm can also be used for real-time analysis of the
underwater environment. It is evident from the analysis results and comparison with
actual data that the proposed method is accurate in estimating the area occupied by
underwater coral reefs.
Index Terms— Coral Reef, Deep Learning, Nonlinear, Distance-based
1. Introduction
Coral reefs are one of the world’s most
biological marine ecosystems. Coral reefs,
ecologically species diversity and bio
productivity in the ocean are important because
they are the counterpart to tropical rain forests.
Since the past decades it is increasingly at risk
of the wide diversity of animal and plant species
contributing to their system and its genetic
heritage. Coral reef allows for the formation of
associated ecosystems, which enable basic
habitats, fish and livelihoods to be created.
Therefore, it is important to monitor the coral
reef ecosystem. The automated analysis
process for underwater visuals would enhance
the investigation of coral reef dynamics and
processes through reduction of the analytic
bottleneck and the full use of large submarine
image data sets by researchers.
Using vision-based technique, analysis of
underwater environment can easily be
performed getting great accuracies. One of the
deep learning architectures, MobileNet SSD
(Neural Network Architecture) is used for
detection of underwater coral reefs from image
frames. Depth wise convolution is used in
978-1-7281-5326-1/19/$31.00 ©2019 IEEE MobileNet architecture, helps in fast
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:31:26 UTC from IEEE Xplore. Restrictions apply.
A.Maurya et al .:Vision Based Underwater Environment Analysis: A Novel Approach to Estimate Size of Coral Reefs 175
processing of high quality image taken from
GoPro Hero4 camera. Base MobileNet Layers,
used as feature extractor converts image pixels
to features that describes the image content.
SSD, a single-shot multicategory detector is
faster than previous state-of -the-Art single-shot
detectors (YOLO). SSD layers of the Neural
network responsible for predictions of bounding
boxes.
Image Enhancement of frames becomes
important as quality of images are different with
varying depth and hence increases accuracy of
detection. Using Image frames Pixel per metric
ratio of it with different distance is used to
estimate the size of coral reef, taking nonlinear
parameters into consideration.
2. Detection of Coral Reef
using MobileNet SSD
Figure 1 shows method of detection of
underwater coral reefs.
Fig. 1. Methods of detection of underwater coral reefs
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:31:26 UTC from IEEE Xplore. Restrictions apply.
176 ©2019 IEEE PROCEEDINGS OF SYMPOL-2019
2.1 Pre-processing of Data
Parameters of image acquisition are
important for coral reef size estimation.
Processing of Image frames is required for
enhancing the accuracy of detection after
training of neural network.
2.1.1 Image Acquisition
Underwater environment was captured at an
approximate height of 3.5 ft. by taking video
using GoPro hero 4 camera.
There are sand patches, fish and various
types of living coral reefs in this environment.
Images were taken using the OpenCV library in
Python to extract the frames from the video.
Extraction was done at 2 frames per second to
avoid overlapping the same images.
2.1.2 Image Enhancement
High frequency components in image are
enhanced by increasing the contrast of all
frames thus increasing detailing. Image frames
are then Histogram Equalized for red, blue and
green component before features are extracted
from them.
2.2 Detection using Processed data
Using convolution neural network
architecture, sand patches and coral reefs are
detected. It has been found that MobileNet is
best suited for underwater imaging [1]. The
MobileNet model is based on depth-separable
convolutions, a form of factorized convolutions
that factor a standard convolution into a
depthwise convolution and a convolution called
a pointwise convolution. The standard
convolution layer is parameterized by the
convolution kernel, where spatial dimension of
the kernel is supposed to be square with number
of input channels and N number of output
channels.
The architecture for MobileNet is defined in
Table 1. ReLU nonlinearity follows all layers
except the final fully connected layer that has no
nonlinearity and feeds into a classification
softmax layer. Down sampling is handled in
both the depth convolutions and the first layer
with strided convolution. A final average
pooling in front of the fully connected layer
reduces the spatial resolution to 1.
(a) (b)
Fig. 2. Different convolution layers. (a)
Standard convolution layers. (b)
Depthwise separable convolution
layers.
3. Flowchart of Coral reef Size
Estimation
Figure 3 shows the detailed flowchart of
size estimation of coral reef. Normalized
coordinates are pixel values of detection boxes
in an image frame.
3.1 Detailed Steps of Size estimation
i. Normalized pixels obtained are
converted to absolute pixels.
ii. Pixels covered by object in length and
width is found using the information of
absolute pixel.
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:31:26 UTC from IEEE Xplore. Restrictions apply.
A.Maurya et al .:Vision Based Underwater Environment Analysis: A Novel Approach to Estimate Size of Coral Reefs 177
iii. Sigmoidal 5PL function (Equation 1) is
applied on the values of Pixel per metric
with different distance.
iv. The function is optimized and values of
constants are obtained such that curve is
fitted accurately.
v. Accurate Pixel per metric ratio is
obtained for desired Distance.
vi. Size of object is obtained using the
information of pixels covered and pixel
per metric obtained using Equation 1.
Fig. 3. Detailed flowchart of size
estimation of coral reef
4. Coral Reef Size Estimation
In Underwater environment, area covered
by coral reefs are estimated by estimating the
size and area of each detected coral patch
using information of pixels taken by patch and
pixel per metric ratio.
4.1 Using Pixel per Metric Ratio
After coral reef detection, neural network
coordinates were used to create a bounding
box around coral detected. Standardized
coordinates are changed to absolute pixel
values by multiplying coordinates with size of
image given as input to neural network for
training. ‘X’ coordinate is multiplied by ‘400’
and ‘Y’ coordinate is multiplied by ‘300’.
Number of pixels occupied by the coral’s
length and breadth is found using set of
absolute pixel values. Different samples were
taken with different distances, from an object
underwater and the pixel values are obtained.
Using pixel values and object size in cm scale,
pixel per metric ratio (number of pixels in one
cm) is calculated.
4.2 Formulation and Optimization
Using the obtained Pixel per Metric values,
a nonlinear function is formulated and
optimized accordingly as shown in Figure 4.
Fig. 4. Variation of pixel per metric
ratio with distance.
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:31:26 UTC from IEEE Xplore. Restrictions apply.
178 ©2019 IEEE PROCEEDINGS OF SYMPOL-2019
Formula used for Fitting:
y = 􀭢􀬾(􀭟􀬿􀭢)
(􀬵 􀬾 (􀱮
􀱙) 􀱘)􀱣 (1)
The Five Parameter Logistic Regression or
5PL nonlinear regression model is widely used
for curve-fitting analysis [4]. It is defined by the
classic "S" or sigmoidal form that matches the
curve's bottom and top plateaus, the EC50 and
the slope factor (the slope of Hill). This curve is
symmetrical about its point of inflection. The
5PL formula adds an additional parameter to
expand the model to accommodate curves that
are not symmetrical, quantifying the
asymmetry. The 5PL equation parameters are:
A = Minimum asymptote. In a bioassay
where you have a standard curve, this can be
thought of as the response value at 0 standard
concentration.
B = Hill's slope. The Hill's slope refers to the
steepness of the curve. It could either be
positive or negative.
C = Inflection point. The inflection point is
defined as the point on the curve where the
curvature changes direction or signs.
D = Maximum asymptote. In a bioassay
where you have a standard curve, this can be
thought of as the response value for infinite
standard concentration.
M = Asymmetry factor. When M=1 we have
a symmetrical curve around inflection point and
so we have a four-parameter logistic equation.
Obtained value after optimization:
a=20.7794; b= 32.8038; c= 10.5057; d=
0.023534; m= 0.0276115.Real object size is
obtained using Equation 2.
Object size(cm) = Object size(pixels)
Pixels per metric
(2)
5. Result and Discussions
Dataset for images was created by
extracting the image frames from underwater
videos taken with known height. Results of
training of Neural Network and testing of it
were obtained as follows:
 Total Images in Dataset – 4000.
 Training Set- 70%, Testing Set- 15%
 Validation Method- Revalidation,
 Validation Set - 15%,
 Average Training accuracy of images
is 100%.
 Average Test accuracy of images is
93%
M. Marcos et al. [2] used feedforward
backpropagation neural network to classify
close-up images of coral reef components into
three benthic categories: living coral, dead
coral and sand, reaching a success rate of
86.7% while their two-step classifier, on the
other hand, reached 79.7%. A. Shihavuddin et
al. [3] achieved overall accuracy of 85.5
percent on the dataset of Moorea-labelled
corals (MLC). The network used to train our
dataset outperformed the coral reef detection
techniques implemented in the past.
For training the convolution neural network
Google Collab was used. It uses 1xTesla k80
GPU having 2496 CUDA cores having 12 GB
GDDR5 VRAM. It uses a 1x single core hyper
threaded i.e. (1 core, 2 threads) Xeon
processors @ 2.3 GHz CPU. It has a 12.6 GB
RAM. It took 2 hours to train the network.
Figure 5 shows detection of coral reef
patches from an underwater image frame.
Starting from top first patch is detected with
96% accuracy, with another two patch detected
as 93% and 90% respectively. Fig. 6 shows
the detected coral reef from a different data
set. It had an accuracy of 95% signifying that
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:31:26 UTC from IEEE Xplore. Restrictions apply.
A.Maurya et al .:Vision Based Underwater Environment Analysis: A Novel Approach to Estimate Size of Coral Reefs 179
our trained model is accurate.
Fig. 5. Coral reef detection
Fig. 6. Elkhorn coral detection
Fig. 7. Coral reef detection
Fig. 8. Table coral detection
Figure 7 shows the detection of Brain corals
underwater image frame and size estimation of
area covered by coral reef is shown in Table 1.
First Patch shows detected reef with an
accuracy of 95%, and second patch is detected
with accuracy of 85%. Its Standard size ranges
upto 1.8 m or 5.9 ft. After applying the
proposed size estimation algorithm, the size is
estimated as 1.72 m, which verifies our result
correctly.
Figure 8 shows the detected Table corals,
species of Acropora genus which has standard
width upto 2.2 m. With the detection of table
coral in Fig. 8 its size is estimated as 2.48 m,
after applying the algorithm which validates our
result.
Table 1 shows the analysis of coral reef
detected in an underwater image frame, in
terms of numerical values to find out the area
covered by detected coral reef patches, in real
life scenario.
On the scale of 1920 pixels lengthwise and
1080 breadthwise, values in Pixels Taken
column gives the number of pixels taken by
the detected coral reef patch. Using algorithm
with other known parameters such as pixel per
metric, size of coral is estimated (length and
width). Area covered under coral reef is
estimated by the product of length and width
of detected coral reef patch. In this table reef
no. 1 and 2 are from Figure 7 and reef no. 3 is
from Figure 8.
Table 2 shows the comparison between the
average estimated length and actual length of
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:31:26 UTC from IEEE Xplore. Restrictions apply.
180 ©2019 IEEE PROCEEDINGS OF SYMPOL-2019
corals in underwater system. Average
Percentage Error is found to be minimal after
comparison.
Table 1. Estimated Size of Coral Reef
Table 2. Estimated length of coral vs actual
length
Coral types
Average
Estimated
length (m)
Actual
length
(m)
Average
Percentage
Error (%)
Brain coral
(Figure 7) 1.75 1.8 2.77 %
Table coral
(Figure 8) 2.31 2.2 5%
6. Conclusion
Coral reef analysis in 400×300 size image
frame is performed at 2 fps from underwater
environment video for each second image from
the extracted frames. In MobileNet architecture,
detachable convolution is done before giving
features to fully connected layers to reduce the
complexity of extraction of features. Detected
coral reefs provide relevant information about
the pixels occupied longitudinally and
breadthwise in an image used to estimate the
size. Non-linear function, mapping the pixel
values to the metric distance ratio is optimized
to give the desired distance near accurate ratio
value. The observations obtained using this
algorithm provides the reliable values of the
area covered in an image frame by the coral
reefs.
ACKNOWLEDGEMENT
It is a great pleasure to acknowledge my
deepest thanks and gratitude to Prof. Pramod
Maurya, scientist of National Institute of
Oceanography. I express my sincere heartfelt
gratitude to Dr. Anirban Chatterjee from NIT
Goa and all the members of NIT Goa, who
helped me directly or indirectly during this
course of work.


&&&&&&&8.Automatic Annotation of Coral Reefs
using Deep Learning
A. Mahmood∗, M. Bennamoun∗, S. An∗, F. Sohel†, F. Boussaid∗, R. Hovey∗, G. Kendrick∗ and R.B. Fisher‡
∗The University of Western Australia, Australia; email: ammar.mahmood@research.uwa.edu.au
†Murdoch University, Australia
‡University of Edinburgh, Scotland
Abstract—Healthy coral reefs play a vital role in maintaining
biodiversity in tropical marine ecosystems. Deep sea exploration
and imaging have provided us with a great opportunity to look
into the vast and complex marine ecosystems. Data acquisition
from the coral reefs has facilitated the scientific investigation
of these intricate ecosystems. Millions of digital images of
the sea floor have been collected with the help of Remotely
Operated Vehicles (ROVs) and Autonomous Underwater Vehicles
(AUVs). Automated technology to monitor the health of the
oceans allows for transformational ecological outcomes by
standardizing methods for detecting and identifying species.
Manual annotation is a tediously repetitive and a time consuming
task for marine experts. It takes 10-30 minutes for a marine
expert to meticulously annotate a single image. This paper aims
to automate the analysis of large available AUV imagery by
developing advanced deep learning tools for rapid and large-scale
automatic annotation of marine coral species. Such an automated
technology would greatly benefit marine ecological studies in
terms of cost, speed, accuracy and thus in better quantifying the
level of environmental change marine ecosystems can tolerate.
We propose a deep learning based classification method for coral
reefs. We also report the application of the proposed technique
towards the automatic annotation of unlabelled mosaics of
the coral reef in the Abrolhos Islands, Western Australia.
Our proposed method automatically quantifies the coral
coverage in this region and detects a decreasing trend in coral
population which is in line with conclusions by marine ecologists.
Index Terms—corals, deep learning, marine images, classification,
marine ecosystems.
I. INTRODUCTION
The automatic annotation of marine images allows for transformational
ecological outcomes by standardizing methods
to detect and identify species. It frees up experts from the
tediously repetitive task of manual annotation. It also enables
rapid and accurate processing of massive datasets. An ever
expanding human activity coupled with climate change have
severely damaged marine ecosystems, which play a key role
in our planet’s ability to sustain life. Yet accurate automated
technology to monitor the health of our oceans exist only on a
limited scale. Marine scientists still have to manually annotate
a massive amount of raw underwater imagery. This research
aims to address this bottleneck by developing advanced deep
learning and computer vision based models for automatic
annotation of imagery from coral reefs.
Rapidly increasing carbon dioxide levels in the atmosphere
due to ever expanding human activities are posing severe
threats to marine ecosystems in general [1] and coral reefs
in particular [2], [3] and [4]. Increased water temperatures are
thought to be responsible for bleaching and death of corals [2].
Some coral species are in danger of extinction due to these
adverse effects of pollution, industrial fishing and exploitation
of marine resources. This has resulted in a dramatic decline
in our planet’s marine biodiversity [5]. Today’s underwater
video cameras mounted on AUVs are an excellent alternative
to trawl nets, grabs and towed video surveys for remote
monitoring of marine ecosystems as they sample along a preprogrammed
flight path, producing geo-referenced imagery of
the sea-floor [6]. However, analysing raw imagery to extract
useful information is not only labour intensive, but it also
requires an expert to manually process each image. Typically
less than 2% of the acquired imagery ends up being manually
annotated by a marine expert, resulting in a significant loss of
information [7]. An accurate automatic annotation of marine
imagery would enable automatic counting, sizing and movement
tracking of specific marine organisms. Computer vision
and machine learning based techniques [8] have the potential
to automate the annotation of marine images and also reduce
the time consumed in manual processing. The accuracy of
these techniques depends on the availability of high quality
expertly annotated training and testing data.
Convolutional neural networks (CNNs) [9], also known as
deep networks, are an important class of machine learning
algorithms applicable, among others, to numerous computer
vision problems. Deep CNNs, in particular, are composed of
multiple layers of processing involving linear as well as nonlinear
operators. To solve a particular task, the parameters of
networks are learned in an end-to-end manner. Image representations
extracted from deep CNNs trained on a large dataset
such as ImageNet [10] have shown to produce a promising
performance for diverse classification and recognition tasks
[11], [12], [13], [14] and [15]. Spatial pyramid pooling (SPP)
[16] and Multi-scale Orderless Pooling (MOP) [17] schemes
have made CNNs independent of the input image size and
robust for diverse classification and recognition applications.
In this paper, we propose a computer vision and deep
learning based framework for the automatic annotation of
unlabelled coral images. This framework is based on a novel
coral classification algorithm, which employs the powerful
image representations of CNNs. Since we do not have ground
truth labels for millions of coral reef images, a human expert
978-1-5090-1537-5/16/$31.00 ©2016 IEEE
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:32:38 UTC from IEEE Xplore. Restrictions apply.
Fig. 1: Block diagram of our proposed framework.
is included in the loop to corroborate the accuracy of the proposed
classification method. With the trained coral classifiers,
we analyse the coral reefs of the Abrolhos Islands which form
one of Western Australia’s unique marine areas. We analyse
unlabelled coral mosaics of three sites of this coral reef from
two years.
The main contributions of this paper include: (1) a method
to learn features using a CNN for coral reef classification; (2)
automatic annotation of unlabelled coral images and mosaics
from the Abrolhos Islands in Western Australia; (3) coral
population analysis for these mosaics.
II. RELATED WORK
In 2010, Collaborative and Automated Tools for the Analysis
of Marine Imagery and Video (CATAMI)[18] was initiated
in Australia. It introduced a new classification system to ensure
that consistent names are given to the marine species seen in
underwater images. However, this system does not actually
automate the data analysis. It just streamlines the project by
facilitating manual data entry and provides a standard protocol
for assigning ground truth labels. Previous research ([7], [19],
[20], [21] and [22]) have highlighted the potential of using
computer vision based techniques for the automatic annotation
of benthic data. However, this is an uphill task given the factors
such as changing water turbidity, ambiguous class boundaries
and underwater color degradation.
III. PROPOSED METHOD
The proposed method is outlined in Fig. 1. The training
image set consists of images from multiple locations in
Western Australia, a subset of Benthoz15 dataset [23]. These
images are used to train a deep network which then classifies
unlabelled images and mosaics. Marine experts are included
in this pipeline to give feedback on the classification accuracy.
The best performing classifier is then used to generate coral
maps from the mosaics of the Abrolhos Islands. Next, we
explain the key components of the proposed method in the
following subsections.
A. Classification Process
Image representations extracted from deep neural networks,
trained on large datasets such as ImageNet [9] and fine tuned
on domain specific datasets, have shown state-of-art performance
in numerous image classification problems [14]. The
activation vectors of the first fully connected layer of a pretrained
VGGnet [24] are employed as feature representations
in our work. The weights of this deep network are fine tuned
using the Benthoz15 dataset [23] which consists of expertannotated
and geo-referenced marine images from Australian
seas.
It is a common practice in marine imagery to annotate the
images with pixel labels. Each training image has 50 pixels
marked with corresponding ground truth labels. State-of-art
deep learning architectures take an input image of fixed size
and hence image or patch ground truth labels are required. To
overcome this bottleneck, square patches were extracted with
the labelled pixel at their centre. There is no restriction on the
size of these patches. Instead of using the whole image for
training, we extracted patches at multiple scales centred around
the given labelled pixels. We achieved higher classification
accuracy when multi-scale patches were used instead of just
one fixed size. This technique is termed as spatial pyramid
pooling (SPP) [16]. This patch extraction method makes the
resulting features scale invariant. A 2-layered neural network
was then used to classify corals from non-corals. More details
on the classification process are given in our previous work
[25].
B. Unlabelled Mosaics and Coral Maps
The unlabelled images and mosaics from the Abrolhos
Islands were annotated with the best performing trained coral
classifier. We analysed mosaics of three different sites of the
Abrolhos Islands spanning an area of 625 sq. meters each for
years 2010 and 2013. Fig. 2 shows the path followed by the
Sirius AUV [23] to capture the coral reef and some sample
images. A marine expert was added in the loop to validate the
labels assigned by this classifier. After validation, the coral
mosaics of each site were analysed to investigate the changes
in the coral population. We focused on generating coral maps
for these sites to investigate the health of coral population for
each site over a period of three years. These coral maps were
automatically generated by our classifier and provide useful
insight for quantifying the population changes of the reef.
Marine experts were included in the pipeline to corroborate
and comment on the authenticity of these maps.
IV. EXPERIMENTS AND RESULTS
A. Benthoz15 Dataset
This Australian benthic data set (Benthoz15) [23] consists
of an expert-annotated set of georeferenced benthic images and
associated sensor data, captured by an autonomous underwater
vehicle (AUV) around Australia. The whole dataset contains
407,968 expert labelled points, on 9,874 distinct images collected
at different depths from nine sites around Australia
over the past few years. There are almost 40 distinct class
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:32:38 UTC from IEEE Xplore. Restrictions apply.
Fig. 2: The path traversed by the Sirius AUV over the 3 years near the Abrolhos Islands in WA and sample images. Latitude=28
degrees 48 minutes South. Longitude = 113 degrees and 57 minutes East.
Site Survey Year # of Labels # of Images
Abrolhos Islands 2011, 2012, 2013 119,273 1,377
Rottnest Island 2011 63,600 1,272
Jurien Bay 2011 55,050 1,101
TABLE I: WA subset of Benthoz15 in numbers.
labels in this dataset which make it quite challenging. Table. I
details some statistics of the Western Australia (WA) subset of
this dataset. We have used a subset of this dataset containing
images from Western Australia (WA) to train our classifier.
This subset consists of 4,750 images with 237,500 expertannotated
points collected over a span of 3 years (2011 to
2013).
B. Pre-processing
We applied color channel stretch on each image in the
dataset. We calculated the 1% and 99% intensity percentiles
for each color channel. The lower intensity was subtracted
from all the intensities in each respective channel and the
negative values were set to zero. These intensities were then
divided by the upper percentile. The resulting intensities
achieved a better performance compared to the original ones.
C. Classification Experiments and Results
Selecting patch sizes that give the best classification accuracy
is an important step. We trained our classifier using
multiple patches at different scales and achieved the best
performance when these three patch sizes were used: 28×28,
224×224, and 448×448. These correspond to small, medium
and large scales. Feature extraction at different sizes insures an
efficient encoding of coral species independently of their size.
28x28
224x
224
448x
448
VGGnet
Max-pooling
Spatial
Pyramid
2 Layer MLP
Coral
Non-
Coral
Fig. 3: Block diagram of the proposed classification method.
The image representations extracted at these three scales were
then max-pooled to retain the most prominent information
which is present in the neighbourhood of a labelled pixel.
These multi-scale deep features were used to train a Multi
Layer Perceptron (MLP) network for classification. This network
consists of two fully connected hidden layers of neurons
followed by an output layer with 2 nodes: corals and noncorals.
The number of neurons in the hidden layers were
optimized for maximum performance. Fig. 3 shows the block
diagram of our proposed classification method.
We conducted three experiments to evaluate our classifier:
(i) the classifier was trained on two-thirds of the images from
the year 2011 and tested on the remaining images from the
same year, (ii) the images from year 2011 were used for
training and the images from 2012 and 2013 constitute the
test set, (iii) the training set consisted of two-thirds of the
images from the years 2011, 2012 and 2013, whereas the test
set consists of all the remaining images from the same years.
Table. II shows the details and reports the preliminary results
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:32:38 UTC from IEEE Xplore. Restrictions apply.
Experiment # of Training Samples # of Test Samples Accuracy
Exp 1: Train and test
on 2011 108,000 53,000 97.00%
Exp 2: Train on 2011 and
test on 2012 and 2013 108,000 130,000 92.45%
Exp 3: Train and test on
2011,2012 and 2013 157,173 80,750 95.33%
TABLE II: Overall classification accuracies for different
experiments
Site Coral Coverage in 2010 Coral Coverage in 2013
1 95% 79%
2 82% 53%
3 96% 74%
TABLE III: Coral coverage of three sites of the Abrolhos
Islands for years 2010 and 2013.
of coral classification on Benthoz15 dataset. We achieved a
classification accuracy greater than 90% in all of our experiments.
The best performance is achieved when the training and
testing sets contain images from the same year. The performance
dropped when the experiments were done across
multiple years. This illustrates the difficulty encountered when
the training and test set have images from different years.
The main reason being the changes occurring in the coral
reefs with time. The major causes of misclassification were:
the ambiguous boundaries between corals and non-corals,
dead corals (non-coral species start covering corals) and the
abundance of non-coral labels in the dataset.
D. Coral Population Analysis
For the coral population analysis of the Abrolhos Islands,
we automatically annotated the unlabelled mosaics using our
best classifier. Outputs were validated by a marine expert
as ground-truth labels were not available. Coral cover maps
were then generated using the best performance classifier
for years 2010 and 2013, and percentage coral cover was
calculated for each site and year. Results of this analysis
reveal a decline in coral cover at all three from 2010 to
2013. This loss of corals was expected as an acute warming
event occurred in 2011 which resulted in significant coral
bleaching. Importantly, the magnitude of decline reported here
is comparable to those previously reported across a similar
time period for the Abrolhos Islands from imagery annotated
by marine experts, with an average decline in coral cover
from 73% to 59% across multiple sites [3]. The ability to
efficiently report coral response to particular impacts (such as
intense warming events) or gradual environmental change, is
crucial for implementing appropriate management strategies
[4]. Our initial results indicate that the combination of AUVs
and automated image analysis have the capacity to improve our
efficiency of transferring information to managers and policy
makers.
V. CONCLUSION
In this work, we applied pre-trained CNN image representations
extracted from VGGnet to a coral reef classification
problem. We investigated the effectiveness of our trained classifier
on unlabelled coral mosaics of the Abrolhos Islands. We
analysed the coral reef of the Abrolhos Islands to investigate
the trends in coral population. We generated coral maps for
this region and quantified the coral population automatically.
Our framework detected the decreasing trend in the coral
population of this region as well. The proposed framework is
an important step towards investigating the long-term effects
of environmental change on the effective sustenance of marine
ecosystems automatically.
ACKNOWLEDGMENT
This research was partially supported by Australian Research
Council Grants (DP150104251 and DE120102960) and
the Integrated Marine Observing System (IMOS) through the
Department of Innovation, Industry, Science and Research (DIISR),
National Collaborative Research Infrastructure Scheme.
The authors also acknowledge NVIDIA for providing a Titan-
X GPU for the experiments involved in this research.



&&&&&&&9.Surveillance of Coral Reef Development Using an
Autonomous Underwater Vehicle
Mohammad Fahmi Amri Bin Mohd Murad
Centre for Artificial Intelligence & Robotics,
Universiti Teknologi Malaysia,
Kuala Lumpur, Malaysia
fahmiamri92@gmail.com
Mohamed Idzham Bin Samah
Centre for Artificial Intelligence & Robotics,
Universiti Teknologi Malaysia,
Kuala Lumpur, Malaysia
mohamedidzham@gmail.com
Zool H Ismail
Centre for Artificial Intelligence & Robotics,
Universiti Teknologi Malaysia,
Kuala Lumpur, Malaysia
zool@utm.my
Karl Sammut
Centre for Maritime Engineering
Flinders University
Adelaide, Australia
karl.sammut@flinders.edu.au
Abstract—With the arrival of new robotic technologies
such as an Autonomous Underwater Vehicle (AUV), a
number of tasks that cannot be done by human now can be
accomplished and bring human to reach the dangerous
and unreachable areas. An AUV can be related to the most
invention which can reduce human’s work using
specialized equipment and devices that control and
perform the particular tasks. Therefore, it is able to fulfil
the predetermined task without any human control and to
react against unexpected situation by its artificial
intelligence. This paper examines the effectiveness of using
AUV as surveillance system for coral reef ecology. We
propose a new AUV that ease researcher or activist to
survey the condition of coral reef ecology environment.
Keywords—AUV; Coral Reef; morphological; ROS;
I. INTRODUCTION
Nowadays, technology of robotics expands in all area of
engineering and environment expertise which focuses on an
Autonomous Underwater (AUV). Recently, underwater
vehicle has been used in limited number of task. With further
research and development, underwater vehicle can reduce the
limitation and increase the capability to do more tasks. There
are lots of AUVs with different function and tasks operated
worldwide. Today, most of Autonomous Underwater Vehicles
(AUV) can be seen in military field, oiled and gas industry
and some of the expedition to explore under the deep sea.
However, this AUV's project focuses on an inspection of the
coral reef ecology which is on environment expertise. Coral
reefs are an important natural source which can be found in a
tropical water throughout the world [1]. The condition of coral
reef can be maintained from time to time by developing an
AUV.
Currently, the existing AUV used to explore a coral reef is
still an expensive technology to be developed by researcher
throughout the world. It can be said that it is still an expensive
technology to be developed by researcher throughout the
world. From this reason, this project proposes a low cost AUV
which allows the researchers and environmental activist to use
for their coral reef surveillance and exploration.
II. THE GOAL OF THE AUV PROJECT
This paper will describe the suitable design and affordable
autonomous underwater vehicle for coral reef surveillance and
overcome human limitation. In the end of this project, an
AUV can be operated as surveillance system for researchers or
environment activists.
III. KEY DESIGN PROPOSAL
To build an AUV, budget and technology need to be
planed carefully. In this paper we target to ease researcher and
activist by produce affordable autonomous underwater vehicle
and yet have the same function as existing autonomous
underwater vehicle that operate on coral reef sector in the
world.
A. Mechanical Design
To build an Autonomous Underwater Vehicle that can
tackle harsh environment and reach beyond human limit a
toughest and high manueverability AUV are needed [2]. Our
imaginary AUV will have size about 125cm long, 42cm width
and 18cm height. Those dimension is sizeable as our AUV
need to carry sensing and navigation instruments. This AUV
uses a ‘torpedo’ type design which can moves up to 5 degree
of freedom. As shown in our schematic design, our AUV
design based on torpedo shape to make it easy to maneuver on
challenging environment which human cannot arrive on that
area. 5 small-yet-powerful thrusters from BlueRobotic allow
the AUV moves in all area of the reef.
978-1-5090-2442-1/16/$31.00 ©2016 IEEE 14 AUV 2016
SP3
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:35:18 UTC from IEEE Xplore. Restrictions apply.
1386 IEEE JOURNAL OF OCEANIC ENGINEERING, VOL. 45, NO. 4, OCTOBER 2020
Peer-Reviewed Technical Communication

&&&&&&&10.Development of an Efficient Coral-Coverage Estimation Method Using a Towed
Optical Camera Array System [Speedy Sea Scanner (SSS)] and
Deep-Learning-Based Segmentation: A Sea Trial
at the Kujuku-Shima Islands
Katsunori Mizuno , Kei Terayama, Shigeru Tabeta, Shingo Sakamoto, Yoshinori Matsumoto, Yusuke Sugimoto,
Toshihiro Ogawa, Kenichi Sugimoto, Hironobu Fukami, Masaaki Sakagami, Mayumi Deki, and Akihiro Kawakubo
Abstract—Various methods have been developed and used for monitoring
marine benthic habitats, such as coral reefs and seagrass meadows.
However, the efficiency of general survey methods [e.g., line intercept
transects and autonomous underwater vehicles (AUVs)] still is not high.
In this article, we propose a practical coral-coverage estimation method
combining an effective survey system [Speedy Sea Scanner (SSS)] and a
deep-learning-based estimation method. The SSS is a towed-type system
with six cameras arrayed on the platform. The depth rating of the system
in our trial was 50 m. The length of the array baseline was 4.4 m, and
six cameras were placed on the platform with equal spacing. The sea
trial was conducted at Kujuku-Shima, Japan, on September 30, 2017. We
successfully generated 3-D models and high-quality orthophotos of the
seafloor with high resolution of about 1.5 mm/pixel. The survey efficiency
of the SSS was about 7000 m2/h. In addition, the experimental results of
coral-coverage estimation showed that the corals can be distinguished with
accuracy of about 80% in places with relatively high transparency, and the
error of coverage estimation was 10% or less. The proposed coral-coverage
estimation method is more efficient than other survey techniques and costs
less than AUV surveying; therefore, it is expected to become a promising
tool for marine environmental surveying.
Manuscript received August 30, 2018; revised April 1, 2019 and July 16,
2019; accepted August 27, 2019. Date of publication October 8, 2019; date
of current version October 13, 2020. This work was supported in part by the
Aid for Young Scientists A (17H04974), in part by the Japan Society for the
Promotion of Science (JSPS), in part by theACT-I Japan Science andTechnology
Agency (JSPS) under Grant JPMJPR16UJ, and in part by the Kurita Water and
Environment Foundation. This work was presented in part at the MTS/IEEE
OCEANS Conference, Kobe, Japan,May 28–31, 2018. (Corresponding author:
Katsunori Mizuno.)
Associate Editor: J. Cobb.
K. Mizuno and S. Tabeta are with the Department of Environment Systems,
Graduate School of Frontier Sciences, The University of Tokyo, Kashiwa 277-
8561, Japan (e-mail: kmizuno@edu.k.u-tokyo.ac.jp; tabeta@edu.k.u-tokyo.
ac.jp).
K. Terayama is with the RIKEN Center for Advanced Intelligence Project,
Chuo-ku 103-0027, Japan, and alsowith theGraduate School of Medicine,Kyoto
University, Kyoto 277-8501, Japan (e-mail: terayama.kei.8e@kyoto-u.ac.jp).
S. Sakamoto, Y. Matsumoto, Y. Sugimoto, T. Ogawa, and K. Sugimoto
are with the Windy Network Corporation, Shizuoka 422-8006, Japan (e-mail:
s-sakamoto@windy-net.co.jp; y-matsumoto@windy-net.co.jp; y-sugimoto@
windy-net.co.jp; ogawa@windy-net.co.jp; sugimoto@windy-net.co.jp).
H. Fukami is with the Faculty of Agriculture, University of Miyazaki,
Miyazaki 889-2192, Japan (e-mail: hirofukami@cc.miyazaki-u.ac.jp).
M. Sakagami is with the Graduate School of Human and Environmental
Studies, Kyoto University, Kyoto 606-8501, Japan (e-mail: sakagami.masaaki.
6x@kyoto-u.ac.jp).
M. Deki and A. Kawakubo are with the Saikai National Park Kujukushima
Aquarium, Sasebo 858-0922, Japan (e-mail: m-deki@pearlsea.jp;
a-kawakubo@pearlsea.jp).
Digital Object Identifier 10.1109/JOE.2019.2938717
IndexTerms—Coral, deep learning, optical camera array, structure from
motion (SfM).
I. INTRODUCTION
CORALS play a fundamental role in primary production and
habitat formation for other species [1], [2]. To monitor coral
environments, efficient and comprehensive survey methods are indispensable.
Various methods have been developed and used for monitoring
benthic marine habitats such as coral reefs. Generally, field
transects, such as line intercept transects, photo line intercept transects,
and video transects, are the most widely used methods as they are
easy and simple to conduct as well as relatively inexpensive [3]–[6].
However, these in situ visual methods require long sampling times
because of their small-scale coverage. In addition, diving limits the
survey time based on air tank usage and poses varying degrees of risk.
Thus, marine biologists and ecologists have increasingly come to rely
on imagery from platforms such as autonomous underwater vehicles
(AUVs) or remotely operated vehicles for marine monitoring [7]–[12].
In the decade, the navigation ability of AUVs has been evolved and
the underwater imagery obtained by AUVs has been used to construct
large 2-D mosaic maps and/or high-quality 3-D models of the seabed,
and can be used to classify and count the abundance of various species
in a given area [7]–[10]. Recently, AUVs have been used to inspect
the environments for which no previous information was available
[11]. However, the development and maintenance costs of this method
are still high because the underwater vehicles, in general, need other
expensive equipment [e.g., underwater positioning system, Doppler
velocity logger (DVL), and motion sensor]. The cost for operation is
also high because expertise and vessel with adequate equipment are
usually required to operate these vehicles. In addition, AUV systems
usually use mono or stereo cameras for observations; therefore, the
coverage of the survey is limited by the small number of cameras.
As a common subject, coral subdivision and classification methods
based on clear recorded images obtained in tropical regions have also
been studied and investigated using techniques in the field of computer
vision [9], [13]–[15]. In recent years, recognition of the importance of
coral surveying in turbid water has been increasing. For example, it was
reported that, as sea surface, temperatures have risen and the distribution
of corals in temperate areas, especially in Japan, has been spreading to
more northern areas [16]. However, in seawater with low transparency,
the above methods are not directly applicable; most monitoring methods
have been developed for seawater with high transparency such as in
tropical regions, using vision-based methods with clear images in which
experts can classify corals.
0364-9059 © 2019 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See https://www.ieee.org/publications/rights/index.html for more information.
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:36:23 UTC from IEEE Xplore. Restrictions apply.
MIZUNO et al.: DEVELOPMENT OF AN EFFICIENT CORAL-COVERAGE ESTIMATION METHOD 1387
Fig. 1. Speedy Sea Scanner (SSS).
In this article, we address this problem by combining an effective
survey system [Speedy Sea Scanner (SSS)] that is applicable in turbid
water and a deep-learning-based estimation method. SSS is developed
to achieve low-cost and high-quality performance survey. The system
is a towed optical camera array system with 4.4-m array baseline.
Orthophotos can be reconstructed from recorded images using the
low-cost commercial software (Photoscan, Agisoft), which is based
on the structure from motion (SfM) method [17]. We also developed a
coral-coverage estimation method using a convolutional neural network
(CNN) [18] for the orthophotos. To demonstrate the effectiveness of
the proposed method, we conducted an experiment around the Kujuku-
Shima Islands, Nagasaki Prefecture, Japan.
II. METHODS
A. Towed Optical Camera Array [Speedy Sea Scanner (SSS)]
1) System Configuration: A towed optical camera array system
(SSS) (see Fig. 1) was newly developed as a tool for marine monitoring
[19]. The following is a list of the general characteristics of the SSS.
1) The cost for development and maintenance is lower than that for
underwater vehicles because other equipment (e.g., underwater
positioning system,DVL, and motion sensor) except for thewater
proof camera is unnecessary in the present system.
2) The survey efficiency is higher than that by diver and AUVs
methods. The large number of cameras can be installed on the
present system and the survey boat for the towing is able tomove
faster.
3) The operation is very simple. Turning on the power of cameras
and deploying the system by two or three adults into thewater are
required. It can be towed by the small boat (see Fig. 2). This easy
system will contribute to avoid the electronical and mechanical
problems.
4) The pair matching between the adjacent images for the image
mosaicking is robust. The system can gather the multiple images
in the across direction at the same moment with keeping the
certain overlap rate of images by the large number of cameras.
Especially, in the coastal survey, the light condition and water
transparency are very changeable in a short time and affect
the image quality because of the diurnal motion; therefore, the
images should be taken at the same time.
5) The system is very portable. It can be carried by a standard van
and easily deployed on the local survey area.
The depth rating of the system is 50 m. The length of the array
baselinewas 4.4 m, and six cameras (DC-GH5, Panasonic, with custommade
waterproof housing) were installed on the platform with equal
Fig. 2. Survey with small boat.
spacing. We determined the length to enable us to handle the system
by two adults and carry it by small boat to the survey area. The attitude
during the towing is stable by the tailplane and the tilt angle of the system
can be tuned by the attachment position of the rope for towing. Before
the field survey, we conducted some tests for the stable and safe towing.
As an option for surveying, six LEDs can be attached to the system;
however, we did not use them in this situation. The optical camera
recorded the full high-definition video with a recording rate of 23.98
frames/s. The systemwas towed by the survey boat, whichwas equipped
with a global positioning system (GPS; Crescent A100, Hemisphere
GNSS). The height from the sea bottom was set around 2–3 m, and
the speed of the survey boat was maintained around 2–3 kn during the
survey.Before the SSS survey, precise seabed topographywasmeasured
using multibeam sonar (Sonic 2024, R2Sonic LLC) for determination
of the survey line. During the survey, we used the seabed topography
and a fish echosounder (HDS-5,Lowrance) to help in keeping the survey
safe.
2) Data Processing: The data processing flow is shown in Fig. 3.
First, the GPS device and cameras were time-synchronized using GPS
time. Next, continuous still images were obtained from the video data.
We extracted four still images per second in this study.Color corrections
were performed on the images because the attenuation of light underwater
is very high and differs greatly from that in the air, especially in turbid
water such as coastal sites. A photograph for calibration of this color
correction was obtained before the survey. As shown in Fig. 4(a), the
water transparency offshore at Kujuku-Shima was unfavorable during
the survey (water transparency changes dramatically depending on
the season and weather conditions in this region). Therefore, color
correction was necessary before the SfM processing. We used a white
plate for color correction calibration as shown in Fig. 4. The raw data
show very high attenuation of light, especially for red light as shown in
Fig. 4(b). To improve the white balance of the images, the histograms
of each color were digitally shifted and stretched to be the almost same
averaged values at each color using the region of interest (see ROI:
red-dashed square in Fig. 4). Here, we obtained the coefficient for the
histogram transformation at each color and used them for the pixels
in other images. Several methods for the color correction have been
proposed, and the reader is referred to recent article by Li et al. [20].
The method used in this study is almost the same as the histogram
equalization method discussed in the article, however, another method
could have been chosen. The camera locations were estimated based on
GPS data and added to the corresponding still images. The GPS data
were up-sampled using the cubic spline interpolation method.Aportion
of the estimated camera locations is also shown in Fig. 2. In this case, the
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:36:23 UTC from IEEE Xplore. Restrictions apply.
1388 IEEE JOURNAL OF OCEANIC ENGINEERING, VOL. 45, NO. 4, OCTOBER 2020
Fig. 3. Data processing flow.
Fig. 4. Typical image and color histogram of a white board, obtained at the
survey site. (a) and (b) before color correction. (c) and (d) after color correction.
up-sampling ratewas ten times the number of original data points.A3-D
modelwas reconstructed from the continuous images using the low-cost
commercial software (Photoscan, Agisoft) based on SfM techniques.
SfM is a technique which utilizes a series of 2-D images to reconstruct
the 3-D structure [21], [22]. This technique can be used to create
high-resolution digital surface models and with consumer grade digital
cameras. The relatively new technique has been applied for the wide
range of surveyswith unmanned aerial systems [23]–[25]. In addition, it
has advantage of the 3-D models generation without extensive expertise
or expensive equipment (e.g., accurate positioning system and motion
sensor). Recently, the SfM technique has also been available for the
marine surveys [6], [17]. In this article, we applied and integrated
this technique with the SSS. From the generated 3-D model, a digital
elevation model and orthophoto can be produced. Finally, the output
data are handled using QGIS free software for practical use. Most of
this data processing is performed automatically in our original program.
B. Deep-Learning-Based Estimation Method
In recent years, various methods of object recognition and segmentation
using deep neural networks have been proposed [18]. For this
article, we adopted a CNN, which is a kind of deep neural network
commonly used for image recognition, and built a model to distinguish
between corals and other objects.We also employed evaluation metrics
to verify the accuracy of the proposed model.
1) Coral-Coverage Estimation Using Deep Learning: To
train our model and verify its accuracy, it is necessary to prepare labeled
data of coral regions for recorded orthophotos. The details and results of
coral labeling are described in Section III-B1.We trained the following
CNN-based network using the labeled images.
We constructed a pixel-wise estimation model of coral regions using
the CNN (see Fig. 5) based on an image segmentation method [18].
The input of the model is a local image of 128 × 128 pixels centered
on a certain pixel, and the output is the probability that the center is
a coral. In the model, each input image is resized to 32 × 32 and
then passed through five convolution layers with a rectified linear unit
(ReLu) as an activation function and two full connection layers. In
the convolution layers, we employed the batch normalization [26]
and dropout [27] methods, which are commonly used to improve
generalization performance and stabilize learning of neural network
models.
To train the network, we added horizontally and vertically flipped
training images to the training data as data augmentation, because flip
has been shown to be effective in image recognition and classification
using CNN [28], [29]. The weights of the network were updated by the
Adam optimizer [30] with a learning rate 0.001. The maximum number
of epochs and the batch size were set to 20 and 100, respectively. It
was confirmed that the maximum number of epochs was sufficient, by
examining the relationship between training and validation losses and
the number of epochs using training and validation data in advance.
2) Coral-Coverage Estimation Using Hand-Crafted Descriptor:
To verify the usefulness of the CNN-based estimation
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:36:23 UTC from IEEE Xplore. Restrictions apply.
MIZUNO et al.: DEVELOPMENT OF AN EFFICIENT CORAL-COVERAGE ESTIMATION METHOD 1389
Fig. 5. Overview of coral region estimation based on the CNN.
Fig. 6. Seabed topography and survey lines.
method, we developed a method based on bag-of-visual-words (BoVW)
[31] using a hand-crafted image descriptor. To compare with the CNNbased
method, the BoVW-based method calculates image features
densely from the input local image, represents a histogram based on
the idea of BoVW, and finally outputs whether the center of the local
images is coral or not using a classifier.We adopted the speed up robust
features (SURF) [32],which is a rotation and scale invariant feature and
is one of the most widely used descriptors. As the classifier, we used
random forests (RFs), which is a nonlinear classifier and well adopted
as an excellent classifier with BoVW [33]. We built 256 clusters of
descriptors using the K-means algorithm for BoVW. The number of
trees and the maximum number of features when splitting a node,which
are hyperparameters of RFs, were optimized with grid search.We refer
to this method as SURF+RF.
3) Evaluation Metrics: We employed three evaluation metrics,
precision, recall, and the F-measure, to evaluate the accuracy of coral
regions predicted by the proposed method. Precision is the fraction
of manually labeled pixels among predicted coral pixels. Recall is the
Fig. 7. Typical images. (a) Before color correction. (b) After color correction.
fraction of the relevant pixels that are successfully predicted as corals
by the proposed method. For accurate prediction, both precision and
recall values should be high. The F-measure is the harmonic mean of
precision and recall as follows:
F − measure =
2 · Precision · Recall
Precision + Recall
.
To evaluate the estimation error for coral coverage,we also evaluated
mean absolute error (MAE) calculated by averaging the absolute errors
between the predicted and manual coral coverages.
C. Survey Site
A sea trial was conducted offshore Kujuku-Shima, Japan, on
September 30, 2017. The area of the Kujuku-Shima Islands, Nagasaki
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:36:23 UTC from IEEE Xplore. Restrictions apply.
1390 IEEE JOURNAL OF OCEANIC ENGINEERING, VOL. 45, NO. 4, OCTOBER 2020
Fig. 8. Orthophotos at various scales.
Fig. 9. Three-dimensional point cloud and XYZ data for part of survey line 3. (a) Point cloud and (b) XYZ data.
Prefecture, Japan, is close to the northern distribution limits of corals
[34] and is designated as a national park. More than 20 kinds of corals,
such as Dipsastraea speciosa and Acropora pruinosa, have been found
in this area, and live at depths of about 5–20 m. The seabed topography
and survey lines offshore Kujuku-Shima are shown in Fig. 6. The SSS
survey was conducted in an area with water depths of around 10–15 m.
In this survey, it seemed that the turbidity of the water was relatively
high, and this degree of turbidity changed dramatically depending on
the location. The survey time offshore Kujuku-Shima was about 55
min for four survey lines. Unfortunately, one camera (#6) did not run
because of a battery problem during the survey. Therefore, the data
from the remaining five cameras were used for data processing.
III. RESULTS AND DISCUSSION
A. Reconstructed Optical Map of the Seafloor
Typical images from before and after color correction are shown
in Fig. 7. The colors of the coral and stones were improved. After
color correction of all images, orthophotos of each survey line were
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:36:23 UTC from IEEE Xplore. Restrictions apply.
MIZUNO et al.: DEVELOPMENT OF AN EFFICIENT CORAL-COVERAGE ESTIMATION METHOD 1391
TABLE I
CORAL PREDICTION RESULTS OF THE CNN-BASED METHOD AND THE CLASSICAL METHOD
USING SURF DESCRIPTOR AND RFS (SURF+RF) FOR HTD AND LTD
Bold values indicate the method with higher accuracy for each evaluation metrics.
constructed. Because of computer memory limitations, the data for each
line were split into five or six blocks for processing. Finally, an optical
map of the seabed was generated by connecting the orthophotos in the
QGIS field (see Fig. 8). The optical map was constructed from a total
of 40 415 still images, and the coverage area was about 7016m2. In this
case, the survey efficiency of the SSS was about 7000 m2/h. According
to previous studies, the efficiency of surveying by divers or swimmers is
about 150 m2/h [5], [6] and that by AUV about 2470 m2/h at 2 m above
the seafloor [35].Although the system can output about 30 images/s,we
used only 4 images/s since it was adequate for making the mosaic map
in the present study. It means we can change the towed speed faster and
improve the survey efficiency within the limit of safety. Thus, using the
SSS, an optical map of the seabed can be obtained relatively efficiently.
In addition, the image quality was good with a resolution of about 1.5
mm/pixel. Therefore, we can use this optical map at various scales.
Fig. 8 shows optical maps with different scales at the same location.
In addition to the orthophotos, 3-D models were also available. A
representative 3-D model generated from the data from line #3 is shown
in Fig. 9. The precise seabed topography can be obtained from the
3-D model, and we can intuitively understand the current status of the
seabed. Such 3-D models are expected to become a useful tool to discuss
the current environment among stakeholders, including for introducing
marine surveying to beginners.
B. Coral-Coverage Estimation
1) Manual Labeling: We manually labeled the coral regions in
the orthophotos, as shown in Fig. 10. Red regions in Fig. 10(c) and
(d) represent corals in Fig. 10(a) and (b) extracted from line #3. Coral
coverage ratios in Fig. 10(c) and (d) were 0.491 and 0.644, respectively.
We refer to the coral coverage data set of (a) as a low-transparency data
set (LTD) and that of (b) as a high-transparency data set (HTD).
2) Evaluation of Deep-Learning-Based Segmentation: We
performed an evaluation of the proposed coral-coverage estimation
method using a CNN in our proceedings [36]. Here, we briefly report
the results. Table I summarizes the predictions and estimates of
coral coverage. For the first experiment, we performed training of the
proposed network and evaluation of coral prediction based on twofold
cross-validation for the LTD and HTD. The F-measure values for the
HTD and LTD were 0.846 and 0.753, respectively. For all the metrics,
precision, recall, and the F-measure, the results were higher for theHTD
than for the LTD. The predicted coverages in the LTD and HTD were
0.539 and 0.629, and the errors with respect to the manually measured
values were only 0.048 and 0.015, respectively. These results suggest
that coral prediction and coverage estimation are possible with high
Fig. 10. Typical orthophotos before and after labeling for line #3. (a) and (c)
LTD. (b) and (d) HTD.
accuracy if sufficient resolution is obtained as an HTD, and even in
low-transparency environments, as LTD coverage estimation may be
possible with practical accuracy. To verify whether the CNN-based
approach is superior to the existing method, we also show the coral
prediction and coverage estimation by SURF+RF. From the result
given in Table I, all prediction by CNN achieved higher accuracy than
SURF+RF, and thus this result shows the usefulness of the CNN-based
approach. In the second experiment, we conducted another prediction
experiment by training the proposed network with HTD or LTD and
tested the performance using the remaining data not used for training.
The accuracies (F-measures) of coral prediction were also between 0.7
and 0.8, and the errors (MAEs) in coral-coverage estimation were about
10%. These results suggest that our method using a CNN is practical
for coral prediction and coverage estimation.
3) Estimation of Coral Coverage in the Surveyed Area: We
calculated the overall coral coverage in the recorded area using the
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:36:23 UTC from IEEE Xplore. Restrictions apply.
1392 IEEE JOURNAL OF OCEANIC ENGINEERING, VOL. 45, NO. 4, OCTOBER 2020
Fig. 11. Estimated overall coral distribution.
Fig. 12. Distribution map of coral coverage: (a) 1-m grid; (b) 2-m grid;
(c) 4-m grid; and (d) 8-m grid.
proposed model trained by both the LTD and HTD data sets. Red
regions in Fig. 11(a) show the estimated coral regions. Fig. 11(c) shows
the details of the estimated coral regions in the same area as Fig. 11(b),
which corresponds to the red-outlined image in Fig. 8. Fig. 11(d) is the
estimated coral regions of line #3 in the green square in Fig. 11(a). The
estimated coverage ratios for lines #1, #2, #3, and #4 were 0.549, 0.521,
0.415, and 0.411, respectively. The estimated ratio in the surveyed area
overall was 0.475. In addition, the distribution map of coral coverage
ratio with multiscale grid sizes can be created from the estimated
results to quantify the coral distribution (see Fig. 12). The changes
of the coral coverage distribution can be clearly seen in this map. This
quantified map will be useful to assess the coral distribution with other
environmental factors (e.g., water temperature, turbidity, water quality,
water depth, and current) in the future survey.
This overall coverage ratio was not significantly different from the
survey results for hermatypic coral communities of Japan reported by
Sugihara et al. [37] using a line transect method [38], although it should
be noted that different survey methods and periods may affect the
estimation results. Our results indicated a relatively high coverage ratio
in high-latitude coral community areas, almost the same as that in the
area around the Iki islands. Moreover, the species composition of the
corals observed offshore Kujuku-Shima was similar to that around the
Iki islands.
IV. CONCLUSION
We have developed a new towed-type survey system for marine
monitoring. This system is more efficient than transect techniques
and less expensive than underwater vehicles surveying; therefore, it
is expected to become a promising tool for marine environmental
surveys with high cost effectiveness. In addition, the system still has
enough capacity for payload; therefore, other equipment (e.g., water
temperature recorder, turbidity recorder, water quality recorder, and
depth recorder) can be embedded on for more informative survey. As
previously stated, SSS has many advantages; however, this towed-type
platform is difficult to be used for the deep-sea survey with keeping
certain altitude. Furthermore, the towed platform needs preliminary
information of bathymetry map for the safe survey. In these cases,
the use of the underwater vehicles will be useful. We suggest that
the camera arrays be used with underwater vehicles with increased
survey efficiency. In addition, we have also developed an effective
coral-coverage estimation method using a combination of SSS and
deep-learning-based segmentation. From the results of our sea trial, it is
suggested that the proposed system effectively estimates coral coverage
even in areas of the sea where transparency is relatively poor.We believe
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:36:23 UTC from IEEE Xplore. Restrictions apply.
MIZUNO et al.: DEVELOPMENT OF AN EFFICIENT CORAL-COVERAGE ESTIMATION METHOD 1393
that this survey system will become a useful tool for quantitatively
investigating biological factors such as coral and seagrass meadow
distributions.
ACKNOWLEDGMENT
The authors would like to thank the staff of the Kuroshima Branch
of the Ainoura Fisheries Cooperative Association at Sasebo for their
support in the sea trial.
AUTHOR CONTRIBUTIONS STATEMENT
K. Mizuno and Y. Matsumoto proposed the idea for SSS design.
K. Mizuno and S. Sakamoto invented the data processing flow and
analyzed the experimental data. K.Mizuno, S. Sakamoto, Y. Sugimoto,
T. Ogawa, K. Sugimoto, A. Kawakubo strategized the idea for data
collection. K. Terayama proposed the idea for segmentation of coral,
implemented the proposed method, and analyzed the data. H. Fukami
and M. Deki identified the coral in the image and created the training
data for deep learning. All authors discussed the results. K.Mizuno and
K. Terayama wrote the entire article.



&&&&&&&11.Binocular Matching Method for Detecting and
Locating Marine Resources Using Binocular Cameras
and Deep Learning
Xiang Li1*, Yejun Kou1 and Yanchun Liang1
1Zhuhai College of Science and Technology, Zhuhai, Guangdong province, China
*lixiang@zcst.edu.cn
Abstract. It is well known that Artificial Intelligence and
underwater robots play a significant role in energy exploration.
This paper proposes a novel method using binocular cameras to
detect, locate, and track marine resources. The method combines
the YOLOv5 object detection method with the BM algorithm
binocular ranging method. The method then uses a neural network
to identify and detect the position of the target in the image,
matches the detection framework with the depth chart, and
employs a weighted average model to calculate the relative
distance between the target and the object detected by the camera.
Finally, underwater robots can use the relative distance to locate
and track marine resources.
Keywords- Binocular camera, YOLOv5, Deep learning,
Underwater vision
I. INTRODUCTION
Machine vision technology has entered a period of rapid
development and it is known as the “eye” of intelligent
manufacturing. Consequently, machine vision is being
extensively utilized in the field of intelligent manufacturing,
and there is closer integration with intelligent manufacturing.
As the oceans of the world continue to be developed rapidly,
underwater robot technology [1] has received unprecedented
attention and research as the most important means for human
exploration of the oceans. The research of underwater
technology has become one of the key focus points of high-tech
research, and intelligent underwater robots play a vital role on
efficient underwater work platforms in the development and
utilization of the oceans.
A. Research background
Using the parallax principle, a binocular camera was used to
match the horizontal distance of pixels in the center of the block
[2], and the relative distance was calculated. The further the
distance from the object, the smaller the parallax; on the
contrary, the closer the distance from the object, the greater the
parallax. The size of the parallax corresponds to the distance
between the object and the eye. The distance perception of the
target object by the binocular system can ensure accurate
measurements. In case obstacles are encountered, the binocular
camera can send an alert or stop as necessary based on the
changing distance information received. Therefore, binocular
ranging technology is widely used in underwater environments.
This study proposes a new scheme to attach a binocular
camera on an underwater robot (BlueROV2). A target detection
method based on YOLOv5 neural networks was combined with
a binocular ranging method to calculate parallax with binocular
matching to detect, locate and track marine life.
The dynamic and static detection performance was
enhanced by adjusting the detection block size. The average
error of dynamic monitoring was only 10mm in clean water and
13mm in muddy water. Also, the effectiveness of the
compensation function was verified by comparison.
II. MATERIALS AND METHODS
A. YOLOv5 Undersea Target Detection Method
1)Model introduction. The YOLO model, which was
described in a paper published by Joseph Redmon in
CVPR2016 [3], is a real-time target detection method that uses
a single neural network to predict bounding boxes and
probabilities directly from a complete image. The YOLOv5 is
an improved model released by Ultraytics in May 2020, and this
model fully benefited from the established Pytorch ecosystem
because it was implemented in the Pytorch framework. The
model weight file is only 27MB, which can be very easily
embedded in a modern device.
YOLOV5 network and general YOLO series algorithms
belong to the category of one-step detection algorithm. In terms
of data preprocessing, YOLOV5 follows the mosaic image
online enhancement method proposed by YOLOV4, aiming to
increase the number of small targets in a single batch, improve
the network's ability to identify small targets, and also increase
the data information in a single batch; In the backbone structure,
FPN (feature pyramid) structure is used to extract features from
the bottom up; In the neck structure, a top-down PAN (path
fusion) structure is used to shorten the path of the low level
feature flow to the prediction level; In the head structure and
prediction structure, the features are integrated, and the lowlevel
features of different receptive fields are fused through
three paths: prediction 1, prediction 2, and prediction 3. Finally,
the boundary box information and category information of the
defect target are output. The CSP layer is used to replace the
residual connection layer.
2) Method for Data Collection. The camera was placed in a
real underwater environment, and the primary data set was
collected through multi-angle photography and the underwater
video frame sampling method.
Underwater images are generally blurry. To solve this
problem, we propose a new and effective strategy to enhance
2023 4th International Conference on Computer Vision, Image and Deep Learning (CVIDL)
979-8-3503-2644-4/23/$31.00 ©2023 IEEE 381
2023 4th International Conference on Computer Vision, Image and Deep Learning (CVIDL) | 979-8-3503-2644-4/23/$31.00 ©2023 IEEE | DOI: 10.1109/CVIDL58838.2023.10167128
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:38:30 UTC from IEEE Xplore. Restrictions apply.
the image data as shown in Fig.1. Compared with the procedure
of adding extra network structures, the data enhancement
method is simple and effective. The experimental results also
showed that geometric transformation and optical
transformation effectively improved the model’s ability to
detect objects through the use of enhanced data.
The camera was placed in a real underwater environment,
and the primary data set was collected through multi-angle
photography and the underwater video frame sampling method.
Underwater images are generally blurry. To solve this
problem, we propose a new and effective strategy to enhance
the image data as shown in Fig.1. Compared with the procedure
of adding extra network structures, the data enhancement
method is simple and effective. The experimental results also
showed that geometric transformation and optical
transformation effectively improved the model’s ability to
detect objects through the use of enhanced data.
Figure 1. Enhanced data diagram
3) Results of underwater detection. In this experiment as
shown in Fig.2, Authors collected a small number of data sets
in order to verify the feasibility of the proposed method which
combines target detection with binocular ranging. Incremental
training was conducted on YOLOv5s.pt using the data
enhancement method which was described in the previous
paragraph.
Figure 2. Actual detection in an underwater environment
B. Binocular Camera Underwater Target Detection Method
1) The Fundamentals of Camera Calibration. The currently
used ranging methods that are representative of the field mainly
have the following characteristics: Uses a monocular camera
along with a laser receiver [4]. Although this method is simple
and can accurately collect data, the laser is still affected by the
strong absorption and scattering effects of the water. Therefore,
this method cannot be used in underwater environments.
Uses sonar to obtain real-time positioning underwater [5].
This method is widely used in submarines and underwater
detection devices. The commonly used sonar is the side-scan
sonar, and images can be added by drag-and-drop. The images
will be fuzzy in shallow water or in close-range situations
because of its long wavelength. 3D sonar, such as Coda
Octopus's Real-time 3D Echoscope, is expensive and requires a
strong method for processing the return signal.
Uses model-based pattern matching recognition techniques.
For this technique, a large amount of data needs to be measured
and collected on the features of different kinds of sea life, then
the relevant data is used to complete the pre-modeling
procedure. Although the matching performance is good,
significant calculation power is still required.
Uses a binocular stereo-matching method based on image
parallax [6]. This approach obtains parallax images by stereomatching
two monocular cameras. The 3D point cloud is
derived and a 3D image is constructed. Then the distance to the
target object is calculated according to the principle of similar
triangles. This is a low-cost method that only requires a
stereoscopic camera to capture the images. Compared with
other stereo-matching methods, the block matching method has
fast ranging speed and the level of accuracy satisfies the
application requirements.
2) Binocular Ranging Process. Monocular camera
calibration: The main purpose of camera calibration is to derive
a relationship between the camera coordinates in the ideal
coordinate system and the position and direction of the camera
in the World Coordinate System as shown in Fig.3.
Figure 3. Camera coordinates and world coordinates
As shown in Fig.4-5, based on the “Flexible Camera
Calibration by Viewing a Plane from Unknown Orientations”
method, the TOOLBOX from MATLAB was used to calibrate
and calculate the internal parameters of the monocular camera.
(a)
382
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:38:30 UTC from IEEE Xplore. Restrictions apply.
(b)
Figure 4. The internal parameters of the monocular camera:
(a) Calibrate with binocular camera (world-centered)
(b) Calibrate with monocular camera (camera-centered)
Figure 5. The relative position of the two cameras and the 28 vice boards is
shown in the figure: Binocular camera calibration diagram
After completing the separate calibration of the left and right
monocular cameras, the next step was to use “TOOLBOX_calib”
from Calibration TOOLBOX to continue the binocular camera
calibration process.
Firstly, the left camera was chosen as the fixed reference
position. Then, the rotation matrix R and the translation matrix
T of the coordinate transformation from the right camera to the
left camera were calculated respectively.
Finally, the camera external parameter matrix W can be
determined according to the following equation. The calibration
of the camera has been completed.
𝑾 = 􁈾𝑹 𝑻􁈿 (1)
3) Actual detection performance
Due to the light absorption and scattering that is inherent in
an underwater environment, underwater imaging is
characterized by uneven illumination and unclear features. As
shown in Table.1, the measurement error is found by comparing
the distance measured by the camera with the actual distance
under water. As the actual distance increases, the measurement
error exhibits a slightly nonlinearly trend. Therefore, the error
correction function can be obtained by fitting the measured
distance against the actual distance.
TABLE 1. ERROR BETWEEN ACTUAL DISTANCE AND MEASURED DISTANCE
Measurement
type First to fourth Fifth to eighth Ninth to
twelfth
Actual distance
(cm)
25.00 45.00 65.00
30.00 50.00 70.00
35.00 55.00 75.00
40.00 60.00 80.00
Measured
distance (cm)
22.18 38.00 55.35
25.30 42.01 59.60
29.38 46.33 63.50
33.83 50.80 68.50
Error (cm)
2.82 7.00 9.65
4.70 7.99 10.40
5.62 8.67 11.50
6.17 9.20 11.50
III. EXPERIMENT RESULTS
A. Static detection
The code in the YOLOv5 detection method was tested and
repackaged, and the binocular ranging algorithm was added into
the code. First, the algorithm identified the object category and
drew a detection box around the object. Next, the binocular
ranging algorithm located the object position on the generated
depth map within the detection box, and determined the final
distance using weighted averages of the effective values on the
outer edge of the object. Considering that the detection process
was executed using the AUV's close-range target recognition
and capture functions, the detection interval was set between
30cm and 100cm.
In the static detection experiment, markings were placed at
200mm intervals on the bottom of a swimming pool and plastic
water bottles and mugs were used as the test objects. The
camera took pictures of the targets at each distance interval,
detecting the position and category of the object and calculating
the distance.
Figure 6. Static detection results of plastic bottles at a distance of 800mm, 600mm, 400mm and 300mm (before correction)
383
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:38:30 UTC from IEEE Xplore. Restrictions apply.
Figure 7. Static detection results of plastic bottles at a distance of 800mm, 600mm, 400mm and 300mm (after correction)
Figure 8. Static detection results of Mugs at a distance of 800mm, 600mm, 400mm and 300mm(before correction)
Figure 9. Static detection results of Mugs at a distance of 800mm, 600mm, 400mm and 300mm (after correction)
As shown in Fig.6-9, the experimental results show that the
system achieved good detection performance for static
underwater targets that were within a range of 300mm-800mm,
with an error within 2%. By comparing the ranging results
before and after the correction, it can be proven that the error
correction scheme for static detection had good performance.
IV. DISCUSSION
This study proposes a new method combining YOLOv5
with a stereo-matching algorithm based on image parallax.
Firstly, a neural network was used to identify and detect the
position of an object in an image. Then, by matching the
detection box with the depth map, the relative distance between
the target and the camera was calculated by the weighted
average method. A large amount of experimental data was
collected and the results show that the method achieved good
performance in both static and dynamic conditions with short
detection time. In summary, this new idea is feasible and can be
effective for real-time target detection of AUV.
V. CONCLUSION
The experimental results show that the new method
proposed in this paper, which combines YOLOv5 with image
disparity based stereo matching algorithm, is feasible and can
be effectively used for real-time object detection in AUV. In the
future, it is necessary to conduct experiments in actual marine
environments and dynamic object detection experiments to
further verify the feasibility of this method.
ACKNOWLEDGMENTS
This work is supported by the Special projects in key fields
of colleges and universities in Guangdong Province
(2021ZDZX1037), the Innovation Team Project of Universities
in Guangdong (2021KCXTD015), the Science and technology
innovation team project of Zhuhai College of Jilin University
(2019CXKYTD001), and Doctor Promotion Project of Zhuhai
University of Science and Technology.


&&&&&&&12.Development of a Coral Monitoring System
for the Use of Underwater Vehicle
Masakazu Arima, Kana Yoshida, Hirofumi Tonai
Department of Marine System Engineering
Osaka Prefecture University
Sakai, Japan
arima@marine.osakafu-u.ac.jp
Abstract— The purpose of this research is to develop a coral
monitoring system for the use of underwater vehicles. Reef coral
is very sensitive about changes of ocean environment, such as
unusual climate changes or increase of crown-of-thorns starfish
resulting from the global warming. Coral can thus be an index of
ocean environment. It is widely known that some kinds of corals
have an inherent characteristic of fluorescence due to ‘coral
fluorescent protein (CFP).’ The authors are developing an oceangoing
solar-powered underwater glider, named ‘Tonai60’. The
operational depth of the Tonai60 glider is 60m, for ocean
environmental monitoring in twilight ocean zone. The Tonai60
glider is equipped with an ocean-environment monitoring data
logger ‘RINKO-Profiler’ of JFE Advantech Co., Ltd. and an
underwater passive acoustic data logger ‘A-tag’ of Marine Micro
Technology Corp. for monitoring marine mammals. Measuring
items of RINKO-Profiler are depth, temperature, conductivity,
salinity, dissolved oxygen, chlorophyll and turbidity. The glider is
also equipped with a coral monitoring system at the forefront of
its fuselage. This system consists of a network camera and 3-axis
digital compass. Coral can be detected from an image binarised
with a certain threshold. This paper deals with a coral
monitoring system using ultraviolet LEDs and image analyses.
Keywords—coral monitoring system; ocean ecosystem;
underwater vehicle; Tonai60
I. INTRODUCTION
In recent years, there often occur natural disasters due to
abnormal weather the whole world over. Climate changes
resulting from the warming of the globe have a great impact on
ocean ecosystem, too. Japanese Ministry of the Environment
has formulated the ‘Marine Biodiversity Conservation
Strategy’ in March 2011. This Conservation Strategy aims to
conserve the biodiversity which supports the sound structure
and function of marine ecosystems, and to utilise ecological
services of the ocean in a sustainable manner. The
Conservation Strategy made mention as follows [1];
‘Coral reefs are suggested to be vulnerable to the climate
change, and their large-scale bleaching by the increased
seawater temperature has been observed frequently in recent
years around the world. Furthermore, increasing ambient
concentrations of carbon dioxide will lead to more carbon
dioxide dissolved into seawater and subsequent aggravation of
its acidification. Acidification of seawater will then suppress
calcification to produce calcium carbonate for the skeleton of
corals and the shell of plankton. Some species may not be able
to form its skeleton or shell, and balance of the ecosystem may
be lost due to changes in the species composition.
To implement the measures necessary for marine
biodiversity such as conservation, to check the effects of those
measures and to react adaptively, changes in marine
ecosystems must be observed, and monitoring must be
encouraged. Through survey programs such as Monitoring
Sites 1000, data on the natural environment such as data on
biota of shallow water ecosystems (seaweed beds, tidal flats,
coral reefs, etc.) will be improved continuously. At the same
time, data on sea turtles, sea birds, marine mammals and so on
will be collected and organized. Furthermore, if information
that has not been collected continuously turns out to be
important in detecting changes in marine biodiversity, a
method to monitor such information will be examined, and
efforts will be made to accumulate it.’
The purpose of this research is development of a coral
monitoring system, especially for the use of autonomous
underwater vehicles (AUVs). Observations of reef corals are
usually conducted by humans; scuba diving for detailed
investigation of dominant species, manta tow technique and
line intercept transects for a local-area investigation of coral
coverage. Aeroplanes and satellites are sometimes used for a
wide-range and rough investigation of coral distribution. Each
of these methods has its advantages and disadvantages [2, 3].
For instance, there are time restrictions for scuba diving at a
depth of over 30 metres. Observations by humans using ships
and aeroplanes are influenced by weather and sea conditions.
Remote sensing by satellites is obstructed by clouds.
The authors plan to realise a wide-range and long-term
monitoring of reef corals using autonomous underwater glider.
This paper deals with a coral monitoring system which consists
of a high-sensitivity network camera and a 3-axis digital
compass for the use of navigation and a controller on board the
ocean-going solar-powered underwater glider, named Tonai60.
It is widely known that some kinds of corals have an inherent
characteristic of fluorescence due to coral fluorescent protein
(CFP) [4]. Coral can be detected from an image binarised with
a certain threshold, and the network camera will be controlled
automatically so as to maintain its lens to a downward direction,
even though the attitude of the glider was changed.
JSPS KAKENHI Grant Number 23360393, Grant-in-Aid for Scientific
Research (B)
978-1-4799-3646-5/14/$31.00 ©2014 IEEE
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:46:31 UTC from IEEE Xplore. Restrictions apply.


&&&&&&&13.Design of an Underwater Robot for Coral Reef
Monitoring in Honduras
Jose Luis Ordoñez Avila, Marcial Gustavo Ordoñez Avila, Maria Elena Perdomo
Faculty of Engineering, Universidad Tecnológica Centroamericana (UNITEC), San Pedro Sula, Honduras
Email: jlordonez@unitec.edu; marcial.ordoñez@unitec.edu; maria_perdomo@unitec.edu
Abstract—Climate change is affecting all ecosystems in the
world, robotics applications are focusing on monitoring
ecosystems effects. Coral reef in Honduras is an important
resource that needs to be preserved. This study shows the
development of a robot for monitoring coral reef, simplifying
some challenges of underwater mechanical robot design. The
robot development implements VDI 2206 model into cycles the
lab design and the prototype integration. Because of different
force estimations, simulations and field test the robot accomplish
its task. PWM outputs control the force thrust. Internal sensors
for leaks and temperature assure the robot functions. The vision
system detect motion to take videos and photos every 2 minutes.
Sea water temperature, videos, and photos are stored in a minicomputer.
The flow simulation assures robot mechanical design
to accomplish its task. The control should be improve adding a
mathematical model for the robot displacement underwater.
Keywords—Robotics, Honduras, Coral reef, PIC
Microcontroller, IHCIETI
I. INTRODUCTION
Today many studies develop at sea; many of these studies
are important to conserve our oceans and coast. These studies
have complexity that can sometimes endanger humans' lives.
An example of this is the Veracruz Reef System in Mexico.
The depth is greater than 30 meters and has a navigation
channel that jeopardizes divers' integrity while conducting
investigations [1]. Other research conducted in Honduras,
found in new coral reef in Tela on 2014, representing the
highest population density after being extinct anywhere in the
Caribbean [2].
The beauty of these sea creatures is admirable. However,
climate change affects these specimens; high temperatures
calcify these reefs by turning them to death. The temperature
trend on the surface continues to rise. This calcification is why
monitoring reefs has become necessary. Survival of corals
reef drop below 50% when maximum temperatures exceed
30.5 ◦C [3]. Figure 1 shows the 30.5 ◦C point and the average
temperature from 2015 to 2020 in Tela, Honduras.
Underwater robots and agricultural robots have a wide
range of applications, including those related to the
monitoring of the environment, and disease detection, among
others [4]. These robots can be divided into remotely operated
vehicles (ROV) and autonomous underwater vehicles (AUV).
Some ROV uses a cable to establish communication.
Unlike traditional communication, underwater,
radiofrequency, and optical waves. These communications are
not practically underwater. Heat and photons affect optical
transmitters, while radio waves are limited by the dense
marine environment, leaving the option of acoustic modems,
which are excessively expensive [5].
Fig. 1. Tela surface temperature [6]
Radiofrequency technology loses strength while performing
at a greater distance. Optics become more convenient.
However, without becoming efficient, underwater
communication networks are a highly acclaimed research area
today [7]. It shows the difficulties of underwater robot
teleoperators.
AUVs are a challenge because of programming,
mathematical models, and algorithms to control the robot's
trajectories. One option is to generate structured maps in cells
that provide information that spaces are free to move the robot
[8]. This article aims to develop an underwater robot with an
artificial vision system, to monitor coral reefs at different depth
below sea level. This robot uses different modes to accomplish
its tasks, such as teleoperation for surface mobility and video
access in submerging mode.
II. METHOD
The method used in this research is a VDI 2206 model [9].
This method proposed different cycles to develop a functional
prototype. The cycles in this project are the lab design and the
functional prototype. Lab design comprises analyzing the
prototype using simulation software, including hydrodynamic
simulations, electronic design, control design, and vision test.
The functional prototype process includes 3D printing
manufacturing, microprocessors programming, and systems
integrations. Fig. 2 shows the systems and subsystems
developed in this project. The two systems are the mechanical
system and the electronic system. For the lab design, we
validated the mechanical systems using SolidWorks flow
simulation and simulation express. The electronic system using
Matlab and a small aquarium for testing.
86
2021 6th International Conference on Control and Robotics Engineering
978-1-6654-1239-1/21/$31.00 ©2021 IEEE
2021 6th International Conference on Control and Robotics Engineering (ICCRE) | 978-1-6654-1239-1/20/$31.00 ©2021 IEEE | DOI: 10.1109/ICCRE51898.2021.9435710
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:47:19 UTC from IEEE Xplore. Restrictions apply.
Fig. 2. Systems and subsystems diagram
III. LAB DESIGN
A. Mechanic System
The first step in the robot's development was to select
components and materials for the structure. The structure
design comprises a 4" diameter waterproof capsule, a PETG
shell. To submerge the robot, you need an airtight design, so
all electronic elements there is a fully sealed acrylic capsule,
placing all electronic elements inside connecting
microcontrollers, turbines and sensors specialized in systems
under the sea. The shell comprises two symmetrical parts used
for mounting the thrust, as shown in figure 3. It has circular
perforations allowing the water flow through the submerging
process.
Fig. 3. Hydrodynamic shell design
The structure volume estimated was 0.0142 m3. We
calculate the buoyancy force using equation 1.
(𝑆𝑒𝑎𝑤𝑎𝑡𝑒𝑟 𝜌 = 1.024 𝑘𝑔/𝑚3)
𝑓𝑏 = 𝜌𝑔𝑉 = 142.5 𝑁 (1)
The next step is the hydrodynamic evaluation in
SolidWorks. This evaluation comprises two parts: calculating
the force needed to submerging the robot and moving it
underwater.
Fig. 4 is a submerging simulation. The simulation was
performed assuming a fluid velocity of 0.5m/s. It shows the
velocity vectors in different colors depending on their speed,
generating the fluid patterns. The fluid pattern generates some
vortex; however, their velocity is too slow, going from 0 to
0.125m/s. The shell works well for submerging. The bidimensional
flow simulation is shown in Fig. 5. This
simulation shows a flow pattern with no vortex and an
isosurface for velocity representation. The trajectories in the
pattern decrease from 0.517 - 0.258m/s because of the capsule
and shell. The green isosurface represents the fluid velocity
(0.249-0.374m/s) as it advances in the water.
Fig. 4. Submerging flow simulation
Fig. 5. Moving flow simulation
Now we have the force to move the robot into the water.
The robot needs more force for submerging than for moving, as
shown in figure 6. The average force for moving is 13.92 N
and for submerging 15.56 N. The maximum water resistance
force 𝑓𝑤𝑟 is 20.75N. The design of the mechanical system is
completed for the functional prototype manufacturing and
assembling.
Fig. 6. Moving and submerging force results
B. Electronic System
The robot has an internal and an external communication.
The internal communication is to communicate the three
microcontrollers. This microcontrollers are for the vision
subsystem, one for the storage of sensory data and robot
movement, and a master for executing an internal timer for
missions underwater. The external communication is used to
teleoperate the robot before submerging. This teleoperation
87
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:47:19 UTC from IEEE Xplore. Restrictions apply.
allows the robot to be placed in a favorable position for
submerging.
The robot mobility depends on the thrust force. Fig. 7
shows the robot mobility. There are two thrust to move at sea;
activating both the robot moves forward and reverse. If only
one thrust is turned on, it rotates to one side. It also has two
more thrust for submerging. The robot structure can work
with 4 degrees of freedom (DF), but programming only 3 DF
to achieve bi-dimensional movements and gain stability.
Fig. 7. Robot mobility
The force needed to at sea surface (1 and 2) was calculated
with SolidWorks flow simulation. Now using the thrust
experimental data from the manufacturer. The force should be
greater than 19.4 N between the two thruster. Each thruster
can perform a force between 0-28.4 N at 12 v as show in
figure 8. The R2=99.27% represents a high reliability between
the RPM and the force. Calculating The thruster RPM by
equation 2.
𝑅𝑃𝑀 = −2.315𝑓2 + 149.75𝑓 + 526.15 = 2185 (2)
The force needed for submerging (𝑓𝑑 ) is greater than the
one for moving and can be calculated in equation 3. The
estimated weight (w) of the robot is 120 N. The 43.25 N are
for the two thrust applying equation 2, each thrust RPM is
equal to 2685.
Fig. 8. Force (X axis) and RPM (Y axis) thrust relation [10]
𝑓𝑑 + 𝑤 = 𝑓𝑏 + 𝑓𝑤𝑟 (3)
𝑓𝑑 = 𝑓𝑏 + 𝑓𝑤𝑟 − 𝑤 = 43.25 N
The relation between the PWM signal and the thruster's
RPM was calculated using Matlab system identification tool.
Basing the transfer function in experimental data. Figure 9
shows the output response.
𝑦(𝑠) =
0.001302
𝑠2+4.673𝑒−6𝑠+0.000995 (4)
Fig. 9. Thruster step response
The internal sensors are used to the detect potential
problems. It includes two sensors, one for internal temperature
and the leak sensor. The vision subsystem needs a lot of
processing, making the microcontroller to overheat. The
microcontrollers are inside a close capsule, avoiding air flow.
The overheating testing determined a temperature increase
from 29 to 50 C. The vision subsystem turns off with this
increasing temperature. If temperature is higher than 40 C, the
process will interrupt aborting the process. The same happens
with the leaking sensors to prevent incidents inside de capsule.
The external sensors receive information from the sea. The
MS5837-30BA use I2C to send the pressure and temperature
underwater. The sensor operating depth is 300m and 30 bar.
The temperature accuracy is 1 degree Celsius. This sensor
communicates with the Atmega to store the sea temperature.
The depth is used to control the submerging.
Fig. 10. Vision testing
One important test is the artificial vision system. . It started
using a camera with VGA output, which did not give the
expected results proceeded with the acquisition of a 1080
Lifecam webcam. We tested the system for over 12 hours using
a small laboratory aquarium that was implemented for this
project. This aquarium test if the camera could detect, record,
and photograph the goldfish in motion. This testing resulted in
the microcontroller's heating, so a heat sink was added to
improve the operation. The artificial vision system has detected
fish, through a webcam, successfully, for more than an hour
continuously, presenting no inconveniences.
IV. PROTOTYPE INTEGRATION
The shell is 3D printing using PETG, infill is 15%, making
the structure with internal cavities. Metallic cummerbunds
88
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:47:19 UTC from IEEE Xplore. Restrictions apply.
holds the shell with the capsule. Fig. 11 shows the subsystems
integration. The PIC microcontroller is in charge of the
mission time for submerging and data storing. The Atmega is
the main microcontroller. It receives serial communication to
be teleoperated before submerging. Controlling the thrust by
PWM signals moving a RPM calculated in section III. The
electrical consumption of the thrusts is vital to determine the
autonomy of the robot, using two 12V batteries with
7500mA/H. Founding that the thrust that goes up and down
the robot discharges the battery in 120 minutes.
The external sensor use I2C storing information in a SD
card. The Raspberry Pi starts its video process and stores it in
a SD. Motion library to perform two minutes videos when
motion was detected. The sensory system receives
information from the sea, the temperature that is vital in
climate change, and also has a pressure sensor to know the
level of depth at which it is located.
Fig. 11. Schematic diagram
We show the functional algorithm that describes the robot
process in Fig. 11. The teleoperation mode activates once
serial communication is stable. This mode allows the user to
move the robot to the sea surface by using a control. The
control has submerging button in which it activate a timer and
monitoring the internal sensors. The set_t is the time set by
the user the data acquisition record videos, photos and the sea
temperature.
The submerging mode turns on the thrust to introduce the
robot inside the sea until a define max depth (max_d). When
the depth is greater than the max_d thrusts turn off. The
floating mode moves the robot by the floating force until it
gets the define minimum depth (min_d). This process has the
advantage of using less energy to maintain the robot
underwater. The process continues until it is greater than set_t.
Then the robot gets to the surface because of the floating force.
Serial communication can connect it to teleoperation mode to
start the process again. The submerging mode also has a close
loop. First, it uses an acceleration ramp to star thrust, reducing
strong movements. This acceleration ramp increase from 0 to
2685 RPM in 1.5s, saving the actual depth. The controller
compares the actual depth with the real depth, increasing
PWM signals if it's necessary.
Fig.12. Functional algorithm
V. EXPERIMENT
We selected Tela, Honduras for testing the robot in the sea.
This place contains two small coral reef barriers. Robot
parameters where set_t = 10min, max_d = 5m, and min_d =
3m. The waves' tallness were less than 0.5m permitting a stable
sea movement. The teleoperation mode has no inconvenient for
displacement of the robot in the sea surface. Submerging the
robot has greater challenge. The robot's movement on some
occasions was too fast, so the camera can't properly focus the
images. Fig. 14 shows a coral reefs section captured from the
video. After two hours of measurements, temperature varies
from 26.59 to 29.74 C.
Fig. 13. Functional prototype
89
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:47:19 UTC from IEEE Xplore. Restrictions apply.
VI. DISCUSSION AND CONCLUSION
In lab design section, implementing a software simulation
technique was important to develop the robot. Another
example of this technique is the projections of motion velocity
[12]. The flow simulation method used to calculate a balloon
prototype force is a similar method used in this project [11].
Drag force and coefficient of drag are among other factors
that determine the profile for the blimp design.
The robot uses a teleoperation mode to move on the sea
surface. Other systems take advantage of minicomputer with
access point and Wi-Fi connection. In addition, robots can
work together, with the control center and with an external
device [14]. The UART communication is a simple way to
teleoperate the robot in short distance improvements could
increase the communication distance.
The robot mechanical design has accomplished its task.
The control should be improve adding a mathematical model
for the robot displacement under water. This improvement
should be like the thrust dispatching module that calculates
control signals [12]. A sonar sensor could help to map its
environment.
AC KNOWLEDGEMEN T
This work was supported by IHCIETI, who gave the funds
to conduct this research as part of PIA 2019.
Fig. 14. Robot video capture a) coral bank, b) Echinoidea Sea hedgehog


&&&&&&&14.Autonomous Robotic Exploration of Coral Reefs
using a Visual Attention-driven Strategy for
Detecting and Tracking Regions of Interest
Alejandro Maldonado-Ram´ırez and L. Abril Torres-M´endez
Robotics and Advanced Manufacturing Group
CINVESTAV Campus Saltillo
Ramos Arizpe, Coahuila, M´exico 25900
Email: {alejandro.maldonado, abril.torres}@cinvestav.edu.mx
Abstract—Visual-based autonomous robotic exploration of
unstructured and highly dynamic environments is a complex
task. We present an approach to carry out an attention-driven
exploration of underwater environments. This work is aimed
to grant autonomy to an exploring agent in terms of deciding
where to move in function of relevant visual information. This
way we could obtain close video-observations of regions of
interest in coral reefs in order to diagnose disease or physical
damage. Our approach first detects relevant points from the
images the vehicle is capturing, this is achieved by utilizing a
visual attention model adapted to work on underwater scenes.
Then, a particular region of interest can be quickly and robustly
tracked by using superpixel descriptors, which are associated
to each of the relevant points. The tracking continues as long
as it results interesting for the visual attention algorithm. The
field experimental results show the effectiveness of the proposed
approach.
I. INTRODUCTION
One of the key aspects of having an integral robotic system
is autonomy. Although, a high level of autonomy has been
reached in mobile robotics, it still remains as an open problem
in diverse fields. In particular, the autonomous navigation
through unknown environments can be inherently considered
as an exploration. When an exploration task is faced, human
navigation is commonly lead by those features that catch our
attention. If we do not have an specific task to perform, we
could simply wander around the environment driven by our
curiosity or even by a more inner desire (for example, if we are
hungry we may begin to search for food). This work is aimed
to reach this kind of autonomy on an Autonomous Underwater
Vehicle (AUV) for coral reef exploration. To achieve this, it
is important to be able to identify the relevant regions from
the scene even when no information about them is available.
Our approach relies on a computational visual attention model
which emulates the human visual attention system. This means
that an algorithm will be capable of detecting a image region
that will likely draw the attention of a person. For this work,
we want the relevant regions to lead the motion of an AUV
with the aim to perform a coral reef exploration in a curious
human-like manner. However, it is important to notice that
the exploration of coral reefs is a complex task due to the
inherent properties in this kind of environments, such as:
color distortion, poor visibility, suspended particles and lack of
structure among others. Furthermore, coral reefs have a high
diversity of color and unstructured information, which makes
difficult to determine what the most relevant part would be. The
visual attention algorithm we propose was designed by keeping
in mind those conditions to obtain a good performance.
Besides of detecting regions of interest, it is also important
to track the same region of interest during several frames.
The tracking should be fast in computational time and robust
in order to obtain useful video-observations while avoiding
erratic motions in the vehicle. We use a descriptor based on a
superpixels segmentation [1] to help tracking a detected region.
The rest of this paper is organized as follows. First, in
Section II, a short overview of the application of visual attention
system in underwater task is given. Section III describes
our proposed method in detail. In Section IV, we give the
experimental results obtained during the field trials. Finally
the conclusion and future work is presented in Section V.
II. RELATED WORK
Visual attention is a selective process that allows us to
determine regions of interest in a scene according to visual
stimuli. Nowadays, there are several visual attention models,
but two of the most known are the Neuromorphic Vision
Toolkit (NVT) [2] and the Visual Object detection with a
CompUtational attention System (VOCUS) [3]. This kind
of models has been used on underwater applications. For
example, in [4] a visual attention model is used to detect
frames of interest in a long video stream, i.e., instead of having
a human watching hours of video to identify interesting frames,
they use a visual attention algorithm to extract the potentially
interesting frames. In [5], the authors use a visual attention
algorithm to detect objects that could draw the attention of a
human. Both of these work utilize the NVT [2] visual attention
model on videos recorded by a Remotely Operated Vehicle
(ROV). In [6], authors proposed a visual attention model to
detect Norway lobsters and help scientist to quantify them. All
the above mentioned work use a visual attention models for
aiding humans in the task of analysis of a video streaming. In
our case, we are interested on using a visual attention model in
an online manner to guide the motion of an AUV with the aim
of increasing the autonomy of underwater robotic systems at
the moment of deciding where the exploration should proceed.
The visual attention models are useful when there is not
enough prior information about the regions of interest and the
environment to explore. They tend to find what is more relevant
978-1-4799-8736-8/15/$31.00 ©2015 IEEE
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:48:03 UTC from IEEE Xplore. Restrictions apply.
Fig. 1. A general diagram of the proposed methodology.
in terms of general criteria such as colors, intensities and/or
orientation, among other features. More detailed information
about the foundations of computational visual attention models
can be found in [7]. An overview of the utilization of visual
attention models under a robotic context can be found in [8].
III. METHODOLOGY
A general diagram of the proposed system is depicted in
Fig. 1. The robot’s camera capture the visual information,
which is processed by the visual attention system to detect
and track the Region of Interest (RoI). The position of the RoI
in the image is used by the Visual Controller. This controller
determines the desired orientation of the robotic system. The
desired values feed the inner control loop of the system. More
detailed information about each of the part are presented in
the following sections.
A. Visual Attention System
A computational visual attention algorithm detects relevant
regions in an image emulating the human visual attention. The
proposed methodology for detection and tracking of regions
of interest it is summarized in Fig. 2. Our visual attention
model relies in some of the key ideas of Itti’s and Frintop’s
visual attention models [2], [3]. The general process to extract
relevant regions on an image can be summarized as [2]:
1. Image filtering to remove noise in the image.
2. Extraction of features, for example: intensity, colors,
orientation or motion.
3. Calculation of Gausssian pyramids for each feature.
4. Applying center-surround differences over each level
of the pyramids to highlight regions that stand out of
their surround.
5. Summation of all the resulting pyramid’s images to
obtain a saliency map.
6. Localizing the most salient region (brightest part of
the image) on the saliency map.
Modifications of the general process were made in the
proposed visual attention system because of the conditions of
the environment. Typically, a natural underwater environment
lack of defined orientation and shape unlike man-made indoor
environments where planes and lines can be found. Also, the
intensity of the light reaching these benthic environments is
not constant. For that reason, our attention model leaves aside
commonly used intensity and orientation features, thus strongly
relying on color information to detect regions of interest. Since
the degradation of color and the poor visibility is inherent in
this type of environments at distances and depth greater than
10 meters [9], it is crucial to select an appropriate color space
to achieve an effortlessly underwater image enhancement.
Similar as in [3], we use the CIELab color space, which is
a perceptual uniform color space. The a and b channels of this
color space naturally comprises contrast colors (red-green and
yellow-blue), which is suitable for underwater image analysis
and processing. For calculating the saliency map, we use the
red, green, yellow and blue colors as features. The colors are
extracted as specified in [3]. A visual attention algorithm gives
us a saliency map that can be used to identify the relevant
parts on a scene (see left side of the diagram in Fig. 2 for an
example of a saliency map). However, we are also interested
in tracking the same relevant part along subsequent frames.
Thus, to track a RoI, we need to have a region descriptor
that takes into account color and position. It turns out that
the use of superpixels is suitable for this task. Superpixels
are defined by their average color and position. To generate
the superpixels the Simple Linear Iterative Clustering (SLIC)
superpixel segmentation algorithm [1] is used. Then, a RoI can
be described by the superpixel it belongs to. To track a given
RoI at frame t−1, we search for it among the k most relevant
regions found by in the saliency map of frame t. In Fig. 3 the
detection and tracking of a RoI is exemplified. A more detailed
explanation of this part of the model as well as results in an
offline manner can be found in our previous work in [10].
In this paper, we have also added the state of the robot in
order to perform the tracking of the RoI. Before searching
for the RoI, the increment of the yaw angle of the robot
from the last frame t − 1 to the current frame t is obtained
(Δψ = ψt−ψt−1). On one hand, if Δψ is greater than half the
field of view (FoV) of the camera, it is likely that the robot is
looking to a completely different scene than the one from the
frame t−1. Therefore, the visual attention algorithm will not
search for the last RoI, instead it will determine a new one.
On the other hand, if Δψ < FoV
2 , then the visual attention
algorithm will search for the last RoI around the left or the
right side of the image depending of the sign of Δψ. This
modification was added to avoid the cases where the visual
attention algorithm is attempting to search for a region of
interest that likely is not in the current image because an abrupt
change of robot’s orientation, e.g. an strong ocean current that
drastically changes the robot’s orientation.
B. Visual Controller
This controller takes the RoI’s center coordinates as input
and calculates the commands to move the robot such that the
RoI stays in the image’s central region defined by a threshold
, as depicted in Fig. 4. Particularly, it calculates the yaw and
speed commands for the robot’s motion controller. Considering
the position of the RoI in the image as pRoI = (uRoI, vRoI),
a mapping over the x−coordinate is done as shown in the
following equation:
xRoI =
2uRoI
cols
− 1, (1)
where cols is the number of columns in the input image. The
resulting xRoI ∈ [−1, 1] is independent of the size of the image
because of the division by cols in equation (1). The desired
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:48:03 UTC from IEEE Xplore. Restrictions apply.
Fig. 2. General diagram of the proposed visual attention system.
Fig. 3. An example of the detection and tracking of a RoI by using the
information of the previous frame.
yaw command ψd is calculated in terms of xRoi as shown in
the following equation:
ψd =

ψt + k1xRoI if |xRoI| ≥ 
ψt if |xRoI| < 
, (2)
where ψt is the current yaw angle of the robot as measured by
the Inertial Measurement Unit (IMU) and k1 is tunable positive
gain. As can be seen in (2), when xRoI is within the threshold
, the desired yaw angle remains unmodified, otherwise the
AUV changes its direction of movement accordingly to xRoI.
Considering the number of frames that the same or similar
regions has been tracked (nRoI), the commanded forward
speed sd is calculated as:
sd =

sdef − k2nRoI if sdef > k2nRoI
0 if sdef ≤ k2nRoI
, (3)
where k2 is a tunable positive gain and sdef is the default
speed for exploration. It can be seen in 3 that when nRoI
increases, the commanded forward speed tends to zero. This
behavior has the objective of allowing the robot to stay in a
hovering position during few frames to gather images from
the same or similar regions of interest. To prevent that the
robot gets stuck exploring the same place because of the speed
reduction, sd returns to its default value when nRoI is greater
than a predefined value.
Fig. 4. Desired task: to locate the Region of Interest in the central region of
the image. The width of the central region is 2.
C. AUV’s Motion Controller
A PD-controller is used to control the robot’s orientation.
The desired yaw orientation is received from the Visual Controller
and the roll and pitch angles are set to zero, which
means that the motion of the robot is limited to its XY plane.
The PD-controller can be substituted by other more advanced
controllers for underwater vehicles, however for our purposes
it worked reasonably well.
Finally, as the robot it is not equipped with an instrument
to measure its forward speed, the range of amplitude of the
robot’s fins is modified to increase or decrease the forward
speed of the robot. The amplitudes are set in terms of the
speed commanded sd by the visual controller. The smaller the
fin’s amplitude is, the slower the robot will move.
IV. RESULTS AND DISCUSSION
A. Field testing and experimental platform
The proposed approach was tested on the amphibious robot
called Mexibot, which belongs to the family of Aqua robots
[11]. The robot’s propulsion is based on the oscillation of six
paddles providing 5 degrees of freedom. Mexibot can operate
at a maximum depth of 35m.
All the trials were performed in coral reefs belonging to
the second largest coral reef system, located in Costa Maya,
Mexico. All the experiments were done in a depth range from
5 to 18 meters under real and uncontrolled conditions.
The method was implemented under the Robotic Operating
System (R.O.S.). The visual controller, the visual attention
system and the motion controller were implemented as nodes
under the R.O.S. architecture. This means that all the components
of the method work asynchronously. The average latency
of the whole algorithm was about 0.3s. For our purpose this
latency does not represent an issue because the robot moves
to a low speed (< 1 m/s).
In Fig. 5, the x−coordinate of the RoI is shown. As it
can be seen, this coordinate tends to stay in the green shaded
region which represents the threshold  = 0.3. This means
that in fact, the AUV moved such that the RoI was tracked
during a period of time. Also, in the lower plot of the Fig.
5, the yaw angle of the robot as well as the command yaw
angle are shown. It can be seen how the robotic system was
following the desired yaw calculated by the visual controller.
The blue boxes enclose the time lapses (intervals) when the
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:48:03 UTC from IEEE Xplore. Restrictions apply.
Fig. 5. The x−coordinate of the ROI is shown in the upper plot. It can be seen
how the coordinate tends to stay within the threshold  = 0.3 (green shaded
region). The lower plot shows the yaw angle of the robot and the desired yaw
angle calculated by the visual controller. The blue rectangles enclose the time
lapses when the same region of interest was tracked.
Fig. 6. The RoI detected at the beginning and end of the time lapses enclosed
on the blue rectangles of the Fig. 5 are shown. It can be seen how the same
region of interest is keep in the field view of the robot.
same RoI was tracked. In average, the regions were tracked
about 10 seconds. It is a good tracking time considering all
the hindering environmental conditions.
The first and last frame during the time lapse the same
RoI was tracked are shown. It can be seen how in the first
frame the RoI is detected far from the AUV camera and by
the last frame the RoI ends up near of the robot. It is important
to mention that during the navigation, sometimes the robotic
system was near of colliding with protrusions of coral reef, for
example in the cases of the images A and C of the Fig. 6 it can
be seen how near the RoI is from the camera. In these cases
a diver has to intervene to avoid the imminent collision. This
indicates us the importance of adding a collision avoidance
method to be able to perform a more complete exploration of
an environment.
B. Discussion
We want to highlight the importance of the utilization of
a visual attention algorithm as the main part to guide a robot
navigation in a coral reef for exploration purposes. One of
the main advantages of this kind of algorithms is that they
do not need to have prior information of the environment. For
example, the algorithm does not need to know any information
of landmark to detect. Instead, the model determines automatically
the most relevant region in terms of some general criteria
such as color, intensity, orientation. Specifically, our algorithm
detects relevant regions in terms of colors.
Something important to mention is that the sole fact of
detecting the most relevant region on one frame is not enough
to lead the motion of the robot, it is also necessary to track the
same region along the subsequent frames as long as possible.
With that in mind, a superpixel descriptor is used. This way
we can have the same or similar regions of interest detected
in consecutive frames. Another aspect to highlight of the
proposed methodology is the flexibility of the system when
the region to follow is lost or no longer visible. In those cases,
a new region of interest is automatically detected. This action
resembles the natural behavior of selective attention in living
beings.
Finally, an important limitation of the proposed method
is that it does not incorporate a collision avoidance algorithm.
Part of this is due to the lack of 3D information, particularly the
distance from the camera to the region of interest. To compensate
this drawback and keeping the whole method using only
visual information the method for obstacle avoidance proposed
in [12] has been considered to be implemented along the visual
attention system.
V. CONCLUSION AND FUTURE WORK
We have presented a visual attention-driven navigation to
perform an autonomous robotic exploration of coral reefs. The
core of the presented work is the visual attention model that
allows the robot to find and track regions of interest in an
environment that lacks defined structure, has poor visibility
and presents loss of color, among other challenges. The near
real-time tracking of relevant features allows for a more
efficient vehicle control as it helps to establish a set of suitable
directions for the exploration. This way, the path that the robot
follows is built in terms of the regions that draw its attention
and is not defined a priori, as existing work do.
We have successfully tested the proposed framework. The
results show that, despite of the high diversity of visual
information found in the coral reefs, our approach was able
to identify the most relevant parts and track them. Since the
proposed framework does not rely on a priori information
about the structure of the coral reef or a map of it, it can be
used in any underwater environment with good results in terms
of exploring relevant regions. However, an integral scheme
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:48:03 UTC from IEEE Xplore. Restrictions apply.
combining an obstacle avoidance approach with the attentiondriven
exploration model is needed.
As future work, we would like to improve the motion
controller of the robotic system by using a controller based
on behaviors to combine the attention-driven exploration and
the collision avoidance.
ACKNOWLEDGMENT
We would like to thank CONACyT for their support and
project funding (CB-220540). We also thank Mar Adentro
Diving, Mahahual, for their support during our sea trials.


&&&&&&&15.Action Learning for Coral Detection and Species Classification
Junhong Xu, Lantao Liu
School of Informatics, Computing, and Engineering
Indiana University, Bloomington, IN 47408, USA.
E-mail: {xu14, lantao}@iu.edu
Abstract—This paper presents a method for exploring and
monitoring coral reef habitats using an autonomous underwater
vehicle (AUV) equipped with an onboard camera. To accomplish
this task, the vehicle needs to learn to detect and classify
different coral species, and also make motion decisions for
exploring larger unknown areas while trying to detect as more
corals (with species labels) as possible. We propose a systematic
framework that integrates object detection, occupancy grid
mapping, and reinforcement learning methods. To enable the
vehicle to adjudicate decisions between exploration of larger
space and exploitation of promising areas, we propose a
reward function that combines both an information-theoretic
objective for environment spatial coverage and an ingredient
that encourages coral detection.We have validated the proposed
method through extensive simulations, and the results show that
our approach can achieve a good performance even by training
with a small number of images (50 images in total) collected
in the simulator.
I. INTRODUCTION
Autonomous underwater vehicles (AUVs) are highly attractive
alternatives to human divers for many underwater
search, surveillance, and monitoring tasks [1], [2]. For instance,
the coral reef monitoring has become increasingly
important because corals have direct and indirect impact on
the habitats of other marine animals. Oftentimes this task
is carried out by humans, e.g., scuba divers. In this paper,
we present an approach that allows an AUV to explore
underwater environments and detect corals autonomously.
The major challenge is how to utilize limited visual
information to plan motions for the AUV. One method is to
use the end-to-end learning to obtain actions from directly
using the visual data [3]. However, images contain only
local information within the camera’s field of view, and thus
without a carefully designed reward function that contains
global environmental information, it is difficult for the AUV
to learn to explore a large unknown environment. Instead of
learning directly from raw pixels, we combine deep learning,
occupancy grid mapping [4], and reinforcement learning
methods in a systematic way. Specifically, a YOLOv3 object
detector [5] is used as a visual perception system to process
the images streamed from an onboard camera and output
probabilistic distribution of detected classes of coral species.
This information is then used as input to build an occupancy
grid map where each grid represents the probability of
corals in the corresponding location. The map together with
the current pose information of the vehicle are passed to
a reinforcement learning component for motion decisionmaking.
An illustration of the system is shown in Fig. 1.
Fig. 1. An illustration of the system that enables the AUV to autonomously
explore and monitor the coral habitats in unknown environments. The
YOLOv3 detector (shown on the top) [5] processes the images captured
by an onboard camera. The obtained coral species distribution is used to
update the occupancy grid map of the environment. The AUV’s motion
decision is then computed based on the updated map and the current pose
of the vehicle.
In order to allow the vehicle to explore the environment,
we use an information-theoretic reward function calculated
with the information gain of the constructed map to train the
AUV. This will ensure that when a coral-rich area is fully
explored, the vehicle will switch to search for more corals in
other areas. Because the distribution of corals follow some
specific patterns [6], once the AUV is trained in a large set
of environments, it generalizes to explore and monitor other
new environments.
To validate our method, we have conducted extensive
simulations. We evaluated the performance of the AUV in
terms of the amount of map entropy reduction and the number
of detected corals. The results reveal that our proposed
framework allows the AUV to explore and monitor the coral
habitats efficiently.
II. RELATED WORK
There has been considerable research on environmental
exploration using autonomous robots. The main idea is to
enable the robot to make sequential decisions to efficiently
explore and map an environment with a limited amount
of time [7], [8], [9], [10]. The environmental map entropy
reduction has been a widely used objective if the target
978-0-578-57618-3 ©2019 MTS
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:49:05 UTC from IEEE Xplore. Restrictions apply.
(a) (b) (c) (d)
Fig. 2. (a) Illustration of an underwater image with object bounding boxes, where the blue and red boxes represent two different types of corals. (b)
Demonstration of rotation and translation for image augmentation. Note that the bounding box locations are also changed. (c) Application of Gaussian blur
to the image. (d) Brightness reduction of the original image.
environment is initially unknown [11], [12]. Very relevant
to the map entropy, another important objective is to plan
robot exploration actions that maximize the information gain
of the map where the information gain is usually represented
with mutual information. For example, the Shannon mutual
information has been employed to explore ocean and lake
environments with aquatic vehicles [13], [14], [15], and the
Cauchy-Schwarz mutual information has been used to map
a 3D indoor environment with ground robots [9]. The work
in [16] is the most relevant to ours. The authors trained an RL
agent to choose actions which are modeled as polynomials
in order to minimize the map uncertainty.
End-to-end learning is another way to learn sequential
decisions for autonomous systems. Particularly, the deep
reinforcement learning has been proved to be a great success
for autonomous robot decision-making using raw image
inputs [3]. In our work, instead of directly using pixels
as the input, we pre-process raw images with a series of
useful image processing techniques before feeding them
to the neural network to enhance the learning robustness.
With the large amount of image data and computing power
available, CNN-based object detectors have shown tremendous
progress during past few years [17], [5], [18]. Among
numerous choices, we opt to employ the YOLOv3 object
detector [5] for the coral detection due to its high accuracy
and efficiency.
III. CORAL SPECIES DETECTION AND CLASSIFICATION
In this section, we discuss the training procedure and data
augmentation methods to train a coral detector with limited
data.
A. YOLOv3 Detector
The YOLOv3 object detector is a real-time object detection
system [5] that applies a neural network to a full image.
The network divides the image into regions and predicts
bounding boxes for objects and their probabilities. It uses
anchor boxes to predict the bounding boxes for each object.
Anchor boxes are usually pre-defined before training and
serve as priors for predictions. YOLOv3 splits an input
image into small grid cells. For each grid, it predicts the
position offset of each anchor box relative to the cell center.
During training, the sum-of-squared error is used as the loss
function for the position offset prediction. The algorithm uses
independent logistic classifications for each class in each of
the anchor box to allow multi-label predictions. Since we
may not have multiple labels for each coral species, we opt
to use a softmax classifier.
B. Data Augmentation and Training Procedure
Since currently there is no open-source dataset for coral
species analysis, we created a simulated environment for data
collection as shown in Fig. 1. In this environment, we use 3D
meshes of different species (classes) of corals with differing
visual appearances as shown in Fig. 2. We teleoperate the
simulated AUV to collect a set of 50 images from different
views of the environment. Each image is recorded with pixel
resolution of 600×600. Each coral instance in the collected
images is labeled with a bounding box and its corresponding
class. For data augmentation, we augment the size of dataset
by creating similar images with operations of modifying
brightness, translating and rotating, and adding Gaussian blur
to the original images. The original and augmented images
together serve as the training sets for the YOLOv3. Examples
after the image augmentation are illustrated in Fig. 2.
Even with the data augmentation method, the size of the
dataset is still not sufficient to train the YOLOv3 detector.
We leverage the transfer learning method to further reduce
the chance of overfitting of the network. Instead of training
all the layers from scratch, we use the YOLOv3 model pretrained
on the COCO dataset [19] as a baseline for the
transfer learning. Specifically, only the last three layers of
the network are updated during training [20], [21]. This level
of fine-tuning is similar to training a 3-layer neural network
which requires much less data than training a full YOLOv3
detector.
IV. CORAL HABITATS EXPLORATION AND MONITORING
WITH REINFORCEMENT LEARNING
We propose an approach to combine the YOLOv3 detector,
occupancy grid mapping, and reinforcement learning mechanisms.
The key idea of our method is to first convert the
processed visual data into a coral location map, and then
train the AUV with an information-theoretic reward function
based on the information gain of the map. We assume the
pose of the vehicle can be accurately obtained, e.g. using a
USBL positioning system [22], and the depth of the AUV
does not change during the mission.
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:49:05 UTC from IEEE Xplore. Restrictions apply.
A. Coral Occupancy Grid Map
In order to define information-theoretic reward function,
we first need to introduce what environmental information
will be used. Since we are interested in exploring large
and possibly unknown coral habitats and monitoring those
observed areas, a reasonable choice is the map that contains
the (stochastic) information of coral locations.
1) Occupancy grid map: We use the occupancy grid
map [4] to represent our map. It is defined as a set of 2D
grids m={m0, ...,mM−1}, where M indicates the number of
girds. Every grid mi is associated with a feature fi. In our
case, the feature represents whether the corresponding grid
is occupied by corals. The general goal of the occupancy
grid mapping is to compute the probabilistic distribution of
the map from the past sensor measurements and robot poses
p(m|z0:T ,x0:T) =Πi
p( fi|z0:T ,x0:T ), (1)
where z0:T and x0:T are sequences of measurements and robot
poses at different discrete time steps, respectively. Eq. (1)
assumes that each grid is independent of others, which
makes the computation more tractable [23]. The occupancy
probability of each grid can be calculated using Bayes’ rule
p( fi|z0:T ,x0:T) =η p( fi|zT ,xT )
p(zT |xT )p( fi|z0:T−1,x0:T−1), (2)
where η is the normalizer and p( fi|zT ,xT ) is called inverse
sensor model which represents the probability of the occupancy
at mi given the current measurement and robot pose.
Using the log-odd notation, we can rewrite Eq. (2) as
Li,t = Li,t−1+log
p( fi|zT ,xT )
1− p( fi|zT ,xT )
−log
p( fi)
1− p( fi)
, (3)
where Li,t−1 is the log odds at the previous time step. The
prior of occupancy girds is commonly set as p( fi) = 0.5.
2) Inverse sensor model: To compute the posterior of a
grid being occupied by some coral, we need the inverse
sensor model p( fi|zT ,xT ) which outputs the probability of
coral occupancy at the gird mi given current image (sensing
observation) and the robot pose. We use the output from the
trained YOLOv3 detector to compute this probability distribution.
The output oT = ∪ki
=0
{si
T , ci
T ,bi
T
} from the object
detector at time T given an image zT consists of the class
score si
T , the detected class label ci
T , and the bounding box
information bi
T for each of the detected object. We express
the occupancy probability as follows
p( fi|zT ,xT) = α
kΣ
i=0
p(mi ∩ˆbi
T
|xT , zT )p(si
T
|ci
T , zT ), (4)
where α is the normalization constant and p(si
T
|ci
T , zT) = si
T
is the confidence score of the ith detected object to be the
class ci
T . The term p(mi∩ˆbi
T
|xT , zT ) indicates the probability
of intersection between the object’s predicted bounding box
ˆb
i
T in the world frame and the grid region mi.
In order to compute ˆbi
T , we need to convert the pixel
coordinates of the bounding box in the given image zT to
Fig. 3. An illustration of the camera setup. The camera is mounted
underneath the AUV. We set the camera to look downward (indicated by the
red arrow) so that each image pixel has the same depth value. The red and
gray areas denote the grids that are inside and outside the camera’s field of
view, respectively.
the world frame. To simplify this computation, we make
the camera look downward so that each point in the world
frame within the camera’s field of view is projected onto the
image plane with the same depth, as shown in Fig. 3. In
this case, the depth value of each pixel in the image is fixed
and known, which is equal to the distance between the AUV
and the ground plane. To compute the world coordinates of
the bounding box, we first compute its coordinates in the
camera frame, then convert the result to the world frame
using the transformation matrix of the vehicle computed from
the vehicle pose. The intersection probability can then be
computed as follows
p(mi ∩ˆbi
T
|xT , zT) =N (mci
|ˆbi,c
T ,Σi
T ), (5)
where mci
and ˆbi,c
T are the center positions of the grid and
the bounding box in the world frame, respectively. We
have assumed that the intersection likelihood is normally
distributed around the center of the bounding box and the
variance in the two dimensions are proportional to the height
and width of the bounding box.
B. Reinforcement Learning with Information-Theoretic Reward
The task for the AUV is to explore and monitor large
coral habitats. This requires the vehicle to choose between
staying at those discovered/explored areas or exploring more
unknown regions to increase the number of observed corals.
This is the well-known tradeoff between exploration and
exploitation. To address this issue, we propose a reward function
based on the information gain of the coral map. We use
reinforcement learning to compute vehicle action policies.
Since the coral habitats usually share some patterns [6],
the learned policy can generalize to novel environments
assuming the robot is trained in a large set of different
environments.
Because we assume the pose of the vehicle can be obtained
accurately, we can model the environment as a Markov
Decision Process (MDP). An MDP is represented by a 5-
tuple {S,A,T,R, γ}, where s ∈ S and a ∈ A stand for state
and action, respectively; T(s,a, s) = p(s|s,a) is the state
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:49:05 UTC from IEEE Xplore. Restrictions apply.
(a) (b) (c)
(d) (e) (f)
Fig. 4. Evaluation of the YOLOv3 coral detection. The top row shows the training environment. The bottom row shows the results in the testing
environments. Each detected coral species is labeled by a bounding box and its corresponding species label.
transition function, and R(s,a) is the reward function that
maps from a state-action pair to a scalar reward, and γ is
the discount factor. In the reinforcement learning setting, an
agent seeks to find a policy π that maximizes the expected
discounted reward
π∗
= argmax
π
E[γR(s,a)], (6)
where a = π(s).
1) State space: We represent the state s = (x,m) as a
concatenation of the robot current pose and the map. The
map information provides the robot the areas that have not
been explored and the locations of the observed corals. It
is used by the function approximator to generalize over
different environments.
2) Action space: Since the vehicle operates at the same
depth, an action a = (v,ω) consists of linear and angular
velocities on a plane.
3) Information-theoretic reward: The simplest reward
function for this task is to make the reward with a value
equal to the number of the observed objects from the YOLO
detector r(s,a) = |o|. However, maximizing this observation
reward function will cause the vehicle to circle around an
area where the density of the corals is high. This limits
the vehicle to explore other surrounding areas. To solve this
problem, we use an information-theoretic approach to model
the reward function which maximizes the information gain
of the map. The information gain is defined as the reduction
of the map entropy [24]
IG(m) = H(m)−H(m|at , sT+1, zT+1), (7)
after taking an action aT , transitioning into sT+1, and observing
an image zT+1. The map entropy is calculated as
H(m) =
M−1
Σ
i=0
−pT log pT −(1− pT ) log(1− pT ), (8)
where pT is the map distribution at time step T given by
Eq. (1). The conditional map entropy H(m|at , sT+1, zT+1) is
computed using the map distribution at T +1 after taking the
action at . We define the reward function as a weighted sum
of the observation reward and the information gain.
R(s,a) = wo|o|+wIGIG(m), (9)
where the weights wo,wIG,wv balance the importance between
exploration and exploitation.
4) Learning algorithm: Our reward function can be used
by any reinforcement learning algorithms. We choose to
use the Proximal Policy Optimization (PPO) algorithm [25]
since it is easy to implement and has the state-of-theart
performance. We use fully connected neural networks
as function approximators to handle high-dimensional and
continuous state and action spaces.
V. EXPERIMENTAL RESULTS
We have performed experimental evaluations through extensive
simulations. Here we present results of the AUV
which is tasked for monitoring coral habitats in an initially
unknown underwater environment. To create a high-fidelity
underwater environment, we placed 3D meshes of corals on
the ocean floor in the UWsim simulator [26]. To demonstrate
the algorithms, the corals contain two species.
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:49:05 UTC from IEEE Xplore. Restrictions apply.
(a) (b) (c)
Fig. 5. The training statistics of the PPO algorithm trained with the information-theoretic reward for 200 iterations. (a) The reward value as the number of
iteration increases. (b) The map entropy of each iteration (the lower the better). (c) The total number of corals detected in all frames during each iteration.
A. YOLOv3 Evaluation
The input image dimension is re-sized to be 416×416 to
fit the requirement of the computation process of YOLOv3.
We collected 50 images for training and additional 20 images
for evaluation. During the collection of training and evaluation
images, we set the light attenuation parameter to 0. The
camera is mounted underneath the AUV with a downward
looking angle of 60 degrees. However, for the monitoring
process, the camera viewing angle is different from the one
used in the training process (refer to Section IV-A). We need
to ensure that the detector is capable of handling different
camera viewing angles. In addition, to make the experiment
more realistic, we increase the attenuation parameter so that
the images captured by the camera appear darker.
We have evaluated the YOLOv3 performance in the coral
detection scenario. Some sampled detection results are shown
in Fig. 4. The top and bottom rows represent the visual
images captured in the training and testing scenes, respectively.
It can be seen that the testing and training scenes
have two major differences: first, the field of view of the
camera is smaller in the testing environment; second, the
training environment has no light attenuation. Despite these
differences, the detector can still detect most of the corals
correctly as shown in the bottom row of Fig. 4.
B. Experimental Results on Environmental Exploration and
Coral Detection
In this section, we evaluate the performance of the AUV
trained by the proposed reward function in terms of the
amount of map entropy reduction and the number of observed
corals.
1) Experimental setup: The width and height of the
simulated environment is set to be 120m and 80m. We
operated the vehicle to keep 10m above the simulated ocean
floor. The dimension of each grid mi is set to be 4m×4m
so that there are 30×20 grids. We use two fully-connected
neural networks with two hidden layers (with size of 512
and 1024, respectively) to parameterize the value function
and the policy. We use the learning rate of 0.0001 to train
the networks for 200 iterations. Since the environment map
is large, this task entails a long horizon to complete. By
horizon, we mean that the AUV performs H actions before
updating the parameters of the networks. We set the horizon
to be H = 2048×10.
(a)
(b)
Fig. 6. Statistics of the amount of map entropy and the total number of
detected corals are shown in (a) and (b), respectively. Results are averaged
over 10 test trials and each trial has a maximum H = 2048×80 steps.
2) RL Training Results: To evaluate the performance of
our information-theoretic reward function, we first show the
training statistics of our proposed approach in Fig. 5. It shows
three different metrics over the 200 iterations of training.
Fig. 5(a) shows the reward (Eq. 5(a)) increases as the
number of iterations increases. This indicates that the robot
is able to learn useful actions based on our reward function.
In addition to the total reward, we also plot the curves
of the two components included in our reward function.
Specifically, the trend of the map entropy reduction during
training is shown in Fig. 5(b). The revealed global entropy
reduction implies that the vehicle is able to explore more
areas instead of exploiting a small local region. Fig. 5(c)
describes the number of detected corals. We can see that
along the vehicle’s environmental exploration, it also tends
to maximize coral detection. (Note, since YOLOv3 detector
is not able distinguish if a coral has been detected earlier,
Fig 5(c) is the result that accumulates all corals detected
within all frames at each training iteration.)
3) RL Testing Results: We also compare the RL performance
with a longer horizon (H = 2048×80) than the one
used during training. The baseline of this comparison is
a random walk action policy with a zero-reward function.
Namely, at every time step, this robot executes an action
Authorized licensed use limited to: Harbin Engineering Univ Library. Downloaded on October 20,2023 at 12:49:05 UTC from IEEE Xplore. Restrictions apply.
(a) (b) (c)
Fig. 7. Four sampled trajectories from the 10 test runs of differing rewarding approaches. Different sampled trajectories are shown in different colors.
(a) The trajectories of the robot trained with the proposed information-theoretic reward function. (b) The trajectories of the robot trained with the heuristic
reward function. (c) The trajectories of the random walks (0 reward).
where the linear velocity and angular velocity are uniformly
sampled from [0m/s,4.0m/s] and [−πrad/s,πrad/s],
respectively. To further evaluate the performance of our
proposed approach, we train the AUV with a heuristic reward
function which encourages the movement of the vehicle
R(s,a) = |o|−|v−vd|, (10)
where vd is the desired linear velocity. This reward function
encourages the AUV to move at a constant speed.
We use the number of detected corals and the map entropy
as metrics for performance evaluation because these two
values are of the most interest in our task. During testing,
when the AUV is out of the environment boundary or
the number of the maximum steps has been reached, we
randomly reset the pose of the vehicle to be within a small
region around the center of the map as the starting pose for
the next testing iteration. In Fig. 6(a), we plot the comparison
of the map entropy of the three rewarding schemes. The
result shows that the RL trained with the proposed reward
function has the minimum entropy, indicating that the AUV
is able to explore more and larger areas. The number of
detected corals (averaged across the 10 trials) is shown in
Fig. 6(b). The RL trained with the heuristic reward function
(Eq. (10)) has the maximum number of coral observations.
This is because the robot only focuses on locally explored
areas without gaining much information about a larger scope
of the environment, which is an undesired behavior.
Similar trends can also be observed in Fig. 7 where
we sampled four trajectories for each rewarding method.
The trajectories of the heuristic reward function shown in
Fig. 7(b) imply that the AUV is only able to monitor the
corals within a small region. Our proposed reward function
allows the vehicle to first focus on a local area (see the area
with dense trajectories). After the local area is explored,
the vehicle continues to search for more corals in those
unexplored parts of the environment.
VI. CONCLUSION AND FUTURE WORK
We present a framework that integrates object detection,
occupancy grid mapping, and reinforcement learning approaches
to enable an AUV to explore and monitor coral
habitats efficiently. To enable the vehicle to adjudicate decisions
between exploration and exploitation, we propose a
reward function that combines both an information-theoretic
objective for coral searching and an ingredient that encourages
coral detection. We qualitatively show that our proposed
approach achieves a good performance even by training with
a small number of images (50 images in total) collected in
the simulator.
